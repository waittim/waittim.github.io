<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zekun Blog</title>
    <description>Every failure is leading towards success.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 22 Dec 2019 20:52:31 -0600</pubDate>
    <lastBuildDate>Sun, 22 Dec 2019 20:52:31 -0600</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>H1B Visa Analysis</title>
        <description>&lt;h1 id=&quot;h1b&quot;&gt;H1B&lt;/h1&gt;
&lt;p&gt;The H1B visa is a non-immigrant visa that allows companies in the US to hire graduate-level workers in specialty occupations that require theoretical or technical expertise in specialized fields.&lt;/p&gt;

&lt;h3 id=&quot;requirements&quot;&gt;Requirements&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;A bachelor’s degree.&lt;/li&gt;
  &lt;li&gt;Job offer from a company within the United States for a specialty position that matches your degree.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;why-is-h1b-popular&quot;&gt;Why is H1B Popular?&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;For a company in the US, applying for H1B is generally quicker than applying for a US Green Card, so it is popular when they want to bring in an employee for a longer period.&lt;/li&gt;
  &lt;li&gt;H1B is open to nationals and citizens of any country, as opposed to other visa types that are only open to people with certain countries of citizenship.&lt;/li&gt;
  &lt;li&gt;H1B allows holders to stay for three years initially and can be easily extended to three more years after the first term.&lt;/li&gt;
  &lt;li&gt;H1B allows holders to move their status from one company to another and also allows its holders to work part-time and work for multiple employers at the same time.&lt;/li&gt;
  &lt;li&gt;The main benefit of H1B that attracts a large volume of applicants is the fact that it is a dual intent visa. This means that it allows its holders to seek permanent residency while under the H1B nonimmigrant status.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;caveats&quot;&gt;Caveats&lt;/h3&gt;
&lt;p&gt;Despite having a lot of advantages, applying for the H1B comes with its own set of caveats or disadvantages.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The biggest drawback of H1B is the fact that there is a limit on the number of petitions that are approved each year. Because of the large number of petitions each year, the USCIS has chosen to have all petitions entered into a lottery. Through this an annual general cap of 65,000 in first, and then run the remaining in the master’s-and-above 20,000 categories.
This means there is a strong likelihood that for any given year, your petition will not be selected. Once rejected, you will have to wait another year to submit another petition.&lt;/li&gt;
  &lt;li&gt;Because of its lottery, the deadlines for the H1B applications are very inflexible.&lt;/li&gt;
  &lt;li&gt;It is difficult to find an employer that would be willing to sponsor an employee for H1B, as the process can get expensive and is unreliable.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;h1b-historic-timeline&quot;&gt;H1B Historic Timeline&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/23/uB49UpgOJeoR6Yh.png&quot; alt=&quot;Timeline&quot; /&gt;
The timeline of relevant events is listed above. Sources for this information are from the New York times, government official documents from USCIS (U.S. Citizenship and Immigration Services).&lt;/p&gt;

&lt;h1 id=&quot;exploration&quot;&gt;Exploration&lt;/h1&gt;

&lt;h3 id=&quot;purpose&quot;&gt;Purpose&lt;/h3&gt;
&lt;p&gt;The main purpose of our exploration was to examine and recognize the trends in H1B Visa and see how data-related jobs in the United States have changed over the past 5 years.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;To perform this analysis, we combined data sets of 5 different years from 2014 to 2018.
&lt;img src=&quot;https://i.loli.net/2019/12/23/HhcSbvMuTYt9mDf.png&quot; alt=&quot;df1&quot; /&gt;
&lt;img src=&quot;https://i.loli.net/2019/12/23/wjlLDzWR3EZJ8am.png&quot; alt=&quot;df2&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Column Name&lt;/th&gt;
      &lt;th&gt;Column Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;job_title	Job&lt;/td&gt;
      &lt;td&gt;title of the particular H1B application.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;case_status&lt;/td&gt;
      &lt;td&gt;Certified-withdrawn; Certified; Denied; Withdrawn; Rejected; Invalidated- we filter this data by certified as we only wish to keep the applications that were approved to be submitted.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;employer_name&lt;/td&gt;
      &lt;td&gt;Employer through which the H1B application was submitted.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;prevailing_wage&lt;/td&gt;
      &lt;td&gt;The annual salary of the job in a particular observation.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;year&lt;/td&gt;
      &lt;td&gt;Year the particular H1B application was submitted.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;worksite&lt;/td&gt;
      &lt;td&gt;Location of the worksite.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;lon&lt;/td&gt;
      &lt;td&gt;Longitudinal location of the worksite.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;lat&lt;/td&gt;
      &lt;td&gt;Latitudinal location of the worksite.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;data_relation&lt;/td&gt;
      &lt;td&gt;If the job title is data-related, then this will have “data-related” else “undefined.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;data_job_title&lt;/td&gt;
      &lt;td&gt;This will be used to classify the data-related job titles into 4 different categories of business analysts, data analysts, data engineers, and data scientists.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;stem&lt;/td&gt;
      &lt;td&gt;This is supposed to classify the h1b application into STEM or non-STEM based on another dataset to recognize its classification.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;soc_code&lt;/td&gt;
      &lt;td&gt;SOC is a federal statistical standard used by federal agencies to classify workers into occupational categories to collect, calculate, or disseminate data.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;number-of-applications-per-state&quot;&gt;Number of Applications Per State&lt;/h3&gt;

&lt;p&gt;We wanted to look at how the number of applications differed per state over the course of the last 5 years.
Here we can see that the states of California, Texas, New York, Michigan, Georgia, Pennsylvania, Florida, and Illinois are the states with the highest number of applications. The number of applicants is seen to increase over time (from 2015 to 2017) for Washington, Virginia and North Carolina. States that have the lowest number of applicants are Montana, Wyoming and South Dakota.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;&quot;&gt;Interactive Map&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;stem&quot;&gt;STEM&lt;/h1&gt;

&lt;p&gt;Instead of doing an overview of the actual data, we thought it to be better to first filter by STEM (Science, Technology, Engineering, and Management) related jobs. It was expected that the majority of the applications would be coming from STEM, but a confirmation was needed.&lt;/p&gt;

&lt;p&gt;The following analysis was done to see the proportion of applicants for H1B that were in STEM fields vs non-STEM fields. This was done using the SOC (Standard Occupational Classification)  code associated with educational background. This was then cross-referenced (left-join) with a list of SOC codes that were considered to be STEM . We combined this with our analysis of the number of applicants per year as a sum just to see if some trends had changed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/x1tpYPZV/image010.png&quot; alt=&quot;stem-nonstem&quot; title=&quot;Figure 2: Number of applications per year. Proportion of STEM and non-STEM H1B Applicants.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We observed (from Figure. 2) that the number of applications stays high as compared to the yearly cap of 85,000 applications that get approved. The number of applications also keeps increasing each year with only a slight dip in 2017. The changes in the total number of applications don’t seem to affect the proportion of STEM to non-STEM applications each year. This is also very interesting to note as the number of non-STEM jobs available in the United States is much higher than the number of STEM jobs.&lt;/p&gt;

&lt;p&gt;This disproportionately high number might be attributed to the fact that foreign students with STEM degrees are more likely to pursue STEM fields. The reason for this is that USCIS allows foreign students who pursue a STEM degree during their academic career to stay within the United States for up to three years after they have graduated as opposed to a single year for those of non-STEM backgrounds. This means that during those 3 years, you will have three chances to apply for an H1B and would potentially be eligible to apply for permanent residency.&lt;/p&gt;

&lt;p&gt;Another reason why international students might want to pursue STEM would be the prevailing wages for STEM and non-STEM jobs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/j20Zv3DP/image012.png&quot; alt=&quot;&quot; title=&quot;Figure 3: Modified box plot to show only the spread and the median of the
wage distribution between STEM and non-STEM.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The medians of the prevailing wages of the STEM and non-STEM jobs have gotten wider apart but the median for the STEM is always higher than the non-STEM (Figure. 3). In 2018, we see the gap between the two to appear to be the widest.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/VN2RLZBp/image014.png&quot; alt=&quot;&quot; title=&quot;Figure 4: Density plot for the prevailing wages for STEM vs non-STEM jobs.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is visible from the density plot of the wage distribution of the year 2018 that the wages of the STEM jobs in 2018 are not only higher but the distribution is also tighter as compared to the distribution for the non-STEM jobs.&lt;/p&gt;

&lt;h3 id=&quot;top-jobs-in-stem-in-2018&quot;&gt;Top Jobs In STEM in 2018&lt;/h3&gt;

&lt;p&gt;What naturally followed after this was an analysis of what these STEM jobs were, to gain an understanding of why so many foreign students were attracted to them (Figure 5). Software developer and software engineer positions seem to be the top two jobs with the highest number of applications. This might be attributed to the tech boom of the last 5 years, which has greatly increased the demand for software developers.&lt;/p&gt;

&lt;p&gt;Following right after are business analyst and programmer analyst positions. It is important to note that approximately 10 out of these 20 jobs could be associated with data-related jobs. Even if they were not explicitly mentioned as a data-related job, they have, in their job description, data-related roles.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/HWw2d2Jv/image016.png&quot; alt=&quot;&quot; title=&quot;Figure 5: Top 20 jobs in STEM with the highest number of applications in 2018. All jobs are colored blue, data-related jobs are colored red.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After recognizing the number of data-related jobs within the top 20 job titles in the last year, the next step was to break down what these data-related roles were and if we could combine them and/or break them down based on how complex we wanted our analysis to be.&lt;/p&gt;

&lt;h1 id=&quot;data-related-jobs-trends&quot;&gt;Data Related Jobs Trends&lt;/h1&gt;

&lt;p&gt;We created four categories of data-related jobs for this analysis. This categorization was based on the keywords in the job titles.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Data Job Title&lt;/th&gt;
      &lt;th&gt;Associated Keywords&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Business Analyst&lt;/td&gt;
      &lt;td&gt;“Business Intelligence”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Data Analyst&lt;/td&gt;
      &lt;td&gt;“EDA”, “Visualization”,  “Data aggregation”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Data Scientist&lt;/td&gt;
      &lt;td&gt;“Machine learning”, “Model”, “Algorithm”, “A/B testing”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Data Engineer&lt;/td&gt;
      &lt;td&gt;“Pipeline”, “Data lake”, “ETL”, “Database”, “Warehouse”&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We combined machine learning and deep learning jobs into data science jobs, as they are highly correlated and it also made the following visualizations easier to look at. Since these data points were essentially just combined with another category, we need to note that the number of machine learning/deep learning jobs (with those explicit keywords as titles) have remained very few.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/N026V7q4/image020.png&quot; alt=&quot;&quot; title=&quot;Figure 6: Data related jobs and their four categories. Business Analysts having the highest number of applications and only going down after 2016.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Two important things to notice here (Figure 6). Firstly, the total number of jobs for business analysts is much higher than the total number of jobs for other roles. There also seems to be a downward trend in the number of jobs for business analysts after 2016; however, the number of jobs within business analytics remains much higher than the number of jobs in any of the other categories in any of the years. Secondly, the number of jobs for data scientists, analysts and engineers are all showing an upward trend. There appears to be a higher number of jobs for data analysts as compared to data scientists and data engineers. A reason for this is that the role of data scientists only emerged recently and the number of jobs available in the industry has just recently, in the past couple of years, started growing.&lt;/p&gt;

&lt;h3 id=&quot;prevailing-wages-per-data-related-job-category&quot;&gt;Prevailing Wages Per Data Related Job Category&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/520q5WsD/image022.png&quot; alt=&quot;&quot; title=&quot;Figure 7: Prevailing wages for data-related jobs in 2018&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the prevailing wages for the different categories that we have established, we see that the median salary (from Figure 7) for a data scientist is a lot higher for the median salary for the business analyst. This coincides with our earlier analysis when we saw that the number of applications for H1B is a lot higher than the number of job applications from a data scientist or analyst role. The higher number of jobs and lower salaries make it a good option for someone within data science to use that as an opportunity to get into the industry.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/qvzyhXRT/image024.png&quot; alt=&quot;&quot; title=&quot;Figure 8: Top 5 companies with the highest number of applications for data-related roles&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To get a general trend of what was going on in the industry for data-related roles, we look at five companies with the highest number of applications in the years 2014 to 2017 (Fig. 8). We notice here that despite expecting the highest number of applications to come from tech companies, it comes from 3 consulting firms and two tech companies.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/Y0MQh54n/image026.png&quot; alt=&quot;&quot; title=&quot;Figure 9: Infosys and the downward trend in job applications&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The company Infosys showed a very steep downward trend and also had the highest number of H1B applications in the year 2014 which was on a completely different scale as compared to other similar companies (Fig. 9). This drew the attention of our analysis because it was a highly abnormal trend, but upon deeper examination, it revealed a precautionary tale for foreign students trying to get jobs in the United States.&lt;/p&gt;

&lt;h3 id=&quot;story-behind-infosys&quot;&gt;Story Behind Infosys&lt;/h3&gt;

&lt;p&gt;In 2013, Infosys agreed to pay $34 million to settle a lawsuit against them which claimed they were involved in fraud and abuse of the immigration process in the United States . Infosys brought foreign nationals into the country on visa types that are not authorized for employment in the United States. This means that the company was involved in fraudulent activity. The USCIS has been stricter on companies like Infosys (Wipro, Tata) because they have a history of trying to go around the system and abusing it to employ cheap labor. The denial rate  for these companies has been extremely high and can be seen to effectively reduce the total number of applications submitted through them over the course of the last 5 years.&lt;/p&gt;

&lt;h3 id=&quot;top-tech-companies&quot;&gt;Top Tech Companies&lt;/h3&gt;

&lt;p&gt;A natural assumption for someone entering the market as a data scientist would be that the top tech companies have the highest number of jobs for data-related roles. We examine the top tech giants that are
&lt;img src=&quot;https://i.postimg.cc/HsL7R4by/image028.png&quot; alt=&quot;&quot; title=&quot;Figure 10: Data related job trends in the top tech companies in the past 5 years&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is a general upward trend to the number of data-related jobs for all tech companies, with Google hiring the least number of people within data-related capacities and Amazon hiring the most. While most companies are showing an upward trend, IBM shows a decreasing trend from 2015 to 2017.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/JhXXKrZY/image030.png&quot; alt=&quot;&quot; title=&quot;Microsoft&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/kXhtm6yh/image032.png&quot; alt=&quot;&quot; title=&quot;Google&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;a) Microsoft&lt;/td&gt;
      &lt;td&gt;b) Google&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/cJfKwDGV/image034.png&quot; alt=&quot;&quot; title=&quot;Facebook&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/Zn5Ck4vS/image036.png&quot; alt=&quot;&quot; title=&quot;Amazon &quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;c) Facebook&lt;/td&gt;
      &lt;td&gt;d) Amazon&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Figure 11: Trends in data some of the top tech companies&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All of the tech companies show an increasing trend in the data-related. However, there are two companies that draw our attention. Google (Fig 11 (b)) has a much lower number of data-related jobs as compared to the other companies. This might be attributed to the fact that some jobs might have data-related roles even if they don’t have data-related titles. But overall, Google has very few explicit data roles and thus fewer opportunities for international students.&lt;/p&gt;

&lt;p&gt;Another trend of interest is that of Amazon (Fig 11 (d)) which sees a decrease in the number of applications for data-related roles after the year 2017. This might be attributed to stricter H1B regulations implemented under Trump’s administration and also could be attributed to data scientist jobs doing jobs of data scientists.&lt;/p&gt;

&lt;h3 id=&quot;ibm&quot;&gt;IBM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/wv7jFFCq/image039.png&quot; alt=&quot;&quot; title=&quot;Figure 12: IBM trends for data-related roles - (a)IBM Spaghetti plot for all jobs&quot; /&gt;
&lt;img src=&quot;https://i.postimg.cc/g04WCRVx/image042.png&quot; alt=&quot;&quot; title=&quot;Figure 12: IBM trends for data-related roles - (b)Data-related roles for IBM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;IBM H1B petitions see an overall decrease after the year 2017 (Fig 12 (a)). This is because of how Trump’s administration has affected the H1B denial rates. We can see that  IBM has an increasing denial rate that is not like those of tech firms. This can be attributed to the fact that IBM is not a traditional tech firm; it provides a lot of consulting services. Due to a stricter review, the H1B application process involves providing more information about the type of exact work a company is involved in, the projects and the subcontractors. This hinders non-tech companies from hiring more international students as it will be both expensive and cumbersome. These companies would rather hire US citizens with the same skills if they can avoid it and most of the time, a foreign employee can be replaced with a US citizen.&lt;/p&gt;

&lt;p&gt;This downward trend is probably linked to IBM’s consulting sector combined with stricter government regulations . Despite this trend, the number of applications for specific data-related roles in IBM (Fig 12 (b)) seems to be unaffected by this downward trend.&lt;/p&gt;

&lt;h3 id=&quot;other-consulting-companies&quot;&gt;Other Consulting Companies&lt;/h3&gt;

&lt;p&gt;It would make sense that other consulting or non-tech companies should show trends similar to that of IBM with a decreasing number of applications. And this can be seen in both Deloitte and Accenture (Fig. 13 (a) and (b). Even though Deloitte has been a major employer for data-related roles in the past 5 years, it has shown a sharp decrease in these roles after 2016 (Fig 13 (a)). This decline started before President Trump’s election around 2015, when stricter regulations were implemented on working offsite on H1B.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/FFw7C5F2/image044.png&quot; alt=&quot;&quot; title=&quot;Deloitte&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/9FTzv8rz/image046.png&quot; alt=&quot;&quot; title=&quot;Accenture&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;a) Deloitte Consulting&lt;/td&gt;
      &lt;td&gt;b) Accenture Consulting&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Figure 13: Consulting companies and the trends in data-related roles&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The worksite regulations implemented in 2015 can cause problems for consulting companies as a lot of consulting work may be off-site in another city and not in the official workplace. Also, consulting companies that deal with U.S. Federal Government might not hire international students as the work might require the workers to be U.S. citizens.&lt;/p&gt;

&lt;h1 id=&quot;locations-for-data-related-jobs&quot;&gt;Locations for Data-Related Jobs&lt;/h1&gt;

&lt;h3 id=&quot;nationwide-overview&quot;&gt;Nationwide Overview&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/bvvyN3Nx/image049.png&quot; alt=&quot;&quot; title=&quot;Figure 14: Number of data jobs per state in 2018&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is an incomplete but interesting cartographic analysis (Fig. 14). The first thing we can see from here, however, is that the most number of applications in data-related roles are coming from (starting from the west coast to east) Washington, California, Texas, Illinois, Atlanta, Florida, New York, New Jersey, and Massachusetts.&lt;/p&gt;

&lt;p&gt;Just because the colors seem similar for certain states in this diagram, does not mean that the numbers of data-related H1B applications are the same. They belong to the same interval or above a certain threshold. The numbers for the major states are as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;California - 7,417 applications&lt;/li&gt;
  &lt;li&gt;New York - 2516 applications&lt;/li&gt;
  &lt;li&gt;Washington - 1428 applications&lt;/li&gt;
  &lt;li&gt;Texas - 2499 applications&lt;/li&gt;
  &lt;li&gt;New Jersey - 2007 applications&lt;/li&gt;
  &lt;li&gt;Massachusetts - 1570 applications&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Wyoming and Montana have 2 and 5 H1B applications, respectively,  in 2018. This indicates that these would be the worst states to apply for data-related jobs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/rFqvkzpW/image051.png&quot; alt=&quot;&quot; title=&quot;Figure 15: Alluvial diagram showing the distribution of the number of applications from each role &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Upon taking a deeper look at the different jobs and the top states for those jobs, we can see how the number of applications for the data-related roles is being distributed among the top states. What is interesting to note here is that the Business Analyst role is higher for almost all states except for the state of Michigan where the Data Analyst applications are higher than the Business Analyst applications. We can see the majority of Data Scientist applications are coming from California while Washington and New York follow right after.&lt;/p&gt;

&lt;h3 id=&quot;regions-of-the-united-states&quot;&gt;Regions of the United States&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/vTtknw0c/image053.png&quot; alt=&quot;&quot; title=&quot;Figure 16: 1 Point = 1 City. Color = Data position with the highest number of jobs in that city&quot; /&gt;
&lt;em&gt;Figure 16: 1 Point = 1 City. Color = Data position with the highest number of jobs in that city&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To find where would be best to apply based on the type of data role, a deeper analysis is needed into what cities would be best to apply to, based on the number of proportions of jobs available for each of the data roles. In Figure 16, each point is a city and the color of the city is given by what data-position provided the majority of the application in 2017 and 2018. Using two years for the analysis instead of one is better as it provides a better picture of the data-related jobs.  We can see lots of blue clusters forming on the east coast, while some black clusters forming in Michigan and more of a variety of colors in California in the Bay Area and Washington.&lt;/p&gt;

&lt;h3 id=&quot;the-western-united-states&quot;&gt;The Western United States&lt;/h3&gt;

&lt;p&gt;The Western United States has two main states of interest for data-related roles, Washington and California.
&lt;img src=&quot;https://i.postimg.cc/zG8D4qsP/image055.png&quot; alt=&quot;&quot; title=&quot;Figure 17: State of Washington and data-related roles in that state &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Washington has a significant amount of data-related roles with Seattle having a majority of Business Analyst applications and Redmond (red circle) has a higher number of data scientists. This may be because of the Amazon and Microsoft headquarters being located in Seattle and Redmond.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/85JTLcvs/image057.png&quot; alt=&quot;&quot; title=&quot;Figure 18: The state of California and data-related roles in that state&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If pursuing a data scientist role or job title, state of California seems to have the most variety and also the highest number of applications for H1B applications for that role (668 H1B applications). The big red circles show that a large number of data scientists applied for H1B from the bay area (San Jose, San Francisco, and Oakland). This makes complete sense because of the presence of Silicon Valley, where most of the headquarters for tech companies are located, and the recent boom in data science jobs in the tech industry (cite).&lt;/p&gt;

&lt;h3 id=&quot;the-northeastern-united-states&quot;&gt;The Northeastern United States&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/cJPpymK4/image059.png&quot; alt=&quot;&quot; title=&quot;Figure 19: The Northeastern States and data-related roles in those states&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see four main clusters on this map. Boston, New York, Philadelphia, and Washington DC. The majority of these clusters are blue as northeast has a relatively large number of finance and insurance companies (+ 22.7% relative as compared to other places in the United States) , and these companies have a large number of business analyst or data analytics positions as compared to other roles. The existence of some black clusters is an indication of this industry requiring data analysts.&lt;/p&gt;

&lt;h3 id=&quot;the-midwestern-united-states&quot;&gt;The Midwestern United States&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/wvJS6mJ5/image061.png&quot; alt=&quot;&quot; title=&quot;Figure 20: Midwestern states and data-related roles in those states&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The midwest shows relatively more black clusters as compared to any other region. There is a large black cluster in the state of Michigan indicating a significant number of data analyst positions, in Detroit. And a large blue cluster in Chicago indicating a large number of business analyst positions.&lt;/p&gt;

&lt;h3 id=&quot;state-of-tennessee&quot;&gt;State of Tennessee&lt;/h3&gt;

&lt;p&gt;The final and most important analysis, relevant to the authors of this paper and the potential individuals reading it, is an analysis of the data-related job market for international students, using H1B, in the state of Tennessee.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/zXjYbKQ0/image063.png&quot; alt=&quot;&quot; title=&quot;Figure 21: Data related jobs in the state of Tennessee &quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are two major cities where you find the biggest clusters for data-related roles: Nashville and Memphis. Both these cities have applications mainly for business analytics. We can see some sprinkles of data analyst roles in other parts of the state, with one small dot representing data science applications around the Oak Ridge area. This is from an employer named Oak Ridge National Laboratory and they hire data scientists for the work and research that they perform.&lt;/p&gt;

&lt;p&gt;This does not make Tennessee the best state for foreign students who are pursuing data science; however, growth in data science jobs is projected over the next few years, due to tech companies moving to Tennessee.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/wMYYJVHb/image065.png&quot; alt=&quot;&quot; title=&quot;Figure 13: Proportion of data-related jobs as compared to other jobs&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 13 shows that there has been a growth in data-related roles in the past 5 years, with 2018 showing the highest number of applications for data-related roles in the year 2018. This is promising, as it will mean a higher number of data-related roles that will hire and sponsor international students in the years to come.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;To reiterate, the purpose of this data exploration was to examine and recognize the trends in H1B visa applications and see how data-related jobs in the United States have changed over the past 5 years. This analysis can be used to assist international students in pursuing a data-related degree in determining what type of jobs they should apply for and where they should be considering moving to in the future if they wanted to stay within the United States.&lt;/p&gt;

&lt;p&gt;We can gather from our analysis that among the four data-related job titles that we divided our dataset into (business analyst, data analyst, data engineers and data scientists), the highest number of jobs are available for the business analyst job title or job classifications. We also note that the salary for this job role is much lower as compared to the other data-related jobs. The barrier to entry for data scientists, for this specific role, might be a lot lower as compared to other roles. This is because there are a lot of jobs for data scientists, and potential employers would be more open to hiring data scientists from business backgrounds, as they would be able to perform well in roles associated with business analytics.&lt;/p&gt;

&lt;p&gt;In terms of what type of company data scientists should apply for, we find that some big companies such as Google and Apple (shown in the final-report.Rmd), have fewer data-related jobs for international students despite being major tech companies. However, there does seem to be an upward trend in data-related jobs in the tech industry.&lt;/p&gt;

&lt;p&gt;We can also see those consulting companies are harder to get jobs in for data-related roles, as they are more affected by governmental regulations and policies. These positions may be limited to U.S. citizens or permanent residents. It is also cheaper for these companies to hire non-foreign workers, as consulting roles are not as profitable as purely technical roles and therefore may not need extremely specialized foreign talent (for those companies).&lt;/p&gt;

&lt;p&gt;The location analysis will help international students pursuing a data-related degree, to find the most probable location they should move to based on the type of data-related role they want to pursue. The best location to find a job in business analytics is in the northeastern states as they have a large number of finance and insurance companies. However, there is no shortage of business analytics roles, since these are very commonly found in almost every company. So almost every state has a large number of these roles. Nevertheless, it is still important to note that the total number of applications for H1B in business analytics is showing a generally downward trend.&lt;/p&gt;

&lt;p&gt;The best location to find a job as a data analyst or in data analytics in the Northeast or the Midwest, as seen from the clusters in the maps that we explored. However, since a large number of job applications come from California, almost all of these roles are plentiful in the state of California. California is also where a large number of specific data scientists and data engineering roles are available, specifically in the bay area where a large number of tech companies are located. The state of Washington (Seattle and Redmond area) is good to find data engineering and data science jobs as well because of the existence of Microsoft and Amazon.&lt;/p&gt;

&lt;p&gt;Finally, although Tennessee is not the best for international students pursuing data-related roles, there does seem to be a potential for growth since tech giants such as Amazon and Microsoft are creating more opportunities for jobs in the Nashville area in the future.&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Dec 2019 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2019/12/12/h1b-analysis/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/12/12/h1b-analysis/</guid>
        
        <category>H1B</category>
        
        <category>EDA</category>
        
        <category>Project</category>
        
        
      </item>
    
      <item>
        <title>Power and sample size calculations correlational studies</title>
        <description>&lt;p&gt;A common research objective is to demonstrate that two measurements are highly correlated. One measurement, call it A, may reflect the severity of disease but is difficult or costly to collect. Another measurement, call it B, may be easier to collect and potentially related to measurement A. If there is strong association between A and B, a cost effective strategy for diagnosis may be to collect measurement B instead of A.&lt;/p&gt;

&lt;p&gt;The researcher will collect both measurements on N individuals. The analysis will proceed by calculating a one-sided confidence interval. If the confidence interval is completely within the range from 0.8 to 1, then the researcher will consider the study to be a success: A conclusive demonstration that the correlation between A and B is greater than 0.8.&lt;/p&gt;

&lt;p&gt;Power is the probability that the study will end in success when the true underlying correlation is. The code below provides the power calculation for a single combination of N and population correlation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;set.seed(1122)
suppressPackageStartupMessages(require(mvtnorm))
N &amp;lt;- 50
rho &amp;lt;- .95
null_correlation &amp;lt;- 0.8
R &amp;lt;- 5000

sigma &amp;lt;- array(c(1,rho,rho,1), c(2,2))
mu &amp;lt;- c(0,0)

detect &amp;lt;- rep(NA, R)
for(i in 1:R){
  data &amp;lt;- rmvnorm(N, mean = mu, sigma = sigma)
  results &amp;lt;- cor.test(x = data[,1], y = data[,2], alternative = &quot;greater&quot;)
  detect[i] &amp;lt;- results$conf.int[1] &amp;gt; null_correlation
}
power &amp;lt;- mean(detect)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the simulation part, we need to use correlations from 0.8 to 0.95 and the sample size from 25 to 100. Now, let’s create a table to save the generated powers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;corr_list &amp;lt;-  seq(0.8,0.95,0.01)
N_list &amp;lt;- seq(25,100,25)
result &amp;lt;- expand.grid(corr=corr_list, N = N_list, power=NA)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And use a for loop to apply all of the correlations and sample size and calculate the powers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for (j in 1:nrow(result)){
  N &amp;lt;- result[j,2]    #50
  rho &amp;lt;- result[j,1]  #.8
  null_correlation &amp;lt;- 0.8
  R &amp;lt;- 5000

  sigma &amp;lt;- array(c(1,rho,rho,1), c(2,2))
  mu &amp;lt;- c(0,0)

  detect &amp;lt;- rep(NA, R)
  for(i in 1:R){
    data &amp;lt;- rmvnorm(N, mean = mu, sigma = sigma)
    results &amp;lt;- cor.test(x = data[,1], y = data[,2], alternative = &quot;greater&quot;)
    detect[i] &amp;lt;- results$conf.int[1] &amp;gt; null_correlation
  }
  power &amp;lt;- mean(detect)
  result[j,3] &amp;lt;- power
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Transform the table into data frame type and plot the graph.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;result_df &amp;lt;- as.data.frame(result) %&amp;gt;%
  mutate(N=factor(N))

result_df %&amp;gt;%
  ggplot()+
  geom_line(aes(x=corr,y=power,color=N),size=1)+
  theme_bw()+
  labs(x=&quot;Correlation&quot;,y=&quot;Power&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/02c64Qkt/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, with the N increasing, the power under the certain correlation is growing up. In other words, when we can collect more samples, the probability that the study will end in success will be higher. And definitely, when the real correlation is higher, the probability will be higher.&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Nov 2019 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2019/11/22/power-correlation/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/22/power-correlation/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        <category>Correlation</category>
        
        
      </item>
    
      <item>
        <title>Central Limite Theorem - Approximation</title>
        <description>&lt;p&gt;Import the packages first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(magrittr)
library(sn)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;situation-description&quot;&gt;Situation Description&lt;/h1&gt;

&lt;p&gt;The central limit theorem is an important computational short-cut for generating and making inference from the sampling distribution of the mean. I will recall that the central limit theorem short-cut relies on a
number of conditions, specifically:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Independent observations&lt;/li&gt;
  &lt;li&gt;Identically distributed observations&lt;/li&gt;
  &lt;li&gt;Mean and variance exist&lt;/li&gt;
  &lt;li&gt;Sample size large enough for convergence&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this simulation study, I.m going to compare the sampling distribution of the mean generated by simulation to the sampling distribution implied by the central limit theorem. I will compare the distributions graphically in QQ-plots.&lt;/p&gt;

&lt;p&gt;This will be a 4 × 4 factorial experiment. The first factor will be the sample size, with N = 5, 10, 20, and 40. The second factor will be the degree of skewness in the underlying distribution. The underlying distribution will be the Skew-Normal distribution. The Skew-Normal distribution has three parameters: location
&lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt;, scale
&lt;script type=&quot;math/tex&quot;&gt;\omega&lt;/script&gt;, and slant
&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;. When the slant parameter is 0, the distribution reverts to the normal distribution. As the slant parameter increases, the distribution becomes increasingly skewed. In this simulation, slant will be set to 0, 2, 10, 100. Set location and scale to 0 and 1, respectively, for all simulation settings.&lt;/p&gt;

&lt;h1 id=&quot;plot-preparation&quot;&gt;Plot preparation&lt;/h1&gt;

&lt;p&gt;In the very beginning, we need to set up the parameters that do not change in the following steps. The slant of Skew-Normal distribution will change later, therefore, only the location $\xi$ and scale $\omega$ will be set in this part.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;R &amp;lt;- 5000
location &amp;lt;- 0
scale &amp;lt;- 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;location:$\xi$, scale:$\omega$, slant:$\alpha$&lt;/p&gt;

&lt;p&gt;Before creating the function, let’s clarify the functions for calculate the delta, mean and standard deviation for central limit theorem (CLT).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Delta:
&lt;script type=&quot;math/tex&quot;&gt;\delta =\frac{\alpha}{\sqrt{1+\alpha^2}}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mean:
&lt;script type=&quot;math/tex&quot;&gt;\xi+\omega \delta \sqrt{\frac{2}{\pi}}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Standard deviation:
&lt;script type=&quot;math/tex&quot;&gt;\sqrt{\omega^2(1-\frac{2\delta^2}{\pi})  }&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then define the function for CLT process and generating the QQplots by using the functions before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;qqplot_creator &amp;lt;- function(slant, N) {
  delta &amp;lt;- slant / (sqrt(1 + slant ^ 2))

  # Quantites to calculate/generate
  pop_mean &amp;lt;- location + scale * delta * (sqrt(2 / pi))
  pop_sd &amp;lt;- sqrt(scale ^ 2 * (1 - ((2 * delta ^ 2) / pi)))

  Z &amp;lt;- rnorm(R) # generate the normal distribution as the basement

  #CLT approximation
  sample_dist_clt &amp;lt;- Z * (pop_sd / sqrt(N)) + pop_mean

  #Simulation approximation
  random.skew &amp;lt;- array(rsn(R * N, xi = location, omega = scale, alpha = slant),
                      dim = c(R, N))

  sample_dist_sim &amp;lt;- apply(random.skew, 1, mean)

  qqplot(sample_dist_clt, sample_dist_sim, axes = FALSE, frame.plot=TRUE, ann = FALSE)
  abline(0,1)

  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;qqplot-generation&quot;&gt;QQplot generation&lt;/h1&gt;

&lt;p&gt;Now we can set the slants and Ns we want to test in the following steps. As the requirement, the N = 5, 10, 20, and 40 and slant will be set to 0, 2, 10, 100. Then create a sequence to define the points where we want to test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;slant &amp;lt;- c(0,2,10,100)
N &amp;lt;- c(5,10,20,40)
x &amp;lt;- seq(-2,2,0.01)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set a graph for put all of the QQplots together and use the qqplot_creator function to fill the QQplots inside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;par(mfrow=c(4,5),mai=c(0.1,0.1,0.1,0.1), oma = c(0, 4, 4, 0))

for(i in slant){
  plot(dsn(x,
           xi = location,
           omega = scale,
           alpha = i),
       axes = FALSE,
       frame.plot=TRUE,
       type = &quot;l&quot;,
       xlab = NA, ylab = NA)
  for(j in N){
    qqplot_creator(i, j)
  }
}
mtext(text=&quot;Distribution              N=5                N=10                   N=20                    N=40&quot;,
      side = 3,
      outer = TRUE)
mtext(text=&quot;slant = 100   slant = 10       slant = 2      slant = 0&quot;,
      side = 2,
      outer = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/rwCbSBq2/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Definitely, when the N is bigger, the QQplot will fit the y=x line better, which means the CLT works better when it wants to simulate the distribution. And when the slant is bigger, in other words, the Skew-Normal distribution has higher skewerness, it will be more difficult to simulate the distribution.&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Nov 2019 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2019/11/12/clt/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/12/clt/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        <category>CLT</category>
        
        
      </item>
    
      <item>
        <title>Simulation method comparison</title>
        <description>&lt;h1 id=&quot;situation-description&quot;&gt;Situation description&lt;/h1&gt;

&lt;p&gt;This time, I will perform a 2 × 4 × 2 factorial simulation study to compare the coverage probability of various methods of calculating &lt;strong&gt;90%&lt;/strong&gt; confidence intervals. The three factors in the experiment are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;True, underlying distribution
    &lt;ul&gt;
      &lt;li&gt;standard normal&lt;/li&gt;
      &lt;li&gt;gamma(shape = 1.4, scale = 3)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model
    &lt;ul&gt;
      &lt;li&gt;method of moments with normal&lt;/li&gt;
      &lt;li&gt;method of moments with gamma&lt;/li&gt;
      &lt;li&gt;kernel density estimation&lt;/li&gt;
      &lt;li&gt;bootstrap&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Parameter of interest
    &lt;ul&gt;
      &lt;li&gt;sample min (1st order statistic)&lt;/li&gt;
      &lt;li&gt;median
Other settings in the experiment that will not change are:
    -   Sample size, &lt;em&gt;N&lt;/em&gt; = 201
    -   &lt;em&gt;Outside the loop&lt;/em&gt; estimation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;define-functions&quot;&gt;Define functions&lt;/h1&gt;

&lt;h2 id=&quot;data-generation-function&quot;&gt;Data generation function&lt;/h2&gt;

&lt;p&gt;Set up data generate function what can generate data base on normal distribution or gamma distribution. The distribution will be defined by the parameter “dist”. The default parameter of gamma distribution are shape=1.4 and scale=3.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;generate_data &amp;lt;- function(N, dist, sh=1.4, sc=3){
  if(dist==&quot;norm&quot;){
    rnorm(N)
  }else if(dist==&quot;gamma&quot;){
    rgamma(N, shape=sh, scale=sc)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;confidence-interval-estimation-function&quot;&gt;Confidence interval estimation function&lt;/h2&gt;

&lt;p&gt;Then define the function for calculate the confidence interval, which can estimate the distribution by method of moment with normal distribution, method of moment with gamma distribution, kernal density distribution and bootstrap. At the same time, the function can use the function defined by “par.int” to calculate the parameter we want. The details will be introduced in the chunk.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;estimate.ci &amp;lt;- function(data, mod, par.int, R=10, smoo=0.3){
  # data: input data
  # mod: define the estimation method and the distribution.
  #      (MMnorm: method of moment with normal distribution,
  #       MMgamma: method of moment with gamma distribution,
  #       KDE: kernal density distribution,
  #       Boot: bootstrap)
  # par.int: the function what can get the parameter we want for the estimated distribution

  N &amp;lt;- length(data) # save N as the number of the inputted data
  sum.measure &amp;lt;- get(par.int) # make charactor can be used as function


  if(mod==&quot;MMnorm&quot;){
    # use method of moment with normal distribution to estimate

    mm.mean &amp;lt;- mean(data) # get the original mean of data
    mm.sd &amp;lt;- sd(data) # get the original standard deviation of data

    samp.dist &amp;lt;- NA #
    for(i in 1:R){
      sim.data &amp;lt;-rnorm(length(data), mm.mean, mm.sd)
      # use the mean &amp;amp; sd of data to generate a new normal distribution
      if(par.int==&quot;median&quot;){
        samp.dist[i] &amp;lt;- median(sim.data)
        # save the median of generated distribution
      }else if(par.int==&quot;min&quot;){
        samp.dist[i] &amp;lt;- min(sim.data)
        # save the min of generated distribution
      }
    }
    return(quantile(samp.dist, c(0.05, 0.95)))
    # the output will be the 95% confidence interval of needed distribution

  }else if(mod==&quot;MMgamma&quot;){
    # use method of moment with gamma distribution to estimate

    mm.shape &amp;lt;- mean(data)^2/var(data)
    # get the original shape parameter of data
    mm.scale &amp;lt;- var(data)/mean(data)
    # get the original scale parameter of data

    #create a N*R array to reserve the gamma distributions
    sim.data &amp;lt;- array(rgamma(length(data)*R, shape=mm.shape, scale=mm.scale), dim=c(N, R))
    # use the function called by par.int for each column and reserve the results as a distribution
    samp.dist &amp;lt;- apply(sim.data, 2, FUN=sum.measure)
    return(quantile(samp.dist, c(0.05, 0.95)))
    # the output will be the 95% confidence interval of needed distribution

  }else if(mod==&quot;KDE&quot;){
    # use kernal density estimation

    ecdfstar &amp;lt;- function(t, data, smooth=smoo){
        # use pnorm function which has sd=smoo on the outer product of t and data.
        # t[i] is the quantile, data[j] is the mean for pnorm.
        # in other words, calculate the persentage under quantile &quot;t&quot; of distribution &quot;data&quot;
        outer(t, data, function(a,b){ pnorm(a, b, smooth)}) %&amp;gt;%
        rowMeans
        # then count the mean of each row, AKA, the mean persentage under quantile &quot;t&quot;
    }

    # create a 1 column data frame with a sequence from min-sd to max+sd of the data with break=0.01
    tbl &amp;lt;- data.frame(
        x = seq(min(data)-sd(data), max(data)+sd(data),
                by = 0.01)
    )

    tbl$p &amp;lt;- ecdfstar(tbl$x, data, smoo)
    # add a new column called p reserve the result of ecdfstar function
    tbl &amp;lt;- tbl[!duplicated(tbl$p),]
    # remove all of rows which contain the duplicated value in column &quot;p&quot;.

    qkde &amp;lt;- function(ps, tbl){
      # convert &quot;ps&quot; to a factor with the break as column &quot;p&quot; in &quot;tbl&quot; data frame
      rows &amp;lt;- cut(ps, tbl$p, labels = FALSE)
      tbl[rows, &quot;x&quot;]
      # return the part of column &quot;x&quot; which match the rows number factor
    }

    U &amp;lt;- runif(N*R) # create a uniform distribution with the size=N*R

    # reserve the result of qkde as a N*R matrix
    sim.data &amp;lt;- array(qkde(U,tbl), dim=c(N, R))
    # use the function called by par.int for each column and reserve the results as a distribution
    samp.dist &amp;lt;- apply(sim.data, 2, sum.measure)

    return(quantile(samp.dist, c(0.05, 0.95), na.rm=TRUE))
    # the output will be the 95% confidence interval

  }else if(mod==&quot;Boot&quot;){
    # use bootstrap

    # get random sample with size=N from the data R times, reserve the result as a N*R matrix
    sim.data &amp;lt;- array(sample(data, N*R, replace=TRUE), dim=c(N,R))
    # use the function called by par.int for each column and reserve the results as a distribution
    samp.dist &amp;lt;- apply(sim.data, 2, sum.measure)

    return(quantile(samp.dist, c(0.05, 0.95), na.rm=TRUE))
    # the output will be the 95% confidence interval
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;destination-capture-function&quot;&gt;Destination capture function&lt;/h2&gt;

&lt;p&gt;Create a function to justify that is the confidence interval match the requirement. When the result is TRUE, return 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;capture_par &amp;lt;- function(ci, true.par){
  1*(ci[1] &amp;lt; true.par &amp;amp; true.par &amp;lt; ci[2])
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;simulation&quot;&gt;Simulation&lt;/h1&gt;

&lt;h2 id=&quot;calculation-prepare&quot;&gt;Calculation prepare&lt;/h2&gt;

&lt;p&gt;Now we can set the size of distribution N is 201. When use gamma distribution, we will use the shape 1.4 and scale 3.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;N &amp;lt;- 201
shape.set &amp;lt;- 1.4
scale.set &amp;lt;- 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Define the capture destinations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;true.norm.med &amp;lt;- qnorm(0.5)
true.norm.min &amp;lt;- mean(apply(array(rnorm(N*10000), dim=c(N, 10000)),2,min))
true.gamma.med &amp;lt;- qgamma(0.5, shape = shape.set, scale=scale.set)
true.gamma.min &amp;lt;- mean(apply(array(rgamma(N*10000, shape=shape.set, scale=scale.set), dim=c(N, 10000)),2,min))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the standard min of distribution part, we expand the data size for 10000 times and get the mean min value as the standard min.&lt;/p&gt;

&lt;p&gt;Create a table called “simsettings” to reserve the results of each estimation method and the target parameter.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;simsettings &amp;lt;- expand.grid(dist=c(&quot;norm&quot;, &quot;gamma&quot;), model=c(&quot;MMnorm&quot;, &quot;MMgamma&quot;, &quot;KDE&quot;, &quot;Boot&quot;), par.int=c(&quot;median&quot;, &quot;min&quot;), cov.prob=NA,  stringsAsFactors = FALSE, KEEP.OUT.ATTRS = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add a new column to rserve the capture destinations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;simsettings$truth &amp;lt;- c(true.norm.med, true.gamma.med, true.norm.med, true.gamma.med, true.norm.med, true.gamma.med, true.norm.med, true.gamma.med, true.norm.min, true.gamma.min, true.norm.min, true.gamma.min, true.norm.min, true.gamma.min, true.norm.min, true.gamma.min)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;calculation&quot;&gt;Calculation&lt;/h2&gt;

&lt;p&gt;Base on the “simsettings” table, calculate the capture probabilities and reserve them in the table.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(k in c(1:2,4:10,12:16)){
  dist1 &amp;lt;- simsettings[k,1]
  model1 &amp;lt;- simsettings[k,2]
  par.int1 &amp;lt;- simsettings[k,3]
  true.par1 &amp;lt;- simsettings[k,5]

  cover &amp;lt;- NA
  for(sims in 1:100){
    cover[sims] &amp;lt;- generate_data(N, dist1) %&amp;gt;%
    estimate.ci(mod=model1, par.int=par.int1, R=1000) %&amp;gt;%
    capture_par(true.par = true.par1)
  }
  simsettings[k,4] &amp;lt;- mean(cover)
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Show the result table.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;simsettings
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;##     dist   model par.int cov.prob       truth
## 1   norm  MMnorm  median     0.99  0.00000000
## 2  gamma  MMnorm  median     0.01  3.25311453
## 3   norm MMgamma  median       NA  0.00000000
## 4  gamma MMgamma  median     0.97  3.25311453
## 5   norm     KDE  median     0.94  0.00000000
## 6  gamma     KDE  median     0.93  3.25311453
## 7   norm    Boot  median     0.94  0.00000000
## 8  gamma    Boot  median     0.90  3.25311453
## 9   norm  MMnorm     min     1.00 -2.74462674
## 10 gamma  MMnorm     min     0.00  0.07277036
## 11  norm MMgamma     min       NA -2.74462674
## 12 gamma MMgamma     min     0.99  0.07277036
## 13  norm     KDE     min     0.94 -2.74462674
## 14 gamma     KDE     min     0.35  0.07277036
## 15  norm    Boot     min     0.40 -2.74462674
## 16 gamma    Boot     min     0.57  0.07277036
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Definitely, for the method of moment part, when the data distribution matches the model distribution (e.g. norm - MMnorm, gamma - MMgamma), the capture probabilities are more than 90%. When the distributions do not match with each other, the results will be close to 0% or NA.&lt;/p&gt;

&lt;p&gt;And for kernal density estimation, when we need to get the median of the distributions, the results are pretty good as close to 100%. But when we need to get the min of the distributions, the capture probabilities of normal and gamma are really different. For the normal distribution it works well, but for gamma distribution it is about 37%. The reason might be that gamma distribution is a skewed distribution.&lt;/p&gt;

&lt;p&gt;For bootstrap part, the results are not as good as others. When we want to get the median of the distributions, the capture probabilities are just about 90%. And when it comes to min of the distributions, the probabilities become 50%. Obviously, it is because selecting a sample randomly is too difficult to get the min point every time.&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/10/27/simulation-compare/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/10/27/simulation-compare/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        <category>Method of Moments</category>
        
        <category>KDE</category>
        
        <category>Bootstrap</category>
        
        
      </item>
    
      <item>
        <title>Coverage probability of MLE</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.
&lt;em&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;Wikipedia - Maximum likelihood estimation&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Coverage probability is an important operating characteristic of methods for constructing interval estimates, particularly confidence intervals.&lt;/p&gt;

&lt;p&gt;The destination of this blog is that performing a simulation to calculate the coverage probability of the 95% confidence interval of the median when computed from F&lt;sub&gt;X&lt;/sub&gt;&lt;sup&gt;mle&lt;/sup&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(stats4)
library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;step-1-single-sample&quot;&gt;Step 1: Single sample&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Generate a single sample from a standard normal distribution of size N = 201.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Fisrt, let’s set the size of distribution (N) as 201, and create a standard normal distribution as sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;N &amp;lt;- 201
sample1 &amp;lt;- rnorm(201)
median(sample1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then use the MLE function we learned before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;nll &amp;lt;- function(mean, sd) {
  fs &amp;lt;- dnorm(sample1,
              mean = mean,
              sd = sd,
              log = TRUE)
  - sum(fs, na.rm = T)
}

fit &amp;lt;- stats4::mle(
  nll,
  start = list(mean = 0, sd = 1),
  method = &quot;L-BFGS-B&quot;,
  lower = c(0, 0.01)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By the result of MLE, we can get the estimated mean and standard deviation of the sample distribution before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;coef(fit)
sample1_mean &amp;lt;- coef(fit)[[1]]
sample1_sd &amp;lt;- coef(fit)[[2]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated mean is &lt;code class=&quot;highlighter-rouge&quot;&gt;r sample1_mean&lt;/code&gt; and the estimated standard deviation is &lt;code class=&quot;highlighter-rouge&quot;&gt;r sample1_sd&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;step-2-approximate-median-distribution&quot;&gt;Step 2: Approximate median distribution&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Approximate the sampling distribution of the median, conditional on the estimate of the distribution in the previous step.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the previous step, we get a pair of estimated mean and standard deviation. Based on the parameter, we can generate new distributions to simulate the original distribution.&lt;/p&gt;

&lt;p&gt;Now we need to get the median of the new distribution, and if repeat the process for lots of time, we can get the distribution of median of the estimated distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;median_list &amp;lt;- rep(NA, 500)
for (i in seq_along(median_list)) {
  median_list[i] &amp;lt;-
    median(rnorm(N, mean = sample1_mean, sd = sample1_sd))
}
#hist(median_list,breaks = 100)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;step-3-calculate-95-ci&quot;&gt;Step 3: Calculate 95% CI&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Calculate a 95% confidence interval from the approximated sampling distribution.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For the median distribution we got in the previous step, we can use &lt;em&gt;quantile()&lt;/em&gt; function to get the 95% confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sample1_quantile95 &amp;lt;- quantile(median_list, c(0.025, 0.975))
sample1_quantile95
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;step-4-calculate-coverage-probability&quot;&gt;Step 4: Calculate coverage probability&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Now we will explain the concept of coverage probability. And calculating the coverage probability.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First, create a matrix to save the distributions we are going to generate. Every column is one distribution. Than use the method in the previous steps, generate all of the distributions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;samples &amp;lt;- matrix(nrow = N, ncol = 1000)
for (i in 1:ncol(samples)) {
  samples[, i] &amp;lt;- rnorm(N)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create a matrix to save the estimated parameters of each distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;coef &amp;lt;- matrix(nrow = 2, ncol = ncol(samples))
for (i in 1:ncol(samples)) {

  coef[1, i] &amp;lt;- mean(samples[, i]) #coef(fit)[[1]]
  coef[2, i] &amp;lt;- sd(samples[, i]) #coef(fit)[[2]]

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create a matrix to save 95% confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;quantile95_list &amp;lt;- matrix(nrow = 2, ncol = ncol(samples))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each pair of estimated parameters, create a eatimated distribution, than get the median number. Than also do the process for &lt;code class=&quot;highlighter-rouge&quot;&gt;r N&lt;/code&gt; times, get the 95% confidence interval and save them in the matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for (j in 1:ncol(samples)) {
  median_list &amp;lt;- rep(NA, N)
  for (i in seq_along(median_list)) {
    norm_dist &amp;lt;- rnorm(N, mean = coef[1, j], sd = coef[2, j])
    median_list[i] &amp;lt;- median(norm_dist)
  }
  quantile95 &amp;lt;- quantile(median_list, c(0.025, 0.975))
  quantile95_list[1, j] &amp;lt;- quantile95[[1]]
  quantile95_list[2, j] &amp;lt;- quantile95[[2]]

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For calculating the coverage probability, we can save all of the results of judgement in a vector. Definitely, the coverage probability is the number of successful capture divede by the number of distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;interest &amp;lt;- rep(NA, ncol(samples))
for (i in seq_along(interest)) {
  interest[i] &amp;lt;-
    case_when(
      quantile95_list[1, i] &amp;gt; 0 | quantile95_list[2, i] &amp;lt; 0 ~ 0,
      quantile95_list[1, i] &amp;lt;= 0 &amp;amp;
        quantile95_list[2, i] &amp;gt;= 0 ~ 1,
    )
}
coverage_probability &amp;lt;- sum(interest) / length(interest)
sum(interest)
coverage_probability
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, the coverage probability is &lt;code class=&quot;highlighter-rouge&quot;&gt;r coverage_probability&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sum(quantile95_list[2,] &amp;lt; 0)
sum(quantile95_list[1,] &amp;gt; 0)

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;step-5-simulation-visualization&quot;&gt;Step 5: Simulation visualization&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Perform the simulation.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to make the results more intuitive, we can use &lt;em&gt;geom_linerange()&lt;/em&gt; function in &lt;em&gt;ggplot&lt;/em&gt; to draw the 95% confidence interval. And set the 95% confidence interval which did not capture the real mean of original distribution, “0” as a red line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;quantile95_df &amp;lt;- as.data.frame(t(quantile95_list))
quantile95_df[&quot;sample_num&quot;] &amp;lt;- seq(1:ncol(samples))
quantile95_df[&quot;interest&quot;] &amp;lt;- interest
ggplot(quantile95_df) +
  geom_linerange(aes(x = sample_num,
                     ymin = V1,
                     ymax = V2),
                 color = case_when(interest == 1 ~ &quot;blue&quot;,
                                   interest == 0 ~ &quot;red&quot;)) +
  geom_hline(yintercept = 0,
             color = &quot;gray&quot;,
             alpha = 0.7) +
  coord_flip() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/9MPrGHm9/prob7-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;step-6-future-works&quot;&gt;Step 6: Future works&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;How to change the simulation to learn more about the operating characteristics of the chosen method for constructing the 95% confidence interval.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I think I will choose to use bigger distribution to test the process, likes set the &lt;em&gt;N&lt;/em&gt; as 5000 or 10000. And I could calculate the other quantile of the distributions to find out is there any different. After that, maybe I will use other distribution but not standard normal distribution, such as gamma or beta distribution.&lt;/p&gt;
</description>
        <pubDate>Tue, 15 Oct 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/10/15/coverage-probability/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/10/15/coverage-probability/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        <category>MLE</category>
        
        
      </item>
    
      <item>
        <title>Which quantiles of a continuous distribution can one estimate with more precision?</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;The median is an important quantity in data analysis. It represents the middle value of the data distribution. Estimates of the median, however, have a degree of uncertainty because (a) the estimates are calculated from a finite sample and (b) the data distribution of the underlying data is generally unknown. One important roles of a data scientist is to quantify and to communicate the degree of uncertainty in his or her data analysis.&lt;/p&gt;

&lt;p&gt;This time, we are going to find out what can make our sample distribution more precise during the simulation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(tidyverse)
library(reshape2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;standard-normal-distribution&quot;&gt;Standard Normal Distribution&lt;/h1&gt;

&lt;p&gt;First, we need to set up the initial parameters. For every simulation process, we will generate 200 values base on the standard normal distribution, and we will generate them for 5000 times.&lt;/p&gt;

&lt;p&gt;Therefore, set the sample size as 200, test number is 5000.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sample_size &amp;lt;- 200
test_num &amp;lt;- 5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then let’s create a function for find out the quantile values of each test and output them as sequences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;find_quantile_value &amp;lt;- function(fn,sample_size, quantile_position){
  sample &amp;lt;- fn(sample_size)
  qtseq &amp;lt;- quantile(sample, seq(0.05,0.95,0.05))
  qtseq[[quantile_position]]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to store the values of each quantile, we can create a data frame. For there are 19 quantiles and 5000 tests, the data frame should be a 5000*19 table.
Then use for loop traverse each test and each quantile.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sml_results &amp;lt;- as.data.frame(matrix(NA, nrow = test_num, ncol = 19))

for(i in 1:nrow(sml_results)){
  for(q in 1:ncol(sml_results)){
    sml_results[i,q] &amp;lt;- find_quantile_value(rnorm,sample_size, q)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we need to find the middle 95% length of the distributions of each qutaile value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_list &amp;lt;- rep(NA,ncol(sml_results))
for(q in 1:ncol(sml_results)){
  quantile_2.5_97.5&amp;lt;- quantile(sml_results[,q], c(0.025, 0.975))
  length_list[q] &amp;lt;- quantile_2.5_97.5[[2]]-quantile_2.5_97.5[[1]]
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the data we got before into a data frame and plot them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_df &amp;lt;- as.data.frame(matrix(ncol = 2, nrow = length(length_list)))
names(length_df) &amp;lt;- c(&quot;quantile_position&quot;,&quot;mid95_length&quot;)
length_df[,1] &amp;lt;- seq(0.05,0.95,0.05)
length_df[,2] &amp;lt;- length_list
ggplot(length_df,aes(x=quantile_position,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of normal distribution&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )+
  scale_y_continuous(name = &quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/J0jRCrkY/prob5-s1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;as we can see, when the quantile is approaching 50%, the length is going lower. It means that when the quantile is approaching 50%, simulation error is lower. It means when the quantile is 50%, it has the best precision.&lt;/p&gt;

&lt;p&gt;In other words, when the quantile is 50%, the median have the tightest sampling distribution.&lt;/p&gt;

&lt;p&gt;Then, we need to transfer the x-axis from quantile to the density values of the original distribution. In this part, it should be standard normal distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_d_df &amp;lt;- length_df %&amp;gt;%
  mutate(density=dnorm(qnorm(quantile_position)))

ggplot(length_d_df, aes(x=density,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of normal distribution by density&quot;, x= &quot;Density&quot;,y=&quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/9QLChND2/prob5-s2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this graph, we can find when density is bigger, the simulation error is lower.&lt;/p&gt;

&lt;h1 id=&quot;exponential-distribution&quot;&gt;Exponential Distribution&lt;/h1&gt;

&lt;p&gt;Distribution 2 is the exponential distribution with rate = 1.&lt;/p&gt;

&lt;p&gt;As we did before, calculate the values of each quantile and each test then plot the graph.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;
for(i in 1:nrow(sml_results)){
  for(q in 1:ncol(sml_results)){
    sml_results[i,q] &amp;lt;- find_quantile_value(rexp,sample_size, q)
  }
}

length_list &amp;lt;- rep(NA,ncol(sml_results))
for(q in 1:ncol(sml_results)){
  quantile_2.5_97.5&amp;lt;- quantile(sml_results[,q], c(0.025, 0.975))
  length_list[q] &amp;lt;- quantile_2.5_97.5[[2]]-quantile_2.5_97.5[[1]]
}

length_df &amp;lt;- as.data.frame(matrix(ncol = 2, nrow = length(length_list)))
names(length_df) &amp;lt;- c(&quot;quantile_position&quot;,&quot;mid95_length&quot;)
length_df[,1] &amp;lt;- seq(0.05,0.95,0.05)
length_df[,2] &amp;lt;- length_list
ggplot(length_df,aes(x=quantile_position,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of exponential distribution&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )+
  scale_y_continuous(name = &quot;Length&quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/cCtNMsh4/prob5-e1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For this exponential distribution, when quantile is bigger, the error of simulation is bigger.&lt;/p&gt;

&lt;p&gt;In other words, when the quantile is 5%, the median have the tightest sampling distribution.&lt;/p&gt;

&lt;p&gt;And when the Density is higher, the error is smaller.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_d_df &amp;lt;- length_df %&amp;gt;%
  mutate(density=dexp(qexp(quantile_position)))

ggplot(length_d_df, aes(x=density,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of exponential distribution by density&quot;, x= &quot;Density&quot;,y=&quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/GmdmryzF/prob5-e2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mixture-distribution-3&quot;&gt;Mixture Distribution 3&lt;/h1&gt;

&lt;p&gt;Distribution 3 is a mixture distribution defined by these functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;rf3 &amp;lt;- function(N){
  G &amp;lt;- sample(0:2, N, replace = TRUE, prob = c(5,3,2))
  (G==0)*rnorm(N) + (G==1)*rnorm(N,4) + (G==2)*rnorm(N,-4,2)
}

pf3 &amp;lt;- function(x){
  .5*pnorm(x) + .3*pnorm(x,4) + .2*pnorm(x,-4,2)
}

df3 &amp;lt;- function(x){
  .5*dnorm(x) + .3*dnorm(x,4) + .2*dnorm(x,-4,2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And just like we did in previous chunks, plot the middle 95% length of the quantiles.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(i in 1:nrow(sml_results)){
  for(q in 1:ncol(sml_results)){
    sml_results[i,q] &amp;lt;- find_quantile_value(rf3,sample_size, q)
  }
}

length_list &amp;lt;- rep(NA,ncol(sml_results))
for(q in 1:ncol(sml_results)){
  quantile_2.5_97.5&amp;lt;- quantile(sml_results[,q], c(0.025, 0.975))
  length_list[q] &amp;lt;- quantile_2.5_97.5[[2]]-quantile_2.5_97.5[[1]]
}

length_df &amp;lt;- as.data.frame(matrix(ncol = 2, nrow = length(length_list)))
names(length_df) &amp;lt;- c(&quot;quantile_position&quot;,&quot;mid95_length&quot;)
length_df[,1] &amp;lt;- seq(0.05,0.95,0.05)
length_df[,2] &amp;lt;- length_list
ggplot(length_df,aes(x=quantile_position,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of given mixture distribution 3&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )+
  scale_y_continuous(name = &quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/rwjFpwYv/prob5-m31.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Definitely, when the quantile is 40%, the median have the tightest sampling distribution.&lt;/p&gt;

&lt;p&gt;We don’t have the qf3 function, therefore we need to use pf3 function calculate the values when their counterpart quantile is setted. And the other part is same as previous chunks.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;pf3 &amp;lt;- function(x){
  .5*pnorm(x) + .3*pnorm(x,4) + .2*pnorm(x,-4,2)
}

quantile_seq &amp;lt;- seq(0.05,0.95,0.05)
qf3_seq &amp;lt;- rep(NA, length(quantile_seq))
for(i in seq_along(quantile_seq)){
qf3_seq[i] &amp;lt;- uniroot(function(x){pf3(x)-quantile_seq[i]}, c(-100,100))[[1]]
}

length_d_df &amp;lt;- length_df %&amp;gt;%
  mutate(density=df3(qf3_seq))

ggplot(length_d_df, aes(x=density,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of given mixture distribution 3 by density&quot;, x= &quot;Density&quot;,y=&quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/N00G2v3L/prob5-m32.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mixture-distribution-4&quot;&gt;Mixture Distribution 4&lt;/h1&gt;

&lt;p&gt;Mixture Distribution 4 is the following mixture distribution.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;rf4 &amp;lt;- function(N){
  G &amp;lt;- sample(0:1, N, replace = TRUE)
  (G==0)*rbeta(N,5,1) + (G==1)*rbeta(N,1,5)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As previous chunks, we can plot the length of each quantile.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(i in 1:nrow(sml_results)){
  for(q in 1:ncol(sml_results)){
    sml_results[i,q] &amp;lt;- find_quantile_value(rf4,sample_size, q)
  }
}

length_list &amp;lt;- rep(NA,ncol(sml_results))
for(q in 1:ncol(sml_results)){
  quantile_2.5_97.5&amp;lt;- quantile(sml_results[,q], c(0.025, 0.975))
  length_list[q] &amp;lt;- quantile_2.5_97.5[[2]]-quantile_2.5_97.5[[1]]
}

length_df &amp;lt;- as.data.frame(matrix(ncol = 2, nrow = length(length_list)))
names(length_df) &amp;lt;- c(&quot;quantile_position&quot;,&quot;mid95_length&quot;)
length_df[,1] &amp;lt;- seq(0.05,0.95,0.05)
length_df[,2] &amp;lt;- length_list
ggplot(length_df,aes(x=quantile_position,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of given mixture distribution 4&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )+
  scale_y_continuous(name = &quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/YSyr1y7R/prob5-m41.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Definitely, when the quantile is 5% or 95%, the median have the tightest sampling distribution.&lt;/p&gt;

&lt;p&gt;Base on the rf4 function we had, we can write the pf4 and df4 functions. After that, use the method we used plot the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;rf4 &amp;lt;- function(N){
  G &amp;lt;- sample(0:1, N, replace = TRUE)
  (G==0)*rbeta(N,5,1) + (G==1)*rbeta(N,1,5)
}

pf4 &amp;lt;- function(x){
  .5*pbeta(x,5,1) + .5*pbeta(x,1,5)
}

df4 &amp;lt;- function(x){
  .5*dbeta(x,5,1) + .5*dbeta(x,1,5)
}

quantile_seq &amp;lt;- seq(0.05,0.95,0.05)
qf4_seq &amp;lt;- rep(NA, length(quantile_seq))
for(i in seq_along(quantile_seq)){
qf4_seq[i] &amp;lt;- uniroot(function(x){pf4(x)-quantile_seq[i]}, c(-100,100))[[1]]
}


length_d_df &amp;lt;- length_df %&amp;gt;%
  mutate(density=df4(qf4_seq))

ggplot(length_d_df, aes(x=density,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of given mixture distribution 4 by density&quot;, x= &quot;Density&quot;,y=&quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/hjzSMbPB/prob5-m42.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;other-tests&quot;&gt;Other Tests&lt;/h1&gt;

&lt;p&gt;In this part, we will focus on the situations that when sample size become 400, 800 and 1600.&lt;/p&gt;

&lt;p&gt;Now use a for loop to generate all of the data given different sample sizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sample_size &amp;lt;- c(400,800,1600)
test_num &amp;lt;- 5000

length_df &amp;lt;- as.data.frame(matrix(ncol = 1+length(sample_size), nrow = length(length_list)))
names(length_df) &amp;lt;- c(&quot;quantile_position&quot;,&quot;400_mid95_length&quot;,&quot;800_mid95_length&quot;,&quot;1600_mid95_length&quot;)
length_df[,1] &amp;lt;- seq(0.05,0.95,0.05)

for(n in seq_along(sample_size)){

  sml_results &amp;lt;- as.data.frame(matrix(NA, nrow = test_num, ncol = 19))

  for(i in 1:nrow(sml_results)){
    for(q in 1:ncol(sml_results)){
      sml_results[i,q] &amp;lt;- find_quantile_value(rnorm,sample_size[n], q)
    }
  }

  length_list &amp;lt;- rep(NA,ncol(sml_results))
  for(q in 1:ncol(sml_results)){
    quantile_2.5_97.5&amp;lt;- quantile(sml_results[,q], c(0.025, 0.975))
    length_list[q] &amp;lt;- quantile_2.5_97.5[[2]]-quantile_2.5_97.5[[1]]
  }

  length_df[,1+n] &amp;lt;- length_list
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Than plot the graph of different sample size and compare them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_df %&amp;gt;%
  melt(id = &quot;quantile_position&quot;) %&amp;gt;%
  ggplot()+
  geom_line(aes(x=quantile_position,y=value,color=variable))+
  labs(title = &quot;Length of middle 95% of normal distribution given different sample sizes&quot;, y = &quot;Length&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/Vkfm2cL4/prob5-o1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_df %&amp;gt;%
  mutate(density=dnorm(qnorm(quantile_position))) %&amp;gt;%
  select(-quantile_position) %&amp;gt;%
  melt(id = &quot;density&quot;) %&amp;gt;%
  ggplot()+
  geom_line(aes(x=density,y=value,color=variable))+
  labs(title = &quot;Length of middle 95% of normal distribution given different sample sizes&quot;, y = &quot;Length&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/J4cRThmG/prob5-o2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Definitely, when the sample size is bigger, the error of simulation will be smaller.&lt;/p&gt;

&lt;p&gt;In a word, when you have more simulation test, when the test number is increasing, the length will be smaller, what means the error of simulation is less than before.&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Sep 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/09/18/quantile-precision/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/09/18/quantile-precision/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        
      </item>
    
      <item>
        <title>If home field advantage exists, how much of an impact does it have on winning the world series?</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.technadu.com/wp-content/uploads/2019/05/World-Series-Logo.png&quot; alt=&quot;WORLD SERIES&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In team sports, the term home advantage – also called home ground, home field, home-field advantage, home court, home-court advantage, defender’s advantage or home-ice advantage – describes the benefit that the home team is said to gain over the visiting team. This benefit has been attributed to psychological effects supporting fans have on the competitors or referees; to psychological or physiological advantages of playing near home in familiar situations; to the disadvantages away teams suffer from changing time zones or climates, or from the rigors of travel; … In baseball, in particular, the difference may also be the result of the home team having been assembled to take advantage of the idiosyncrasies of the home ballpark, such as the distances to the outfield walls; most other sports are played in standardized venues.
&lt;em&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Home_advantage&quot;&gt;Wikipedia - Home advantage&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This time we will also talk about the competitions between the Braves and the Yankee, and the table below has the two possible schedules for each game of the series. (NYC = New York City, ATL = Atlanta)&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Overall advantage&lt;/th&gt;
      &lt;th&gt;Game 1&lt;/th&gt;
      &lt;th&gt;Game 2&lt;/th&gt;
      &lt;th&gt;Game 3&lt;/th&gt;
      &lt;th&gt;Game 4&lt;/th&gt;
      &lt;th&gt;Game 5&lt;/th&gt;
      &lt;th&gt;Game 6&lt;/th&gt;
      &lt;th&gt;Game 7&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Braves&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yankees&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; be the probability that the Braves win a single head-to-head match-up with the Yankees, under the assumption that home field advantage doesn’t exist. Let &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;sup&gt;H&lt;/sup&gt;&lt;/em&gt; denote the probability that the Braves win a single head-to-head match-up with the Yankees as the home team (H for home). Let &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;sup&gt;A&lt;/sup&gt;&lt;/em&gt; denote the probability that the Braves win a single head-to-head match-up with the away team (A for away).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Game location&lt;/th&gt;
      &lt;th&gt;No advantage&lt;/th&gt;
      &lt;th&gt;Advantage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;sup&gt;H&lt;/sup&gt; = P&lt;sub&gt;B&lt;/sub&gt; $\times$ 1.1&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;sup&gt;A&lt;/sup&gt; = 1 - (1 - P&lt;sub&gt;B&lt;/sub&gt;)$\times$ 1.1&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(dplyr)
library(data.table)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s look at the questions.&lt;/p&gt;

&lt;h1 id=&quot;questions&quot;&gt;Questions&lt;/h1&gt;
&lt;h2 id=&quot;1-compute-analytically-the-probability-that-the-braves-win-the-world-series-when-the-sequence-of-game-locations-is-nyc-nyc-atl-atl-atl-nyc-nyc-calculate-the-probability-with-and-without-home-field-advantage-when-pb--055-what-is-the-difference-in-probabilities&quot;&gt;1. Compute analytically the probability that the Braves win the world series when the sequence of game locations is {NYC, NYC, ATL, ATL, ATL, NYC, NYC}. Calculate the probability with and without home field advantage when &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt; = 0.55&lt;/em&gt;. What is the difference in probabilities?&lt;/h2&gt;

&lt;p&gt;First, load the &lt;em&gt;.csv&lt;/em&gt; file to make sure what’s the situations we need to calculate, which represents the data we will generate later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Get all possible outcomes
apo &amp;lt;- fread(&quot;all-possible-world-series-outcomes.csv&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we need to define a sequence of game locations. This time, the sequence should be {NYC, NYC, ATL, ATL, ATL, NYC, NYC}. As a result of that the Braves is a Atlanta team, we use 1 to represent Atlanta and use 0 to represent NYC.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Home field indicator
hfi &amp;lt;- c(0,0,1,1,1,0,0) #{NYC, NYC, ATL, ATL, ATL, NYC, NYC}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we use 0.55 to define the &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; and generate other probabilities by &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; as we talked before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# P_B
pb &amp;lt;- 0.55
advantage_multiplier &amp;lt;- 1.1 # Set = 1 for no advantage
pbh &amp;lt;- pb * advantage_multiplier
pba &amp;lt;- 1 - (1 - pb) * advantage_multiplier
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this part, we will use the parameters we defined before. In every row of data.table, we use the different probabilities which influenced by home field advantage to calculate the overall probabilties of each situation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Calculate the probability of each possible outcome
apo[, p := NA_real_] # Initialize new column in apo to store prob
for(i in 1:nrow(apo)){
  prob_game &amp;lt;- rep(1, 7)
  for(j in 1:7){
    p_win &amp;lt;- ifelse(hfi[j], pbh, pba)
    prob_game[j] &amp;lt;- case_when(
        apo[i,j,with=FALSE] == &quot;W&quot; ~ p_win
      , apo[i,j,with=FALSE] == &quot;L&quot; ~ 1 - p_win
      , TRUE ~ 1
    )
  }
  apo[i, p := prod(prob_game)] # Data.table syntax
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we output the probability of that the Braves wins the World Series under the influence of home field advantage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Probability of overall World Series outcomes
p_home &amp;lt;- purrr::flatten_dbl(apo[, sum(p), overall_outcome][1,2])
p_home
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability is &lt;code class=&quot;highlighter-rouge&quot;&gt;r p_home&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then we can calculate the probability when there is no home field advantage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;p_nohome &amp;lt;- 1 - pbinom(3, 7, 0.55)
p_nohome
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probabilty is &lt;code class=&quot;highlighter-rouge&quot;&gt;r p_nohome&lt;/code&gt;. Definitely, when home field advantage exists, the probabity that the Braves win the World Series is lower, &lt;code class=&quot;highlighter-rouge&quot;&gt;r p_home&lt;/code&gt;, than the probabity without home field advantage. Given &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;=0.55&lt;/em&gt;, the probability of the Braves winning the World Series with a home field advantage &lt;code class=&quot;highlighter-rouge&quot;&gt;r (p_home-p_nohome)*100 &lt;/code&gt;a less likely than the probability of the Braves winning the world Series without a home field advantage.&lt;/p&gt;

&lt;h2 id=&quot;2-calculate-the-same-probabilities-as-the-previous-question-by-simulation&quot;&gt;2. Calculate the same probabilities as the previous question by simulation.&lt;/h2&gt;

&lt;p&gt;In this part, we will use simulation to test the probability.&lt;/p&gt;

&lt;p&gt;Given the location sequence, we use different winning probabilities of head to head games and random generate the result of each game. Repeat the process 100000 times, we can get the approximate solution of the probabilty with home field advantage influence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;set.seed(314)
sml_list_h &amp;lt;- rep(NA, 100000)
for (i in seq_along(sml_list_h)){
  round &amp;lt;- rep(NA,7)
  for(j in 1:7){
    p_win &amp;lt;- ifelse(hfi[j], pbh, pba)
    round[j] &amp;lt;- rbinom(1,1,p_win)
  }
  sml_list_h[i] &amp;lt;- ifelse(sum(round)&amp;gt;=4, 1, 0)
}
mean_sml_h &amp;lt;- mean(sml_list_h)
mean_sml_h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can get the approximate solution, &lt;code class=&quot;highlighter-rouge&quot;&gt;r mean_sml_h&lt;/code&gt;, which is a little different from &lt;code class=&quot;highlighter-rouge&quot;&gt;r p_home&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now, let’s simulate the situation without home field advantage influence. It’s easy because we only need to make p_win as a constant value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;set.seed(314)
sml_list_nh &amp;lt;- rep(NA, 100000)
for (i in seq_along(sml_list_nh)){
  round &amp;lt;- rep(NA,7)
  for(j in 1:7){
    p_win &amp;lt;- 0.55
    round[j] &amp;lt;- rbinom(1,1,p_win)
  }
  sml_list_nh[i] &amp;lt;- ifelse(sum(round)&amp;gt;=4, 1, 0)
}
mean_sml_nh &amp;lt;- mean(sml_list_nh)
mean_sml_nh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;3-what-is-the-absolute-and-relative-error-for-your-simulation-in-the-previous-question&quot;&gt;3. What is the absolute and relative error for your simulation in the previous question?&lt;/h2&gt;

&lt;p&gt;Absolute error =
&lt;script type=&quot;math/tex&quot;&gt;|p̂−p|&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Relative error =
&lt;script type=&quot;math/tex&quot;&gt;|p̂−p|/p&lt;/script&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;abs_error_h &amp;lt;- abs(mean(sml_list_h) - p_home)
rel_error_h &amp;lt;- abs(mean(sml_list_h) - p_home)/mean(sml_list_h)
abs_error_h
rel_error_h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, given home field advantage the absolute error is &lt;code class=&quot;highlighter-rouge&quot;&gt;r abs_error_h&lt;/code&gt;. The relative error is &lt;code class=&quot;highlighter-rouge&quot;&gt;r rel_error_h&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;abs_error_nh &amp;lt;- abs(mean(sml_list_nh) - p_nohome)
rel_error_nh &amp;lt;- abs(mean(sml_list_nh) - p_nohome)/mean(sml_list_nh)
abs_error_nh
rel_error_nh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, given no home field advantage the absolute error is &lt;code class=&quot;highlighter-rouge&quot;&gt;r abs_error_nh&lt;/code&gt;. The relative error is &lt;code class=&quot;highlighter-rouge&quot;&gt;r rel_error_nh&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;bonus-1-does-the-difference-in-probabilites-with-vs-without-home-field-advantage-depend-on-pb&quot;&gt;Bonus 1. Does the difference in probabilites (with vs without home field advantage) depend on &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;?&lt;/h2&gt;

&lt;p&gt;The process is similar with the answer of question 1.&lt;/p&gt;

&lt;p&gt;We can create some lists to save the different &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;s, the probabilities of win the World Series with or without home field advantage, and the difference between these two situations given different &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Then given every &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;, calculate these values every time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;pb_list &amp;lt;- seq(0,1,0.01)
ph_win_list &amp;lt;- seq_along(pb_list)
pnh_win_list &amp;lt;- ph_win_list
diff_list &amp;lt;- ph_win_list
for (p in seq_along(pb_list)) {
  pb &amp;lt;- pb_list[p]
  advantage_multiplier &amp;lt;- 1.1
  pbh &amp;lt;- pb * advantage_multiplier
  pba &amp;lt;- 1 - (1 - pb) * advantage_multiplier

  pnh_win_list[p] &amp;lt;- 1 - pbinom(3, 7, pb_list[p])

  # Calculate the probability of each possible outcome
  apo[, p := NA_real_] # Initialize new column in apo to store prob
  for(i in 1:nrow(apo)){
    prob_game &amp;lt;- rep(1, 7)
    for(j in 1:7){
      p_win &amp;lt;- ifelse(hfi[j], pbh, pba)
      prob_game[j] &amp;lt;- case_when(
          apo[i,j,with=FALSE] == &quot;W&quot; ~ p_win
        , apo[i,j,with=FALSE] == &quot;L&quot; ~ 1 - p_win
        , TRUE ~ 1
      )
    }
  apo[i, p := prod(prob_game)] # Data.table syntax
  }
  ph_win_list[p] &amp;lt;- purrr::flatten_dbl(apo[, sum(p), overall_outcome][1][,2])
  diff_list[p] &amp;lt;- pnh_win_list[p] - ph_win_list[p]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, plot a graph to show the relationship between &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; and differences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;plot(x= pb_list, y=diff_list)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/65B70gRL/prob4-b1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Obviously, the relationship between &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; and differences is a compound function containing trigonometric functions.&lt;/p&gt;

&lt;p&gt;And the format might be a trigonometric function times a function, that when going to the start and the end of one period approximate 0, otherwise approximate 1. By the way, the period should be 1.&lt;/p&gt;

&lt;p&gt;There are some examples.
&lt;script type=&quot;math/tex&quot;&gt;y = A \cdot sin(B \cdot x + C)
\\
y = A \cdot sin(B \cdot x + C) \cdot |D \cdot sin(\pi \cdot x + E)|
\\
y = A \cdot sin(B \cdot x + C) \cdot  ( D \cdot sin(\pi \cdot x + E))^2
\\
y = Ax^3 + Bx^2y + Cxy^2+Dy^3 +Ex^2+Fxy+Gy^2+Hx+Iy+J&lt;/script&gt;
Unfortunately, after these functions I listed cannot regress to the graph we got.&lt;/p&gt;

&lt;h2 id=&quot;bonus-2-does-the-difference-in-probabilites-with-vs-without-home-field-advantage-depend-on-the-advantage-factor-the-advantage-factor-in-pbh-and-pba-is-the-11-multiplier-that-results-in-a-10-increase-for-the-home-team&quot;&gt;Bonus 2. Does the difference in probabilites (with vs without home field advantage) depend on the advantage factor? (The advantage factor in PBH and PBA is the 1.1 multiplier that results in a 10% increase for the home team.)&lt;/h2&gt;

&lt;p&gt;In this question, the process is similar to Bonus1. We only need to change the sequence content from &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; to home field advantage factor and make &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; be a constant(0.55). Therefore, we will use &lt;em&gt;ha_list&lt;/em&gt; to save the sequence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;ha_list &amp;lt;- seq(1,2,0.01)
ph_win_list &amp;lt;- seq_along(ha_list)
pnh_win_list &amp;lt;- ph_win_list
diff_list2 &amp;lt;- ph_win_list
for (p in seq_along(ha_list)) {
  pb &amp;lt;- 0.55
  advantage_multiplier &amp;lt;- ha_list[p]
  pbh &amp;lt;- pb * advantage_multiplier
  pba &amp;lt;- 1 - (1 - pb) * advantage_multiplier

  pnh_win_list[p] &amp;lt;- 1 - pbinom(3, 7, 0.55)

  # Calculate the probability of each possible outcome
  apo[, p := NA_real_] # Initialize new column in apo to store prob
  for(i in 1:nrow(apo)){
    prob_game &amp;lt;- rep(1, 7)
    for(j in 1:7){
      p_win &amp;lt;- ifelse(hfi[j], pbh, pba)
      prob_game[j] &amp;lt;- case_when(
          apo[i,j,with=FALSE] == &quot;W&quot; ~ p_win
        , apo[i,j,with=FALSE] == &quot;L&quot; ~ 1 - p_win
        , TRUE ~ 1
      )
    }
  apo[i, p := prod(prob_game)] # Data.table syntax
  }
  ph_win_list[p] &amp;lt;- purrr::flatten_dbl(apo[, sum(p), overall_outcome][1][,2])
  diff_list2[p] &amp;lt;- pnh_win_list[p] - ph_win_list[p]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s look at the graph. Obviously, when the home field advantage factor is increasing, the difference in probabilities between with and without home filed advantage will increase too.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;plot(x= ha_list, y=diff_list2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/rmhHPvs0/prob4-b2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Sep 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/09/12/home-advantage/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/09/12/home-advantage/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        
      </item>
    
      <item>
        <title>How often does the better team win the World Series?</title>
        <description>&lt;p&gt;Import the package at the very first.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.technadu.com/wp-content/uploads/2019/05/World-Series-Logo.png&quot; alt=&quot;WORLD SERIES&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The World Series is the annual championship series of Major League Baseball (MLB) in North America, contested since 1903 between the American League (AL) champion team and the National League (NL) champion team. The winner of the World Series championship is determined through a best-of-seven playoff, and the winning team is awarded the Commissioner’s Trophy. As the series is played during the fall season in North America, it is sometimes referred to as the Fall Classic.&lt;br /&gt;
&lt;em&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/World_Series&quot;&gt;Wikipedia - World Series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this blog, we are going to calculate the probability of several questions about the Braves and the Yankees in the World Series.&lt;/p&gt;

&lt;p&gt;First, we need to define some parameter.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameter&lt;/th&gt;
      &lt;th&gt;Explaination&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;In any given game, the probability that the Braves win&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;P&lt;sub&gt;Y&lt;/sub&gt; = 1 - P&lt;sub&gt;B&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;in any given game, the probability that the Yankees win&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;questions&quot;&gt;Questions&lt;/h1&gt;
&lt;h3 id=&quot;1-what-is-the-probability-that-the-braves-win-the-world-series-given-that-pb055&quot;&gt;1. What is the probability that the Braves win the World Series given that P&lt;sub&gt;B&lt;/sub&gt;=0.55?&lt;/h3&gt;

&lt;p&gt;First,we need to set the value of P&lt;sub&gt;B&lt;/sub&gt; and P&lt;sub&gt;Y&lt;/sub&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;PB &amp;lt;- 0.55
PY &amp;lt;- 1- PB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a function to calculate the probability of win. Win is defined as win 4 times in 7 games.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;calc_prob &amp;lt;- function(p){
  pnbinom(3, 4, p)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now calculate the probability given that P&lt;sub&gt;B&lt;/sub&gt;=0.55.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;calc_prob(PB)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, when the P&lt;sub&gt;B&lt;/sub&gt; is 0.55, the probability that Braves win the World Series is 0.608.&lt;/p&gt;

&lt;h3 id=&quot;2-what-is-the-probability-that-the-braves-win-the-world-series-given-that-pbx&quot;&gt;2. What is the probability that the Braves win the World Series given that P&lt;sub&gt;B&lt;/sub&gt;=x?&lt;/h3&gt;
&lt;p&gt;Now the P&lt;sub&gt;B&lt;/sub&gt; is not defined yet, so we should assume the x could be any number between 0.5 to 1.&lt;/p&gt;

&lt;p&gt;First, we need to generate the series of P&lt;sub&gt;B&lt;/sub&gt; and the probability results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;PBseries &amp;lt;- seq(0.5, 1, 0.01)
win_prob &amp;lt;- rep(NA, length(PBseries))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now use the function we used before to calculate the probability given every P&lt;sub&gt;B&lt;/sub&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(i in 1:length(win_prob)){
  win_prob[i] &amp;lt;- calc_prob(PBseries[i])
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to intepret the relationship between P&lt;sub&gt;B&lt;/sub&gt; and the probability that the Braves win, we can draw a graph for them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;plot(x = PBseries,
     y = win_prob,
     xlim = c(0.5,1),
     ylim = 0:1,
     xlab = &quot;Probability of the Braves winning a head-to-head matchup&quot;,
     ylab = &quot;P(Braves win World Series)&quot;,
     main = &quot;Probability of winning the World Series&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/Bh7NjCIcKU2Myab.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, in this graph, when P&lt;sub&gt;B&lt;/sub&gt; is increasing, the probability that the Braves win the World Series is increasing too. In fact, when we change the x scale to 0.0-1.0, we will find the line looks like a logistic curve.&lt;/p&gt;

&lt;h3 id=&quot;3-suppose-one-could-change-the-world-series-to-be-best-of-9-or-some-other-best-of-x-series-what-is-the-shortest-series-length-so-that-pbraves-win-world-seriespb055--08&quot;&gt;3. Suppose one could change the World Series to be best-of-9 or some other best-of-X series. What is the shortest series length so that P(Braves win World Series|P&lt;sub&gt;B&lt;/sub&gt;=0.55) ≥ 0.8?&lt;/h3&gt;

&lt;p&gt;As same as the first question, the P&lt;sub&gt;B&lt;/sub&gt; needs to be 0.55. And now the game series length is not a certain. Definitely, the series length should be an odd number.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;PB &amp;lt;- 0.55
series_length &amp;lt;- seq(1, 999, 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we need to create a function to calculate the probability when the series length is parameter.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;calc_prob_sl &amp;lt;- function(sl){
  win_threshhold &amp;lt;- ceiling(sl/2)
  pnbinom(win_threshhold - 1, win_threshhold, 0.55)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end, given every series length, calculate the probability that the Braves win World Series. When the probability is equal to or more than 0.8, stop running and give the series length value and the probability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(i in 1:length(series_length)){
  pb_win &amp;lt;- calc_prob_sl(series_length[i])
  if(pb_win &amp;gt;= 0.8){
    shortest &amp;lt;- series_length[i]
    p_shortest &amp;lt;- pb_win
    break}
}
shortest
p_shortest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we get the the shortest series length. It should be 71. In that situation, the probability that the Braves win World Series is about 0.802.&lt;/p&gt;

&lt;h3 id=&quot;4-what-is-the-shortest-series-length-so-that-pbraves-win-world-seriespb-x--08-this-will-be-a-figure-see-below-with-pb-on-the-x-axis-and-series-length-is-the-y-axis&quot;&gt;4. What is the shortest series length so that P(Braves win World Series|P&lt;sub&gt;B&lt;/sub&gt;= x) ≥ 0.8? This will be a figure (see below) with P&lt;sub&gt;B&lt;/sub&gt; on the x-axis and series length is the y-axis.&lt;/h3&gt;
&lt;p&gt;Now the P&lt;sub&gt;B&lt;/sub&gt; is not defined again, so we should assume the x could be any number between 0.51 to 1.&lt;/p&gt;

&lt;p&gt;First, we need to generate the series of P&lt;sub&gt;B&lt;/sub&gt; and a series to save the length results given different P&lt;sub&gt;B&lt;/sub&gt;. But the way, we also need a series of the possible series length we will test. Now the ceiling is 9999. If it’s not enough, we can set a bigger limitation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;PBseries &amp;lt;- seq(0.51, 1, 0.01)
length_record &amp;lt;- rep(NA, length(PBseries))
series_length &amp;lt;- seq(1, 9999, 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to calculate the probabilty that the Braves win the WS, we need a new function with 2 input, because both of the series length and P&lt;sub&gt;B&lt;/sub&gt; are variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;calc_prob_sl_p &amp;lt;- function(sl,pb){
  win_threshhold &amp;lt;- ceiling(sl/2)
  pnbinom(win_threshhold - 1, win_threshhold, pb)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, calculate the shortest series length when P&lt;sub&gt;B&lt;/sub&gt; is changing. Save the values in &lt;em&gt;length_record&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(j in 1:length(PBseries)){
  for(i in 1:length(series_length)){
  pb_win &amp;lt;- calc_prob_sl_p(series_length[i],PBseries[j])
  if(pb_win &amp;gt;= 0.8){
    shortest &amp;lt;- series_length[i]
    break}
  }
  length_record[j] &amp;lt;- shortest
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have already get the shortest series length given different P&lt;sub&gt;B&lt;/sub&gt;. Let’s draw the figure to show the relationship between them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;plot(x = PBseries,
     y = length_record,
     xlim = c(0.5,1),
     xlab = &quot;Probability of the Braves winning a head-to-head matchup&quot;,
     ylab = &quot;Series length&quot;,
     main = &quot;Shortest series so that P(Win WS given p)&amp;gt;=0.8&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/jA2LkzaNK5tF6OB.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;in this graph, when P&lt;sub&gt;B&lt;/sub&gt; is increasing, the shortest series length, when the probability that the Braves win the World Series is more than 0.8, is approching 1. When the P&lt;sub&gt;B&lt;/sub&gt; is bigger than 0.8, the shortest series length is 1.&lt;/p&gt;

&lt;h3 id=&quot;5-calculate-p-pb055braves-lose-3-games-before-winning-a-4th-game-under-the-assumption-that-either--pb055-or--pb045-explain-your-solution&quot;&gt;5. Calculate P( P&lt;sub&gt;B&lt;/sub&gt;=0.55|Braves lose 3 games before winning a 4th game) under the assumption that either  P&lt;sub&gt;B&lt;/sub&gt;=0.55 or  P&lt;sub&gt;B&lt;/sub&gt;=0.45. Explain your solution.&lt;/h3&gt;

&lt;p&gt;According to Conditional probability formula，we can get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B)=\frac{P(A)P(B)}{P(B)}  \to  P(A)P(B)=P(A|B)P(B)\\
P(B|A)=\frac{P(A)P(B)}{P(A)}  \to  P(A)P(B)=P(B|A)P(A)\\
\to P(A|B)P(B)=P(A)P(B)=P(B|A)P(A)\\
\to P(A|B)=\frac{P(B|A)P(A)}{P(B)}&lt;/script&gt;

&lt;p&gt;Now the P(A) = P(P&lt;sub&gt;B&lt;/sub&gt;=0.55), P(B) = P(Braves lose 3 games before winning a 4th game).
As a result, P( P&lt;sub&gt;B&lt;/sub&gt;=0.55|Braves lose 3 games before winning a 4th game) = P(Braves lose 3 games before winning a 4th game|P&lt;sub&gt;B&lt;/sub&gt;=0.55) * P(P&lt;sub&gt;B&lt;/sub&gt;=0.55) ÷ P(Braves lose 3 games before winning a 4th game).&lt;/p&gt;

&lt;p&gt;P(P&lt;sub&gt;B&lt;/sub&gt;=0.55) = 0.5&lt;/p&gt;

&lt;p&gt;Then use &lt;em&gt;dnbinom()&lt;/em&gt; calculate P(Braves lose 3 games before winning a 4th game) and P(Braves lose 3 games before winning a 4th game|P&lt;sub&gt;B&lt;/sub&gt;=0.55):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;(dnbinom(3,4,0.45)+dnbinom(3,4,0.55))/2
dnbinom(3,4,0.55)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;P(Braves lose 3 games before winning a 4th game) = 0.1516092&lt;/p&gt;

&lt;p&gt;P(Braves lose 3 games before winning a 4th game | P&lt;sub&gt;B&lt;/sub&gt;=0.55) = 0.1667701&lt;/p&gt;

&lt;p&gt;P( P&lt;sub&gt;B&lt;/sub&gt;=0.55|Braves lose 3 games before winning a 4th game) = P(Braves lose 3 games before winning a 4th game|P&lt;sub&gt;B&lt;/sub&gt;=0.55) * P(P&lt;sub&gt;B&lt;/sub&gt;=0.55) ÷ P(Braves lose 3 games before winning a 4th game)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;0.1667701 * 0.5 / 0.1516092
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;P( P&lt;sub&gt;B&lt;/sub&gt;=0.55|Braves lose 3 games before winning a 4th game) = 0.1667701 * 0.5 ÷ 0.1516092 = 0.5499999&lt;/p&gt;

&lt;p&gt;Therefore, P( P&lt;sub&gt;B&lt;/sub&gt;=0.55|Braves lose 3 games before winning a 4th game) is 0.5499999, about 0.55.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Sep 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/09/09/world-series/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/09/09/world-series/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        
      </item>
    
      <item>
        <title>What’s the difference between Absolute Error and Relative Error?</title>
        <description>&lt;p&gt;Absolute error and relative error have a little bit difference on calculation but the difference really makes an essential change. Absolute error will be influenced by the samples size but relative error can show the real error without huge bias.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;when used as a measure of precision—is the ratio of the absolute error of a measurement to the measurement being taken. In other words, this type of error is relative to the size of the item being measured. RE is expressed as a percentage and has no units.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;From &lt;a href=&quot;https://www.statisticshowto.datasciencecentral.com/relative-error/&quot;&gt;Statistics How To&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;absolute-error&quot;&gt;Absolute Error&lt;/h2&gt;
&lt;p&gt;absolute error = |p̂−p|&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The difference between the measured or inferred value of a quantity x_0 and its actual value x.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let’s create a table to save 14*5 results of our simulation first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;n &amp;lt;- rep(NA, 14)
for(i in 1:14){n[i] &amp;lt;- 2^(i+1)}
T &amp;lt;- matrix(NA,14,5)
p &amp;lt;- c(0.01,0.05,0.10,0.25,0.5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Than generate 1 or 0 randomly for 1000 times for each situation and calculate the absolute error.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(x in 1:length(p)){
  for(y in 1:length(n)){
    TS &amp;lt;- rep(NA,10000)
    for(m in 1:10000){
      TS[m] &amp;lt;- abs(rbinom(1,n[y],p[x])/n[y]-p[x])
    }
    T[y,x] &amp;lt;- mean(TS)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change the y-scale to log_10.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;T &amp;lt;- log10(T)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end, plot the graph to show the relationship between p and absolute error.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;plot(T[,5],xlim=c(0,14),ylim=range(T),col=&quot;red&quot;,type=&quot;b&quot;,xaxt=&quot;n&quot;,xlab=&quot;N(log_2 scale)&quot;,ylab=&quot;Absolute Error&quot;,pch=16, lwd=3)
lines(T[,2],col=&quot;purple&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T[,3],col=&quot;blue&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T[,4],col=&quot;green&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T[,1],col=&quot;gray&quot;,type=&quot;b&quot;,pch=16, lwd=3)

lname &amp;lt;- c(&quot;0.01&quot;,&quot;0.05&quot;,&quot;0.10&quot;,&quot;0.25&quot;,&quot;0.50&quot;)
lname_p &amp;lt;- paste0(&quot;p = &quot;,lname)
xname &amp;lt;- c(&quot;4&quot;,&quot;8&quot;,&quot;16&quot;,&quot;32&quot;,&quot;64&quot;,&quot;128&quot;,&quot;256&quot;,&quot;512&quot;,&quot;1024&quot;,&quot;2048&quot;,&quot;4096&quot;,&quot;8192&quot;,&quot;16384&quot;,&quot;32768&quot;)

axis(1, at=1:14,las=2, lab=xname)
text(1,T[1,],lname_p,pos=2,cex=0.6)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/6yheRBQ3HIMnDcd.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Absolute error is just the absolute value of the value you have minus expected value. In this simulation, you culculated the absolute error 10000 times and get the mean value of them in every situation.
When we transfer the y scale to log_10, it is obvious that the x &amp;amp; y have linear connection.
The p is higher, the absolute error is bigger.&lt;/p&gt;

&lt;h1 id=&quot;relative-error&quot;&gt;Relative Error&lt;/h1&gt;
&lt;p&gt;relative error = |p̂−p|/p.&lt;/p&gt;

&lt;p&gt;Then do the same thing as before but when we calculate the error, use absolute error divide by p value.
Plot the graph too.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;n &amp;lt;- rep(NA, 14)
for(i in 1:14){n[i] &amp;lt;- 2^(i+1)}

T2 &amp;lt;- matrix(NA,14,5)
p &amp;lt;- c(0.01,0.05,0.10,0.25,0.5)

for(x in 1:length(p)){
  for(y in 1:length(n)){
    T2S &amp;lt;- rep(NA,10000)
    for(m in 1:10000){
      T2S[m] &amp;lt;- abs(rbinom(1,n[y],p[x])/n[y]-p[x])/p[x]
    }
    # T2[y,x] &amp;lt;- abs(rbinom(1,n[y],p[x])/n[y]-p[x])/p[x]
    T2[y,x] &amp;lt;- mean(T2S)
  }
}

T2 &amp;lt;- log10(T2)

xname &amp;lt;- c(&quot;4&quot;,&quot;8&quot;,&quot;16&quot;,&quot;32&quot;,&quot;64&quot;,&quot;128&quot;,&quot;256&quot;,&quot;512&quot;,&quot;1024&quot;,&quot;2048&quot;,&quot;4096&quot;,&quot;8192&quot;,&quot;16384&quot;,&quot;32768&quot;)
lname &amp;lt;- c(&quot;0.01&quot;,&quot;0.05&quot;,&quot;0.10&quot;,&quot;0.25&quot;,&quot;0.50&quot;)
lname_p &amp;lt;- paste0(&quot;p = &quot;,lname)

plot(T2[,1],xlim=c(0,14),ylim=range(T2),col=&quot;red&quot;,type=&quot;b&quot;,pch=16,xaxt=&quot;n&quot;,xlab=&quot;N(log_2 scale)&quot;,ylab=&quot;Relative Error&quot;, lwd=3)
lines(T2[,2],col=&quot;purple&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T2[,3],col=&quot;blue&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T2[,4],col=&quot;green&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T2[,5],col=&quot;gray&quot;,type=&quot;b&quot;,pch=16, lwd=3)
axis(1, at=1:14,las=2, lab=xname)
text(1,T2[1,],lname_p,pos=2,cex=0.6)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/lryCYGkREIN6dWx.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, compare with absolute error, if you want to calculate relative error, the only thing you need to do is use the absolute error divide by p, the value you have. This process keep the influence of the value size, and focus on the error itself.
When we change the y scale into log_10, the xy relationship is also linear. But the bigger p we have, the smaller relative error we get.&lt;/p&gt;
</description>
        <pubDate>Fri, 06 Sep 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/09/06/absolute&relative-error/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/09/06/absolute&relative-error/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        
      </item>
    
      <item>
        <title>Roulette Simulation</title>
        <description>&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Roulette is a casino game named after the French word meaning little wheel. In the game, players may choose to place bets on either a single number, various groupings of numbers, the colors red or black, whether the number is odd or even, or if the numbers are high (19–36) or low (1–18).
&lt;em&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Roulette&quot;&gt;Wikipedia-Roulette&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;This time we will talk about a kind of simplified Roulette, which won’t be influenced by the number on roulette but only be divided into two parts, win or lose.&lt;/p&gt;

&lt;p&gt;To play the game successfully and avoid owing unrealistic debt, we need to set some parameter at first. These parameter will be save in a state list.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameter&lt;/th&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Explaination&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the budget&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the budget threshold for successfully stoping&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the maximum number of plays&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;M&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the casino wager limit&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;plays&lt;/td&gt;
      &lt;td&gt;integer&lt;/td&gt;
      &lt;td&gt;the number of plays executed&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;previous_wager&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the wager in the previous play (0 at first play)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;previous_win&lt;/td&gt;
      &lt;td&gt;TRUE/FALSE&lt;/td&gt;
      &lt;td&gt;indicator if the previous play was a win (TRUE at first play)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;function-setup&quot;&gt;Function Setup&lt;/h1&gt;

&lt;h2 id=&quot;one-play&quot;&gt;One Play&lt;/h2&gt;

&lt;p&gt;In order to use pipes “%&amp;gt;%” in code, we need to import the package first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r,results='hide'}&quot;&gt;library(dplyr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, let’s define the process of one play.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt; one_play &amp;lt;- function(state){

    # Wager
    proposed_wager &amp;lt;- ifelse(state$previous_win, 1, 2*state$previous_wager)
    wager &amp;lt;- min(proposed_wager, state$M, state$B)

    # Spin of the wheel
    red &amp;lt;- rbinom(1,1,18/38)

    # Update state
    state$plays &amp;lt;- state$plays + 1
    state$previous_wager &amp;lt;- wager
    if(red){
      # WIN
      state$B &amp;lt;- state$B + wager
      state$previous_win &amp;lt;- TRUE
    }else{
      # LOSE
      state$B &amp;lt;- state$B - wager
      state$previous_win &amp;lt;- FALSE
    }
  state
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the player run out of the money or win enough money or play enough times, we need to stop the game by set up an stop function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;stop_play &amp;lt;- function(state){
  if(state$B &amp;lt;= 0) return(TRUE)
  if(state$plays &amp;gt;= state$L) return(TRUE)
  if(state$B &amp;gt;= state$W) return(TRUE)
  FALSE
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;multiple-plays&quot;&gt;Multiple Plays&lt;/h2&gt;

&lt;p&gt;Next, we need to play the game under our rules as a series. The function will output a budget list to record the money value after every play.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;one_series &amp;lt;- function(
    B = 200
  , W = 300
  , L = 1000
  , M = 100
){

  # initial state
  state &amp;lt;- list(
    B = B
  , W = W
  , L = L
  , M = M
  , plays = 0
  , previous_wager = 0
  , previous_win = TRUE
  )

  # vector to store budget over series of plays
  budget &amp;lt;- rep(NA, L)

  # For loop of plays
  for(i in 1:L){
    new_state &amp;lt;- state %&amp;gt;% one_play
    budget[i] &amp;lt;- new_state$B
    if(new_state %&amp;gt;% stop_play){
      return(budget[1:i])
    }
    state &amp;lt;- new_state
  }
  budget
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can get the final result of this series of play.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# helper function
get_last &amp;lt;- function(x) x[length(x)]
get_series &amp;lt;- function(x) x
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;simulation&quot;&gt;Simulation&lt;/h1&gt;

&lt;p&gt;In order to figure out the generalized result, we need to repeat the process for a huge number of times then try to find out the distribution and other characteristics of results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Simulation
walk_out_money &amp;lt;- rep(NA, 1000)
for(j in seq_along(walk_out_money)){
  walk_out_money[j] &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_last
}

# Walk out money distribution
hist(walk_out_money, breaks = 100)

# Estimated probability of walking out with extra cash
mean(walk_out_money &amp;gt; 200)

# Estimated earnings
mean(walk_out_money - 200)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;compare&quot;&gt;Compare&lt;/h1&gt;

&lt;p&gt;In this graph, we can see how the budget changes during in one series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
plot(budget_list, type=&quot;l&quot;, xlim=c(0,500), ylim=c(0,300), xlab=&quot;play number&quot;, ylab=&quot;earning money&quot;, main=&quot;budget series&quot;,col=&quot;red&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;orange&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;yellow&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;green&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;gray&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;blue&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;purple&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/4kcjYL5KFvSeA9Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameter&lt;/th&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Explaination&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the budget&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the budget threshold for successfully stoping&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the maximum number of plays&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;M&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the casino wager limit&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;change-the-budget&quot;&gt;Change the budget&lt;/h2&gt;

&lt;p&gt;When B changes, what is the mean earning.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;earning_series &amp;lt;- rep(NA,20)
for(B in seq(100,1000,by=50)){
walk_out_money &amp;lt;- rep(NA, 1000)
for(j in seq_along(walk_out_money)){
  walk_out_money[j] &amp;lt;- one_series(B, W=B+100, L = 1000, M = 100) %&amp;gt;% get_last
}
earning_series[B] &amp;lt;- mean(walk_out_money - B)/B
}
plot(earning_series,xlab=&quot;Budget&quot;,ylab=&quot;mean earning rate&quot;, main=&quot;How Budget influence earning?&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/YBcaSpgdhf7nmwi.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;change-the-budget-threshold-for-successfully-stoping&quot;&gt;Change the budget threshold for successfully stoping&lt;/h2&gt;

&lt;p&gt;When W changes, what is the mean earning.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;earning_series &amp;lt;- rep(NA,20)
for(W in seq(100,1000,by=50)){
walk_out_money &amp;lt;- rep(NA, 10000)
for(j in seq_along(walk_out_money)){
  walk_out_money[j] &amp;lt;- one_series(B=200, W, L = 1000, M = 100) %&amp;gt;% get_last
}
earning_series[W] &amp;lt;- mean(walk_out_money - 200)
}
plot(earning_series,xlab=&quot;successfully stoping number&quot;,ylab=&quot;mean earning&quot;, main=&quot;How successfully stoping influence earning?&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/wn2qDzCjXlxSGIY.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;change-the-maximum-number-of-plays&quot;&gt;Change the maximum number of plays&lt;/h2&gt;

&lt;p&gt;When L changes, what is the mean earning.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;earning_series &amp;lt;- rep(NA,100)
for(L in seq(10,1000,by=10)){
walk_out_money &amp;lt;- rep(NA, 1000)
for(j in seq_along(walk_out_money)){
  walk_out_money[j] &amp;lt;- one_series(B=200, W=300, L, M = 100) %&amp;gt;% get_last
}
earning_series[L] &amp;lt;- mean(walk_out_money - 200)
}
plot(earning_series,xlab=&quot;maximum number of plays&quot;,ylab=&quot;mean earning&quot;, main=&quot;How maximum number of plays influence earning?&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/qeWF58lmvsfiLdE.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;change-the-casino-wager-limit&quot;&gt;Change the casino wager limit&lt;/h2&gt;

&lt;p&gt;When M changes, what is the mean earning.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;earning_series &amp;lt;- rep(NA,100)
for(M in seq(10,1000,by=10)){
walk_out_money &amp;lt;- rep(NA, 1000)
for(j in seq_along(walk_out_money)){
  walk_out_money[j] &amp;lt;- one_series(B=200, W=300, L=500, M) %&amp;gt;% get_last
}
earning_series[M] &amp;lt;- mean(walk_out_money - 200)
}
plot(earning_series,xlab=&quot;casino wager limit&quot;,ylab=&quot;mean earning&quot;, main=&quot;How casino wager limit influence earning?&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/1rZdWUg4KsAntqj.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;play-times&quot;&gt;Play times&lt;/h2&gt;

&lt;p&gt;Next, we can save the times that the game played before walk out then find out the characteristics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;get_times &amp;lt;- function(x) length(x)
walk_out_times &amp;lt;- rep(NA, 10000)
for(j in seq_along(walk_out_times)){
  walk_out_times[j] &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_times
}

hist(walk_out_times, breaks = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/YqzUKQRmIahSVoE.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;mean(walk_out_times)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of walk out time is 203.0846.&lt;/p&gt;

&lt;p&gt;The limitation of simulation is obvious. It is a black box actually. We can not use it as we proof it by mathematical method. We don’t know why it happened and how it happened, so we can only change the parameter to try to understand the process. By the way, it is not a precise result. Everytime we get a answer, it will change a little bit next time.&lt;/p&gt;
</description>
        <pubDate>Mon, 26 Aug 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/08/26/roulette-simulation/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/26/roulette-simulation/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        
      </item>
    
  </channel>
</rss>
