<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zekun Blog</title>
    <description>Every failure is leading towards success.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 22 Dec 2019 12:41:45 -0600</pubDate>
    <lastBuildDate>Sun, 22 Dec 2019 12:41:45 -0600</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Power and sample size calculations correlational studies</title>
        <description>&lt;p&gt;A common research objective is to demonstrate that two measurements are highly correlated. One measurement, call it A, may reflect the severity of disease but is difficult or costly to collect. Another measurement, call it B, may be easier to collect and potentially related to measurement A. If there is strong association between A and B, a cost effective strategy for diagnosis may be to collect measurement B instead of A.&lt;/p&gt;

&lt;p&gt;The researcher will collect both measurements on N individuals. The analysis will proceed by calculating a one-sided confidence interval. If the confidence interval is completely within the range from 0.8 to 1, then the researcher will consider the study to be a success: A conclusive demonstration that the correlation between A and B is greater than 0.8.&lt;/p&gt;

&lt;p&gt;Power is the probability that the study will end in success when the true underlying correlation is. The code below provides the power calculation for a single combination of N and population correlation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;set.seed(1122)
suppressPackageStartupMessages(require(mvtnorm))
N &amp;lt;- 50
rho &amp;lt;- .95
null_correlation &amp;lt;- 0.8
R &amp;lt;- 5000

sigma &amp;lt;- array(c(1,rho,rho,1), c(2,2))
mu &amp;lt;- c(0,0)

detect &amp;lt;- rep(NA, R)
for(i in 1:R){
  data &amp;lt;- rmvnorm(N, mean = mu, sigma = sigma)
  results &amp;lt;- cor.test(x = data[,1], y = data[,2], alternative = &quot;greater&quot;)
  detect[i] &amp;lt;- results$conf.int[1] &amp;gt; null_correlation
}
power &amp;lt;- mean(detect)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the simulation part, we need to use correlations from 0.8 to 0.95 and the sample size from 25 to 100. Now, let’s create a table to save the generated powers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;corr_list &amp;lt;-  seq(0.8,0.95,0.01)
N_list &amp;lt;- seq(25,100,25)
result &amp;lt;- expand.grid(corr=corr_list, N = N_list, power=NA)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And use a for loop to apply all of the correlations and sample size and calculate the powers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for (j in 1:nrow(result)){
  N &amp;lt;- result[j,2]    #50
  rho &amp;lt;- result[j,1]  #.8
  null_correlation &amp;lt;- 0.8
  R &amp;lt;- 5000

  sigma &amp;lt;- array(c(1,rho,rho,1), c(2,2))
  mu &amp;lt;- c(0,0)

  detect &amp;lt;- rep(NA, R)
  for(i in 1:R){
    data &amp;lt;- rmvnorm(N, mean = mu, sigma = sigma)
    results &amp;lt;- cor.test(x = data[,1], y = data[,2], alternative = &quot;greater&quot;)
    detect[i] &amp;lt;- results$conf.int[1] &amp;gt; null_correlation
  }
  power &amp;lt;- mean(detect)
  result[j,3] &amp;lt;- power
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Transform the table into data frame type and plot the graph.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;result_df &amp;lt;- as.data.frame(result) %&amp;gt;%
  mutate(N=factor(N))

result_df %&amp;gt;%
  ggplot()+
  geom_line(aes(x=corr,y=power,color=N),size=1)+
  theme_bw()+
  labs(x=&quot;Correlation&quot;,y=&quot;Power&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/02c64Qkt/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, with the N increasing, the power under the certain correlation is growing up. In other words, when we can collect more samples, the probability that the study will end in success will be higher. And definitely, when the real correlation is higher, the probability will be higher.&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Nov 2019 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2019/11/22/power-correlation/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/22/power-correlation/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        <category>Correlation</category>
        
        
      </item>
    
      <item>
        <title>Central Limite Theorem - Approximation</title>
        <description>&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(magrittr)
library(sn)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;situation-description&quot;&gt;Situation Description&lt;/h1&gt;

&lt;p&gt;The central limit theorem is an important computational short-cut for generating and making inference from the sampling distribution of the mean. I will recall that the central limit theorem short-cut relies on a
number of conditions, specifically:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Independent observations&lt;/li&gt;
  &lt;li&gt;Identically distributed observations&lt;/li&gt;
  &lt;li&gt;Mean and variance exist&lt;/li&gt;
  &lt;li&gt;Sample size large enough for convergence&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this simulation study, I.m going to compare the sampling distribution of the mean generated by simulation to the sampling distribution implied by the central limit theorem. I will compare the distributions graphically in QQ-plots.&lt;/p&gt;

&lt;p&gt;This will be a 4 × 4 factorial experiment. The first factor will be the sample size, with N = 5, 10, 20, and 40. The second factor will be the degree of skewness in the underlying distribution. The underlying distribution will be the Skew-Normal distribution. The Skew-Normal distribution has three parameters: location
&lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt;, scale
&lt;script type=&quot;math/tex&quot;&gt;\omega&lt;/script&gt;, and slant
&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;. When the slant parameter is 0, the distribution reverts to the normal distribution. As the slant parameter increases, the distribution becomes increasingly skewed. In this simulation, slant will be set to 0, 2, 10, 100. Set location and scale to 0 and 1, respectively, for all simulation settings.&lt;/p&gt;

&lt;h1 id=&quot;plot-preparation&quot;&gt;Plot preparation&lt;/h1&gt;

&lt;p&gt;In the very beginning, we need to set up the parameters that do not change in the following steps. The slant of Skew-Normal distribution will change later, therefore, only the location $\xi$ and scale $\omega$ will be set in this part.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;R &amp;lt;- 5000
location &amp;lt;- 0
scale &amp;lt;- 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;location:$\xi$, scale:$\omega$, slant:$\alpha$&lt;/p&gt;

&lt;p&gt;Before creating the function, let’s clarify the functions for calculate the delta, mean and standard deviation for central limit theorem (CLT).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Delta:
&lt;script type=&quot;math/tex&quot;&gt;\delta =\frac{\alpha}{\sqrt{1+\alpha^2}}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mean:
&lt;script type=&quot;math/tex&quot;&gt;\xi+\omega \delta \sqrt{\frac{2}{\pi}}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Standard deviation:
&lt;script type=&quot;math/tex&quot;&gt;\sqrt{\omega^2(1-\frac{2\delta^2}{\pi})  }&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then define the function for CLT process and generating the QQplots by using the functions before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;qqplot_creator &amp;lt;- function(slant, N) {
  delta &amp;lt;- slant / (sqrt(1 + slant ^ 2))

  # Quantites to calculate/generate
  pop_mean &amp;lt;- location + scale * delta * (sqrt(2 / pi))
  pop_sd &amp;lt;- sqrt(scale ^ 2 * (1 - ((2 * delta ^ 2) / pi)))

  Z &amp;lt;- rnorm(R) # generate the normal distribution as the basement

  #CLT approximation
  sample_dist_clt &amp;lt;- Z * (pop_sd / sqrt(N)) + pop_mean

  #Simulation approximation
  random.skew &amp;lt;- array(rsn(R * N, xi = location, omega = scale, alpha = slant),
                      dim = c(R, N))

  sample_dist_sim &amp;lt;- apply(random.skew, 1, mean)

  qqplot(sample_dist_clt, sample_dist_sim, axes = FALSE, frame.plot=TRUE, ann = FALSE)
  abline(0,1)

  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;qqplot-generation&quot;&gt;QQplot generation&lt;/h1&gt;

&lt;p&gt;Now we can set the slants and Ns we want to test in the following steps. As the requirement, the N = 5, 10, 20, and 40 and slant will be set to 0, 2, 10, 100. Then create a sequence to define the points where we want to test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;slant &amp;lt;- c(0,2,10,100)
N &amp;lt;- c(5,10,20,40)
x &amp;lt;- seq(-2,2,0.01)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set a graph for put all of the QQplots together and use the qqplot_creator function to fill the QQplots inside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;par(mfrow=c(4,5),mai=c(0.1,0.1,0.1,0.1), oma = c(0, 4, 4, 0))

for(i in slant){
  plot(dsn(x,
           xi = location,
           omega = scale,
           alpha = i),
       axes = FALSE,
       frame.plot=TRUE,
       type = &quot;l&quot;,
       xlab = NA, ylab = NA)
  for(j in N){
    qqplot_creator(i, j)
  }
}
mtext(text=&quot;Distribution              N=5                N=10                   N=20                    N=40&quot;,
      side = 3,
      outer = TRUE)
mtext(text=&quot;slant = 100   slant = 10       slant = 2      slant = 0&quot;,
      side = 2,
      outer = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/rwCbSBq2/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Definitely, when the N is bigger, the QQplot will fit the y=x line better, which means the CLT works better when it wants to simulate the distribution. And when the slant is bigger, in other words, the Skew-Normal distribution has higher skewerness, it will be more difficult to simulate the distribution.&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/10/27/clt/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/10/27/clt/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        <category>CLT</category>
        
        
      </item>
    
      <item>
        <title>Simulation method comparison</title>
        <description>&lt;h1 id=&quot;situation-description&quot;&gt;Situation description&lt;/h1&gt;

&lt;p&gt;This time, I will perform a 2 × 4 × 2 factorial simulation study to compare the coverage probability of various methods of calculating &lt;strong&gt;90%&lt;/strong&gt; confidence intervals. The three factors in the experiment are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;True, underlying distribution
    &lt;ul&gt;
      &lt;li&gt;standard normal&lt;/li&gt;
      &lt;li&gt;gamma(shape = 1.4, scale = 3)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model
    &lt;ul&gt;
      &lt;li&gt;method of moments with normal&lt;/li&gt;
      &lt;li&gt;method of moments with gamma&lt;/li&gt;
      &lt;li&gt;kernel density estimation&lt;/li&gt;
      &lt;li&gt;bootstrap&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Parameter of interest
    &lt;ul&gt;
      &lt;li&gt;sample min (1st order statistic)&lt;/li&gt;
      &lt;li&gt;median
Other settings in the experiment that will not change are:
    -   Sample size, &lt;em&gt;N&lt;/em&gt; = 201
    -   &lt;em&gt;Outside the loop&lt;/em&gt; estimation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;define-functions&quot;&gt;Define functions&lt;/h1&gt;

&lt;h2 id=&quot;data-generation-function&quot;&gt;Data generation function&lt;/h2&gt;

&lt;p&gt;Set up data generate function what can generate data base on normal distribution or gamma distribution. The distribution will be defined by the parameter “dist”. The default parameter of gamma distribution are shape=1.4 and scale=3.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;generate_data &amp;lt;- function(N, dist, sh=1.4, sc=3){
  if(dist==&quot;norm&quot;){
    rnorm(N)
  }else if(dist==&quot;gamma&quot;){
    rgamma(N, shape=sh, scale=sc)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;confidence-interval-estimation-function&quot;&gt;Confidence interval estimation function&lt;/h2&gt;

&lt;p&gt;Then define the function for calculate the confidence interval, which can estimate the distribution by method of moment with normal distribution, method of moment with gamma distribution, kernal density distribution and bootstrap. At the same time, the function can use the function defined by “par.int” to calculate the parameter we want. The details will be introduced in the chunk.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;estimate.ci &amp;lt;- function(data, mod, par.int, R=10, smoo=0.3){
  # data: input data
  # mod: define the estimation method and the distribution.
  #      (MMnorm: method of moment with normal distribution,
  #       MMgamma: method of moment with gamma distribution,
  #       KDE: kernal density distribution,
  #       Boot: bootstrap)
  # par.int: the function what can get the parameter we want for the estimated distribution

  N &amp;lt;- length(data) # save N as the number of the inputted data
  sum.measure &amp;lt;- get(par.int) # make charactor can be used as function


  if(mod==&quot;MMnorm&quot;){
    # use method of moment with normal distribution to estimate

    mm.mean &amp;lt;- mean(data) # get the original mean of data
    mm.sd &amp;lt;- sd(data) # get the original standard deviation of data

    samp.dist &amp;lt;- NA #
    for(i in 1:R){
      sim.data &amp;lt;-rnorm(length(data), mm.mean, mm.sd)
      # use the mean &amp;amp; sd of data to generate a new normal distribution
      if(par.int==&quot;median&quot;){
        samp.dist[i] &amp;lt;- median(sim.data)
        # save the median of generated distribution
      }else if(par.int==&quot;min&quot;){
        samp.dist[i] &amp;lt;- min(sim.data)
        # save the min of generated distribution
      }
    }
    return(quantile(samp.dist, c(0.05, 0.95)))
    # the output will be the 95% confidence interval of needed distribution

  }else if(mod==&quot;MMgamma&quot;){
    # use method of moment with gamma distribution to estimate

    mm.shape &amp;lt;- mean(data)^2/var(data)
    # get the original shape parameter of data
    mm.scale &amp;lt;- var(data)/mean(data)
    # get the original scale parameter of data

    #create a N*R array to reserve the gamma distributions
    sim.data &amp;lt;- array(rgamma(length(data)*R, shape=mm.shape, scale=mm.scale), dim=c(N, R))
    # use the function called by par.int for each column and reserve the results as a distribution
    samp.dist &amp;lt;- apply(sim.data, 2, FUN=sum.measure)
    return(quantile(samp.dist, c(0.05, 0.95)))
    # the output will be the 95% confidence interval of needed distribution

  }else if(mod==&quot;KDE&quot;){
    # use kernal density estimation

    ecdfstar &amp;lt;- function(t, data, smooth=smoo){
        # use pnorm function which has sd=smoo on the outer product of t and data.
        # t[i] is the quantile, data[j] is the mean for pnorm.
        # in other words, calculate the persentage under quantile &quot;t&quot; of distribution &quot;data&quot;
        outer(t, data, function(a,b){ pnorm(a, b, smooth)}) %&amp;gt;%
        rowMeans
        # then count the mean of each row, AKA, the mean persentage under quantile &quot;t&quot;
    }

    # create a 1 column data frame with a sequence from min-sd to max+sd of the data with break=0.01
    tbl &amp;lt;- data.frame(
        x = seq(min(data)-sd(data), max(data)+sd(data),
                by = 0.01)
    )

    tbl$p &amp;lt;- ecdfstar(tbl$x, data, smoo)
    # add a new column called p reserve the result of ecdfstar function
    tbl &amp;lt;- tbl[!duplicated(tbl$p),]
    # remove all of rows which contain the duplicated value in column &quot;p&quot;.

    qkde &amp;lt;- function(ps, tbl){
      # convert &quot;ps&quot; to a factor with the break as column &quot;p&quot; in &quot;tbl&quot; data frame
      rows &amp;lt;- cut(ps, tbl$p, labels = FALSE)
      tbl[rows, &quot;x&quot;]
      # return the part of column &quot;x&quot; which match the rows number factor
    }

    U &amp;lt;- runif(N*R) # create a uniform distribution with the size=N*R

    # reserve the result of qkde as a N*R matrix
    sim.data &amp;lt;- array(qkde(U,tbl), dim=c(N, R))
    # use the function called by par.int for each column and reserve the results as a distribution
    samp.dist &amp;lt;- apply(sim.data, 2, sum.measure)

    return(quantile(samp.dist, c(0.05, 0.95), na.rm=TRUE))
    # the output will be the 95% confidence interval

  }else if(mod==&quot;Boot&quot;){
    # use bootstrap

    # get random sample with size=N from the data R times, reserve the result as a N*R matrix
    sim.data &amp;lt;- array(sample(data, N*R, replace=TRUE), dim=c(N,R))
    # use the function called by par.int for each column and reserve the results as a distribution
    samp.dist &amp;lt;- apply(sim.data, 2, sum.measure)

    return(quantile(samp.dist, c(0.05, 0.95), na.rm=TRUE))
    # the output will be the 95% confidence interval
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;destination-capture-function&quot;&gt;Destination capture function&lt;/h2&gt;

&lt;p&gt;Create a function to justify that is the confidence interval match the requirement. When the result is TRUE, return 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;capture_par &amp;lt;- function(ci, true.par){
  1*(ci[1] &amp;lt; true.par &amp;amp; true.par &amp;lt; ci[2])
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;simulation&quot;&gt;Simulation&lt;/h1&gt;

&lt;h2 id=&quot;calculation-prepare&quot;&gt;Calculation prepare&lt;/h2&gt;

&lt;p&gt;Now we can set the size of distribution N is 201. When use gamma distribution, we will use the shape 1.4 and scale 3.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;N &amp;lt;- 201
shape.set &amp;lt;- 1.4
scale.set &amp;lt;- 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Define the capture destinations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;true.norm.med &amp;lt;- qnorm(0.5)
true.norm.min &amp;lt;- mean(apply(array(rnorm(N*10000), dim=c(N, 10000)),2,min))
true.gamma.med &amp;lt;- qgamma(0.5, shape = shape.set, scale=scale.set)
true.gamma.min &amp;lt;- mean(apply(array(rgamma(N*10000, shape=shape.set, scale=scale.set), dim=c(N, 10000)),2,min))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the standard min of distribution part, we expand the data size for 10000 times and get the mean min value as the standard min.&lt;/p&gt;

&lt;p&gt;Create a table called “simsettings” to reserve the results of each estimation method and the target parameter.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;simsettings &amp;lt;- expand.grid(dist=c(&quot;norm&quot;, &quot;gamma&quot;), model=c(&quot;MMnorm&quot;, &quot;MMgamma&quot;, &quot;KDE&quot;, &quot;Boot&quot;), par.int=c(&quot;median&quot;, &quot;min&quot;), cov.prob=NA,  stringsAsFactors = FALSE, KEEP.OUT.ATTRS = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add a new column to rserve the capture destinations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;simsettings$truth &amp;lt;- c(true.norm.med, true.gamma.med, true.norm.med, true.gamma.med, true.norm.med, true.gamma.med, true.norm.med, true.gamma.med, true.norm.min, true.gamma.min, true.norm.min, true.gamma.min, true.norm.min, true.gamma.min, true.norm.min, true.gamma.min)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;calculation&quot;&gt;Calculation&lt;/h2&gt;

&lt;p&gt;Base on the “simsettings” table, calculate the capture probabilities and reserve them in the table.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(k in c(1:2,4:10,12:16)){
  dist1 &amp;lt;- simsettings[k,1]
  model1 &amp;lt;- simsettings[k,2]
  par.int1 &amp;lt;- simsettings[k,3]
  true.par1 &amp;lt;- simsettings[k,5]

  cover &amp;lt;- NA
  for(sims in 1:100){
    cover[sims] &amp;lt;- generate_data(N, dist1) %&amp;gt;%
    estimate.ci(mod=model1, par.int=par.int1, R=1000) %&amp;gt;%
    capture_par(true.par = true.par1)
  }
  simsettings[k,4] &amp;lt;- mean(cover)
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Show the result table.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;simsettings
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;##     dist   model par.int cov.prob       truth
## 1   norm  MMnorm  median     0.99  0.00000000
## 2  gamma  MMnorm  median     0.01  3.25311453
## 3   norm MMgamma  median       NA  0.00000000
## 4  gamma MMgamma  median     0.97  3.25311453
## 5   norm     KDE  median     0.94  0.00000000
## 6  gamma     KDE  median     0.93  3.25311453
## 7   norm    Boot  median     0.94  0.00000000
## 8  gamma    Boot  median     0.90  3.25311453
## 9   norm  MMnorm     min     1.00 -2.74462674
## 10 gamma  MMnorm     min     0.00  0.07277036
## 11  norm MMgamma     min       NA -2.74462674
## 12 gamma MMgamma     min     0.99  0.07277036
## 13  norm     KDE     min     0.94 -2.74462674
## 14 gamma     KDE     min     0.35  0.07277036
## 15  norm    Boot     min     0.40 -2.74462674
## 16 gamma    Boot     min     0.57  0.07277036
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Definitely, for the method of moment part, when the data distribution matches the model distribution (e.g. norm - MMnorm, gamma - MMgamma), the capture probabilities are more than 90%. When the distributions do not match with each other, the results will be close to 0% or NA.&lt;/p&gt;

&lt;p&gt;And for kernal density estimation, when we need to get the median of the distributions, the results are pretty good as close to 100%. But when we need to get the min of the distributions, the capture probabilities of normal and gamma are really different. For the normal distribution it works well, but for gamma distribution it is about 37%. The reason might be that gamma distribution is a skewed distribution.&lt;/p&gt;

&lt;p&gt;For bootstrap part, the results are not as good as others. When we want to get the median of the distributions, the capture probabilities are just about 90%. And when it comes to min of the distributions, the probabilities become 50%. Obviously, it is because selecting a sample randomly is too difficult to get the min point every time.&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/10/27/simulation-compare/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/10/27/simulation-compare/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        <category>Method of Moments</category>
        
        <category>KDE</category>
        
        <category>Bootstrap</category>
        
        
      </item>
    
      <item>
        <title>Coverage probability of MLE</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.
&lt;em&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;Wikipedia - Maximum likelihood estimation&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Coverage probability is an important operating characteristic of methods for constructing interval estimates, particularly confidence intervals.&lt;/p&gt;

&lt;p&gt;The destination of this blog is that performing a simulation to calculate the coverage probability of the 95% confidence interval of the median when computed from F&lt;sub&gt;X&lt;/sub&gt;&lt;sup&gt;mle&lt;/sup&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(stats4)
library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;step-1-single-sample&quot;&gt;Step 1: Single sample&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Generate a single sample from a standard normal distribution of size N = 201.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Fisrt, let’s set the size of distribution (N) as 201, and create a standard normal distribution as sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;N &amp;lt;- 201
sample1 &amp;lt;- rnorm(201)
median(sample1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then use the MLE function we learned before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;nll &amp;lt;- function(mean, sd) {
  fs &amp;lt;- dnorm(sample1,
              mean = mean,
              sd = sd,
              log = TRUE)
  - sum(fs, na.rm = T)
}

fit &amp;lt;- stats4::mle(
  nll,
  start = list(mean = 0, sd = 1),
  method = &quot;L-BFGS-B&quot;,
  lower = c(0, 0.01)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By the result of MLE, we can get the estimated mean and standard deviation of the sample distribution before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;coef(fit)
sample1_mean &amp;lt;- coef(fit)[[1]]
sample1_sd &amp;lt;- coef(fit)[[2]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated mean is &lt;code class=&quot;highlighter-rouge&quot;&gt;r sample1_mean&lt;/code&gt; and the estimated standard deviation is &lt;code class=&quot;highlighter-rouge&quot;&gt;r sample1_sd&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;step-2-approximate-median-distribution&quot;&gt;Step 2: Approximate median distribution&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Approximate the sampling distribution of the median, conditional on the estimate of the distribution in the previous step.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the previous step, we get a pair of estimated mean and standard deviation. Based on the parameter, we can generate new distributions to simulate the original distribution.&lt;/p&gt;

&lt;p&gt;Now we need to get the median of the new distribution, and if repeat the process for lots of time, we can get the distribution of median of the estimated distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;median_list &amp;lt;- rep(NA, 500)
for (i in seq_along(median_list)) {
  median_list[i] &amp;lt;-
    median(rnorm(N, mean = sample1_mean, sd = sample1_sd))
}
#hist(median_list,breaks = 100)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;step-3-calculate-95-ci&quot;&gt;Step 3: Calculate 95% CI&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Calculate a 95% confidence interval from the approximated sampling distribution.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For the median distribution we got in the previous step, we can use &lt;em&gt;quantile()&lt;/em&gt; function to get the 95% confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sample1_quantile95 &amp;lt;- quantile(median_list, c(0.025, 0.975))
sample1_quantile95
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;step-4-calculate-coverage-probability&quot;&gt;Step 4: Calculate coverage probability&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Now we will explain the concept of coverage probability. And calculating the coverage probability.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First, create a matrix to save the distributions we are going to generate. Every column is one distribution. Than use the method in the previous steps, generate all of the distributions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;samples &amp;lt;- matrix(nrow = N, ncol = 1000)
for (i in 1:ncol(samples)) {
  samples[, i] &amp;lt;- rnorm(N)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create a matrix to save the estimated parameters of each distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;coef &amp;lt;- matrix(nrow = 2, ncol = ncol(samples))
for (i in 1:ncol(samples)) {

  coef[1, i] &amp;lt;- mean(samples[, i]) #coef(fit)[[1]]
  coef[2, i] &amp;lt;- sd(samples[, i]) #coef(fit)[[2]]

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create a matrix to save 95% confidence interval.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;quantile95_list &amp;lt;- matrix(nrow = 2, ncol = ncol(samples))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each pair of estimated parameters, create a eatimated distribution, than get the median number. Than also do the process for &lt;code class=&quot;highlighter-rouge&quot;&gt;r N&lt;/code&gt; times, get the 95% confidence interval and save them in the matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for (j in 1:ncol(samples)) {
  median_list &amp;lt;- rep(NA, N)
  for (i in seq_along(median_list)) {
    norm_dist &amp;lt;- rnorm(N, mean = coef[1, j], sd = coef[2, j])
    median_list[i] &amp;lt;- median(norm_dist)
  }
  quantile95 &amp;lt;- quantile(median_list, c(0.025, 0.975))
  quantile95_list[1, j] &amp;lt;- quantile95[[1]]
  quantile95_list[2, j] &amp;lt;- quantile95[[2]]

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For calculating the coverage probability, we can save all of the results of judgement in a vector. Definitely, the coverage probability is the number of successful capture divede by the number of distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;interest &amp;lt;- rep(NA, ncol(samples))
for (i in seq_along(interest)) {
  interest[i] &amp;lt;-
    case_when(
      quantile95_list[1, i] &amp;gt; 0 | quantile95_list[2, i] &amp;lt; 0 ~ 0,
      quantile95_list[1, i] &amp;lt;= 0 &amp;amp;
        quantile95_list[2, i] &amp;gt;= 0 ~ 1,
    )
}
coverage_probability &amp;lt;- sum(interest) / length(interest)
sum(interest)
coverage_probability
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, the coverage probability is &lt;code class=&quot;highlighter-rouge&quot;&gt;r coverage_probability&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sum(quantile95_list[2,] &amp;lt; 0)
sum(quantile95_list[1,] &amp;gt; 0)

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;step-5-simulation-visualization&quot;&gt;Step 5: Simulation visualization&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Perform the simulation.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to make the results more intuitive, we can use &lt;em&gt;geom_linerange()&lt;/em&gt; function in &lt;em&gt;ggplot&lt;/em&gt; to draw the 95% confidence interval. And set the 95% confidence interval which did not capture the real mean of original distribution, “0” as a red line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;quantile95_df &amp;lt;- as.data.frame(t(quantile95_list))
quantile95_df[&quot;sample_num&quot;] &amp;lt;- seq(1:ncol(samples))
quantile95_df[&quot;interest&quot;] &amp;lt;- interest
ggplot(quantile95_df) +
  geom_linerange(aes(x = sample_num,
                     ymin = V1,
                     ymax = V2),
                 color = case_when(interest == 1 ~ &quot;blue&quot;,
                                   interest == 0 ~ &quot;red&quot;)) +
  geom_hline(yintercept = 0,
             color = &quot;gray&quot;,
             alpha = 0.7) +
  coord_flip() +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/9MPrGHm9/prob7-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;step-6-future-works&quot;&gt;Step 6: Future works&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;How to change the simulation to learn more about the operating characteristics of the chosen method for constructing the 95% confidence interval.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I think I will choose to use bigger distribution to test the process, likes set the &lt;em&gt;N&lt;/em&gt; as 5000 or 10000. And I could calculate the other quantile of the distributions to find out is there any different. After that, maybe I will use other distribution but not standard normal distribution, such as gamma or beta distribution.&lt;/p&gt;
</description>
        <pubDate>Tue, 15 Oct 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/10/15/coverage-probability/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/10/15/coverage-probability/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        <category>MLE</category>
        
        
      </item>
    
      <item>
        <title>Which quantiles of a continuous distribution can one estimate with more precision?</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;The median is an important quantity in data analysis. It represents the middle value of the data distribution. Estimates of the median, however, have a degree of uncertainty because (a) the estimates are calculated from a finite sample and (b) the data distribution of the underlying data is generally unknown. One important roles of a data scientist is to quantify and to communicate the degree of uncertainty in his or her data analysis.&lt;/p&gt;

&lt;p&gt;This time, we are going to find out what can make our sample distribution more precise during the simulation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(tidyverse)
library(reshape2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;standard-normal-distribution&quot;&gt;Standard Normal Distribution&lt;/h1&gt;

&lt;p&gt;First, we need to set up the initial parameters. For every simulation process, we will generate 200 values base on the standard normal distribution, and we will generate them for 5000 times.&lt;/p&gt;

&lt;p&gt;Therefore, set the sample size as 200, test number is 5000.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sample_size &amp;lt;- 200
test_num &amp;lt;- 5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then let’s create a function for find out the quantile values of each test and output them as sequences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;find_quantile_value &amp;lt;- function(fn,sample_size, quantile_position){
  sample &amp;lt;- fn(sample_size)
  qtseq &amp;lt;- quantile(sample, seq(0.05,0.95,0.05))
  qtseq[[quantile_position]]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to store the values of each quantile, we can create a data frame. For there are 19 quantiles and 5000 tests, the data frame should be a 5000*19 table.
Then use for loop traverse each test and each quantile.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sml_results &amp;lt;- as.data.frame(matrix(NA, nrow = test_num, ncol = 19))

for(i in 1:nrow(sml_results)){
  for(q in 1:ncol(sml_results)){
    sml_results[i,q] &amp;lt;- find_quantile_value(rnorm,sample_size, q)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we need to find the middle 95% length of the distributions of each qutaile value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_list &amp;lt;- rep(NA,ncol(sml_results))
for(q in 1:ncol(sml_results)){
  quantile_2.5_97.5&amp;lt;- quantile(sml_results[,q], c(0.025, 0.975))
  length_list[q] &amp;lt;- quantile_2.5_97.5[[2]]-quantile_2.5_97.5[[1]]
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the data we got before into a data frame and plot them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_df &amp;lt;- as.data.frame(matrix(ncol = 2, nrow = length(length_list)))
names(length_df) &amp;lt;- c(&quot;quantile_position&quot;,&quot;mid95_length&quot;)
length_df[,1] &amp;lt;- seq(0.05,0.95,0.05)
length_df[,2] &amp;lt;- length_list
ggplot(length_df,aes(x=quantile_position,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of normal distribution&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )+
  scale_y_continuous(name = &quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/J0jRCrkY/prob5-s1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;as we can see, when the quantile is approaching 50%, the length is going lower. It means that when the quantile is approaching 50%, simulation error is lower. It means when the quantile is 50%, it has the best precision.&lt;/p&gt;

&lt;p&gt;In other words, when the quantile is 50%, the median have the tightest sampling distribution.&lt;/p&gt;

&lt;p&gt;Then, we need to transfer the x-axis from quantile to the density values of the original distribution. In this part, it should be standard normal distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_d_df &amp;lt;- length_df %&amp;gt;%
  mutate(density=dnorm(qnorm(quantile_position)))

ggplot(length_d_df, aes(x=density,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of normal distribution by density&quot;, x= &quot;Density&quot;,y=&quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/9QLChND2/prob5-s2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this graph, we can find when density is bigger, the simulation error is lower.&lt;/p&gt;

&lt;h1 id=&quot;exponential-distribution&quot;&gt;Exponential Distribution&lt;/h1&gt;

&lt;p&gt;Distribution 2 is the exponential distribution with rate = 1.&lt;/p&gt;

&lt;p&gt;As we did before, calculate the values of each quantile and each test then plot the graph.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;
for(i in 1:nrow(sml_results)){
  for(q in 1:ncol(sml_results)){
    sml_results[i,q] &amp;lt;- find_quantile_value(rexp,sample_size, q)
  }
}

length_list &amp;lt;- rep(NA,ncol(sml_results))
for(q in 1:ncol(sml_results)){
  quantile_2.5_97.5&amp;lt;- quantile(sml_results[,q], c(0.025, 0.975))
  length_list[q] &amp;lt;- quantile_2.5_97.5[[2]]-quantile_2.5_97.5[[1]]
}

length_df &amp;lt;- as.data.frame(matrix(ncol = 2, nrow = length(length_list)))
names(length_df) &amp;lt;- c(&quot;quantile_position&quot;,&quot;mid95_length&quot;)
length_df[,1] &amp;lt;- seq(0.05,0.95,0.05)
length_df[,2] &amp;lt;- length_list
ggplot(length_df,aes(x=quantile_position,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of exponential distribution&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )+
  scale_y_continuous(name = &quot;Length&quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/cCtNMsh4/prob5-e1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For this exponential distribution, when quantile is bigger, the error of simulation is bigger.&lt;/p&gt;

&lt;p&gt;In other words, when the quantile is 5%, the median have the tightest sampling distribution.&lt;/p&gt;

&lt;p&gt;And when the Density is higher, the error is smaller.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_d_df &amp;lt;- length_df %&amp;gt;%
  mutate(density=dexp(qexp(quantile_position)))

ggplot(length_d_df, aes(x=density,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of exponential distribution by density&quot;, x= &quot;Density&quot;,y=&quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/GmdmryzF/prob5-e2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mixture-distribution-3&quot;&gt;Mixture Distribution 3&lt;/h1&gt;

&lt;p&gt;Distribution 3 is a mixture distribution defined by these functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;rf3 &amp;lt;- function(N){
  G &amp;lt;- sample(0:2, N, replace = TRUE, prob = c(5,3,2))
  (G==0)*rnorm(N) + (G==1)*rnorm(N,4) + (G==2)*rnorm(N,-4,2)
}

pf3 &amp;lt;- function(x){
  .5*pnorm(x) + .3*pnorm(x,4) + .2*pnorm(x,-4,2)
}

df3 &amp;lt;- function(x){
  .5*dnorm(x) + .3*dnorm(x,4) + .2*dnorm(x,-4,2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And just like we did in previous chunks, plot the middle 95% length of the quantiles.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(i in 1:nrow(sml_results)){
  for(q in 1:ncol(sml_results)){
    sml_results[i,q] &amp;lt;- find_quantile_value(rf3,sample_size, q)
  }
}

length_list &amp;lt;- rep(NA,ncol(sml_results))
for(q in 1:ncol(sml_results)){
  quantile_2.5_97.5&amp;lt;- quantile(sml_results[,q], c(0.025, 0.975))
  length_list[q] &amp;lt;- quantile_2.5_97.5[[2]]-quantile_2.5_97.5[[1]]
}

length_df &amp;lt;- as.data.frame(matrix(ncol = 2, nrow = length(length_list)))
names(length_df) &amp;lt;- c(&quot;quantile_position&quot;,&quot;mid95_length&quot;)
length_df[,1] &amp;lt;- seq(0.05,0.95,0.05)
length_df[,2] &amp;lt;- length_list
ggplot(length_df,aes(x=quantile_position,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of given mixture distribution 3&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )+
  scale_y_continuous(name = &quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/rwjFpwYv/prob5-m31.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Definitely, when the quantile is 40%, the median have the tightest sampling distribution.&lt;/p&gt;

&lt;p&gt;We don’t have the qf3 function, therefore we need to use pf3 function calculate the values when their counterpart quantile is setted. And the other part is same as previous chunks.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;pf3 &amp;lt;- function(x){
  .5*pnorm(x) + .3*pnorm(x,4) + .2*pnorm(x,-4,2)
}

quantile_seq &amp;lt;- seq(0.05,0.95,0.05)
qf3_seq &amp;lt;- rep(NA, length(quantile_seq))
for(i in seq_along(quantile_seq)){
qf3_seq[i] &amp;lt;- uniroot(function(x){pf3(x)-quantile_seq[i]}, c(-100,100))[[1]]
}

length_d_df &amp;lt;- length_df %&amp;gt;%
  mutate(density=df3(qf3_seq))

ggplot(length_d_df, aes(x=density,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of given mixture distribution 3 by density&quot;, x= &quot;Density&quot;,y=&quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/N00G2v3L/prob5-m32.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mixture-distribution-4&quot;&gt;Mixture Distribution 4&lt;/h1&gt;

&lt;p&gt;Mixture Distribution 4 is the following mixture distribution.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;rf4 &amp;lt;- function(N){
  G &amp;lt;- sample(0:1, N, replace = TRUE)
  (G==0)*rbeta(N,5,1) + (G==1)*rbeta(N,1,5)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As previous chunks, we can plot the length of each quantile.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(i in 1:nrow(sml_results)){
  for(q in 1:ncol(sml_results)){
    sml_results[i,q] &amp;lt;- find_quantile_value(rf4,sample_size, q)
  }
}

length_list &amp;lt;- rep(NA,ncol(sml_results))
for(q in 1:ncol(sml_results)){
  quantile_2.5_97.5&amp;lt;- quantile(sml_results[,q], c(0.025, 0.975))
  length_list[q] &amp;lt;- quantile_2.5_97.5[[2]]-quantile_2.5_97.5[[1]]
}

length_df &amp;lt;- as.data.frame(matrix(ncol = 2, nrow = length(length_list)))
names(length_df) &amp;lt;- c(&quot;quantile_position&quot;,&quot;mid95_length&quot;)
length_df[,1] &amp;lt;- seq(0.05,0.95,0.05)
length_df[,2] &amp;lt;- length_list
ggplot(length_df,aes(x=quantile_position,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of given mixture distribution 4&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )+
  scale_y_continuous(name = &quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/YSyr1y7R/prob5-m41.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Definitely, when the quantile is 5% or 95%, the median have the tightest sampling distribution.&lt;/p&gt;

&lt;p&gt;Base on the rf4 function we had, we can write the pf4 and df4 functions. After that, use the method we used plot the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;rf4 &amp;lt;- function(N){
  G &amp;lt;- sample(0:1, N, replace = TRUE)
  (G==0)*rbeta(N,5,1) + (G==1)*rbeta(N,1,5)
}

pf4 &amp;lt;- function(x){
  .5*pbeta(x,5,1) + .5*pbeta(x,1,5)
}

df4 &amp;lt;- function(x){
  .5*dbeta(x,5,1) + .5*dbeta(x,1,5)
}

quantile_seq &amp;lt;- seq(0.05,0.95,0.05)
qf4_seq &amp;lt;- rep(NA, length(quantile_seq))
for(i in seq_along(quantile_seq)){
qf4_seq[i] &amp;lt;- uniroot(function(x){pf4(x)-quantile_seq[i]}, c(-100,100))[[1]]
}


length_d_df &amp;lt;- length_df %&amp;gt;%
  mutate(density=df4(qf4_seq))

ggplot(length_d_df, aes(x=density,y=mid95_length))+
  geom_point()+geom_line()+
  labs(title = &quot;Length of middle 95% of given mixture distribution 4 by density&quot;, x= &quot;Density&quot;,y=&quot;Length&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/hjzSMbPB/prob5-m42.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;other-tests&quot;&gt;Other Tests&lt;/h1&gt;

&lt;p&gt;In this part, we will focus on the situations that when sample size become 400, 800 and 1600.&lt;/p&gt;

&lt;p&gt;Now use a for loop to generate all of the data given different sample sizes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sample_size &amp;lt;- c(400,800,1600)
test_num &amp;lt;- 5000

length_df &amp;lt;- as.data.frame(matrix(ncol = 1+length(sample_size), nrow = length(length_list)))
names(length_df) &amp;lt;- c(&quot;quantile_position&quot;,&quot;400_mid95_length&quot;,&quot;800_mid95_length&quot;,&quot;1600_mid95_length&quot;)
length_df[,1] &amp;lt;- seq(0.05,0.95,0.05)

for(n in seq_along(sample_size)){

  sml_results &amp;lt;- as.data.frame(matrix(NA, nrow = test_num, ncol = 19))

  for(i in 1:nrow(sml_results)){
    for(q in 1:ncol(sml_results)){
      sml_results[i,q] &amp;lt;- find_quantile_value(rnorm,sample_size[n], q)
    }
  }

  length_list &amp;lt;- rep(NA,ncol(sml_results))
  for(q in 1:ncol(sml_results)){
    quantile_2.5_97.5&amp;lt;- quantile(sml_results[,q], c(0.025, 0.975))
    length_list[q] &amp;lt;- quantile_2.5_97.5[[2]]-quantile_2.5_97.5[[1]]
  }

  length_df[,1+n] &amp;lt;- length_list
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Than plot the graph of different sample size and compare them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_df %&amp;gt;%
  melt(id = &quot;quantile_position&quot;) %&amp;gt;%
  ggplot()+
  geom_line(aes(x=quantile_position,y=value,color=variable))+
  labs(title = &quot;Length of middle 95% of normal distribution given different sample sizes&quot;, y = &quot;Length&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/Vkfm2cL4/prob5-o1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;length_df %&amp;gt;%
  mutate(density=dnorm(qnorm(quantile_position))) %&amp;gt;%
  select(-quantile_position) %&amp;gt;%
  melt(id = &quot;density&quot;) %&amp;gt;%
  ggplot()+
  geom_line(aes(x=density,y=value,color=variable))+
  labs(title = &quot;Length of middle 95% of normal distribution given different sample sizes&quot;, y = &quot;Length&quot;)+
  scale_x_continuous(
    name = &quot;pth quantile&quot;,
    breaks = seq(0.05, 0.95, 0.05)
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/J4cRThmG/prob5-o2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Definitely, when the sample size is bigger, the error of simulation will be smaller.&lt;/p&gt;

&lt;p&gt;In a word, when you have more simulation test, when the test number is increasing, the length will be smaller, what means the error of simulation is less than before.&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Sep 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/09/18/quantile-precision/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/09/18/quantile-precision/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        
      </item>
    
      <item>
        <title>If home field advantage exists, how much of an impact does it have on winning the world series?</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.technadu.com/wp-content/uploads/2019/05/World-Series-Logo.png&quot; alt=&quot;WORLD SERIES&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In team sports, the term home advantage – also called home ground, home field, home-field advantage, home court, home-court advantage, defender’s advantage or home-ice advantage – describes the benefit that the home team is said to gain over the visiting team. This benefit has been attributed to psychological effects supporting fans have on the competitors or referees; to psychological or physiological advantages of playing near home in familiar situations; to the disadvantages away teams suffer from changing time zones or climates, or from the rigors of travel; … In baseball, in particular, the difference may also be the result of the home team having been assembled to take advantage of the idiosyncrasies of the home ballpark, such as the distances to the outfield walls; most other sports are played in standardized venues.
&lt;em&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Home_advantage&quot;&gt;Wikipedia - Home advantage&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This time we will also talk about the competitions between the Braves and the Yankee, and the table below has the two possible schedules for each game of the series. (NYC = New York City, ATL = Atlanta)&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Overall advantage&lt;/th&gt;
      &lt;th&gt;Game 1&lt;/th&gt;
      &lt;th&gt;Game 2&lt;/th&gt;
      &lt;th&gt;Game 3&lt;/th&gt;
      &lt;th&gt;Game 4&lt;/th&gt;
      &lt;th&gt;Game 5&lt;/th&gt;
      &lt;th&gt;Game 6&lt;/th&gt;
      &lt;th&gt;Game 7&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Braves&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Yankees&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;NYC&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; be the probability that the Braves win a single head-to-head match-up with the Yankees, under the assumption that home field advantage doesn’t exist. Let &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;sup&gt;H&lt;/sup&gt;&lt;/em&gt; denote the probability that the Braves win a single head-to-head match-up with the Yankees as the home team (H for home). Let &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;sup&gt;A&lt;/sup&gt;&lt;/em&gt; denote the probability that the Braves win a single head-to-head match-up with the away team (A for away).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Game location&lt;/th&gt;
      &lt;th&gt;No advantage&lt;/th&gt;
      &lt;th&gt;Advantage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ATL&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;sup&gt;H&lt;/sup&gt; = P&lt;sub&gt;B&lt;/sub&gt; $\times$ 1.1&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NYC&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;sup&gt;A&lt;/sup&gt; = 1 - (1 - P&lt;sub&gt;B&lt;/sub&gt;)$\times$ 1.1&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(dplyr)
library(data.table)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let’s look at the questions.&lt;/p&gt;

&lt;h1 id=&quot;questions&quot;&gt;Questions&lt;/h1&gt;
&lt;h2 id=&quot;1-compute-analytically-the-probability-that-the-braves-win-the-world-series-when-the-sequence-of-game-locations-is-nyc-nyc-atl-atl-atl-nyc-nyc-calculate-the-probability-with-and-without-home-field-advantage-when-pb--055-what-is-the-difference-in-probabilities&quot;&gt;1. Compute analytically the probability that the Braves win the world series when the sequence of game locations is {NYC, NYC, ATL, ATL, ATL, NYC, NYC}. Calculate the probability with and without home field advantage when &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt; = 0.55&lt;/em&gt;. What is the difference in probabilities?&lt;/h2&gt;

&lt;p&gt;First, load the &lt;em&gt;.csv&lt;/em&gt; file to make sure what’s the situations we need to calculate, which represents the data we will generate later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Get all possible outcomes
apo &amp;lt;- fread(&quot;all-possible-world-series-outcomes.csv&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we need to define a sequence of game locations. This time, the sequence should be {NYC, NYC, ATL, ATL, ATL, NYC, NYC}. As a result of that the Braves is a Atlanta team, we use 1 to represent Atlanta and use 0 to represent NYC.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Home field indicator
hfi &amp;lt;- c(0,0,1,1,1,0,0) #{NYC, NYC, ATL, ATL, ATL, NYC, NYC}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we use 0.55 to define the &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; and generate other probabilities by &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; as we talked before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# P_B
pb &amp;lt;- 0.55
advantage_multiplier &amp;lt;- 1.1 # Set = 1 for no advantage
pbh &amp;lt;- pb * advantage_multiplier
pba &amp;lt;- 1 - (1 - pb) * advantage_multiplier
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this part, we will use the parameters we defined before. In every row of data.table, we use the different probabilities which influenced by home field advantage to calculate the overall probabilties of each situation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Calculate the probability of each possible outcome
apo[, p := NA_real_] # Initialize new column in apo to store prob
for(i in 1:nrow(apo)){
  prob_game &amp;lt;- rep(1, 7)
  for(j in 1:7){
    p_win &amp;lt;- ifelse(hfi[j], pbh, pba)
    prob_game[j] &amp;lt;- case_when(
        apo[i,j,with=FALSE] == &quot;W&quot; ~ p_win
      , apo[i,j,with=FALSE] == &quot;L&quot; ~ 1 - p_win
      , TRUE ~ 1
    )
  }
  apo[i, p := prod(prob_game)] # Data.table syntax
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we output the probability of that the Braves wins the World Series under the influence of home field advantage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Probability of overall World Series outcomes
p_home &amp;lt;- purrr::flatten_dbl(apo[, sum(p), overall_outcome][1,2])
p_home
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability is &lt;code class=&quot;highlighter-rouge&quot;&gt;r p_home&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then we can calculate the probability when there is no home field advantage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;p_nohome &amp;lt;- 1 - pbinom(3, 7, 0.55)
p_nohome
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probabilty is &lt;code class=&quot;highlighter-rouge&quot;&gt;r p_nohome&lt;/code&gt;. Definitely, when home field advantage exists, the probabity that the Braves win the World Series is lower, &lt;code class=&quot;highlighter-rouge&quot;&gt;r p_home&lt;/code&gt;, than the probabity without home field advantage. Given &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;=0.55&lt;/em&gt;, the probability of the Braves winning the World Series with a home field advantage &lt;code class=&quot;highlighter-rouge&quot;&gt;r (p_home-p_nohome)*100 &lt;/code&gt;a less likely than the probability of the Braves winning the world Series without a home field advantage.&lt;/p&gt;

&lt;h2 id=&quot;2-calculate-the-same-probabilities-as-the-previous-question-by-simulation&quot;&gt;2. Calculate the same probabilities as the previous question by simulation.&lt;/h2&gt;

&lt;p&gt;In this part, we will use simulation to test the probability.&lt;/p&gt;

&lt;p&gt;Given the location sequence, we use different winning probabilities of head to head games and random generate the result of each game. Repeat the process 100000 times, we can get the approximate solution of the probabilty with home field advantage influence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;set.seed(314)
sml_list_h &amp;lt;- rep(NA, 100000)
for (i in seq_along(sml_list_h)){
  round &amp;lt;- rep(NA,7)
  for(j in 1:7){
    p_win &amp;lt;- ifelse(hfi[j], pbh, pba)
    round[j] &amp;lt;- rbinom(1,1,p_win)
  }
  sml_list_h[i] &amp;lt;- ifelse(sum(round)&amp;gt;=4, 1, 0)
}
mean_sml_h &amp;lt;- mean(sml_list_h)
mean_sml_h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can get the approximate solution, &lt;code class=&quot;highlighter-rouge&quot;&gt;r mean_sml_h&lt;/code&gt;, which is a little different from &lt;code class=&quot;highlighter-rouge&quot;&gt;r p_home&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now, let’s simulate the situation without home field advantage influence. It’s easy because we only need to make p_win as a constant value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;set.seed(314)
sml_list_nh &amp;lt;- rep(NA, 100000)
for (i in seq_along(sml_list_nh)){
  round &amp;lt;- rep(NA,7)
  for(j in 1:7){
    p_win &amp;lt;- 0.55
    round[j] &amp;lt;- rbinom(1,1,p_win)
  }
  sml_list_nh[i] &amp;lt;- ifelse(sum(round)&amp;gt;=4, 1, 0)
}
mean_sml_nh &amp;lt;- mean(sml_list_nh)
mean_sml_nh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;3-what-is-the-absolute-and-relative-error-for-your-simulation-in-the-previous-question&quot;&gt;3. What is the absolute and relative error for your simulation in the previous question?&lt;/h2&gt;

&lt;p&gt;Absolute error =
&lt;script type=&quot;math/tex&quot;&gt;|p̂−p|&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Relative error =
&lt;script type=&quot;math/tex&quot;&gt;|p̂−p|/p&lt;/script&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;abs_error_h &amp;lt;- abs(mean(sml_list_h) - p_home)
rel_error_h &amp;lt;- abs(mean(sml_list_h) - p_home)/mean(sml_list_h)
abs_error_h
rel_error_h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, given home field advantage the absolute error is &lt;code class=&quot;highlighter-rouge&quot;&gt;r abs_error_h&lt;/code&gt;. The relative error is &lt;code class=&quot;highlighter-rouge&quot;&gt;r rel_error_h&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;abs_error_nh &amp;lt;- abs(mean(sml_list_nh) - p_nohome)
rel_error_nh &amp;lt;- abs(mean(sml_list_nh) - p_nohome)/mean(sml_list_nh)
abs_error_nh
rel_error_nh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, given no home field advantage the absolute error is &lt;code class=&quot;highlighter-rouge&quot;&gt;r abs_error_nh&lt;/code&gt;. The relative error is &lt;code class=&quot;highlighter-rouge&quot;&gt;r rel_error_nh&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;bonus-1-does-the-difference-in-probabilites-with-vs-without-home-field-advantage-depend-on-pb&quot;&gt;Bonus 1. Does the difference in probabilites (with vs without home field advantage) depend on &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;?&lt;/h2&gt;

&lt;p&gt;The process is similar with the answer of question 1.&lt;/p&gt;

&lt;p&gt;We can create some lists to save the different &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;s, the probabilities of win the World Series with or without home field advantage, and the difference between these two situations given different &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Then given every &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt;, calculate these values every time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;pb_list &amp;lt;- seq(0,1,0.01)
ph_win_list &amp;lt;- seq_along(pb_list)
pnh_win_list &amp;lt;- ph_win_list
diff_list &amp;lt;- ph_win_list
for (p in seq_along(pb_list)) {
  pb &amp;lt;- pb_list[p]
  advantage_multiplier &amp;lt;- 1.1
  pbh &amp;lt;- pb * advantage_multiplier
  pba &amp;lt;- 1 - (1 - pb) * advantage_multiplier

  pnh_win_list[p] &amp;lt;- 1 - pbinom(3, 7, pb_list[p])

  # Calculate the probability of each possible outcome
  apo[, p := NA_real_] # Initialize new column in apo to store prob
  for(i in 1:nrow(apo)){
    prob_game &amp;lt;- rep(1, 7)
    for(j in 1:7){
      p_win &amp;lt;- ifelse(hfi[j], pbh, pba)
      prob_game[j] &amp;lt;- case_when(
          apo[i,j,with=FALSE] == &quot;W&quot; ~ p_win
        , apo[i,j,with=FALSE] == &quot;L&quot; ~ 1 - p_win
        , TRUE ~ 1
      )
    }
  apo[i, p := prod(prob_game)] # Data.table syntax
  }
  ph_win_list[p] &amp;lt;- purrr::flatten_dbl(apo[, sum(p), overall_outcome][1][,2])
  diff_list[p] &amp;lt;- pnh_win_list[p] - ph_win_list[p]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, plot a graph to show the relationship between &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; and differences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;plot(x= pb_list, y=diff_list)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/65B70gRL/prob4-b1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Obviously, the relationship between &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; and differences is a compound function containing trigonometric functions.&lt;/p&gt;

&lt;p&gt;And the format might be a trigonometric function times a function, that when going to the start and the end of one period approximate 0, otherwise approximate 1. By the way, the period should be 1.&lt;/p&gt;

&lt;p&gt;There are some examples.
&lt;script type=&quot;math/tex&quot;&gt;y = A \cdot sin(B \cdot x + C)
\\
y = A \cdot sin(B \cdot x + C) \cdot |D \cdot sin(\pi \cdot x + E)|
\\
y = A \cdot sin(B \cdot x + C) \cdot  ( D \cdot sin(\pi \cdot x + E))^2
\\
y = Ax^3 + Bx^2y + Cxy^2+Dy^3 +Ex^2+Fxy+Gy^2+Hx+Iy+J&lt;/script&gt;
Unfortunately, after these functions I listed cannot regress to the graph we got.&lt;/p&gt;

&lt;h2 id=&quot;bonus-2-does-the-difference-in-probabilites-with-vs-without-home-field-advantage-depend-on-the-advantage-factor-the-advantage-factor-in-pbh-and-pba-is-the-11-multiplier-that-results-in-a-10-increase-for-the-home-team&quot;&gt;Bonus 2. Does the difference in probabilites (with vs without home field advantage) depend on the advantage factor? (The advantage factor in PBH and PBA is the 1.1 multiplier that results in a 10% increase for the home team.)&lt;/h2&gt;

&lt;p&gt;In this question, the process is similar to Bonus1. We only need to change the sequence content from &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; to home field advantage factor and make &lt;em&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/em&gt; be a constant(0.55). Therefore, we will use &lt;em&gt;ha_list&lt;/em&gt; to save the sequence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;ha_list &amp;lt;- seq(1,2,0.01)
ph_win_list &amp;lt;- seq_along(ha_list)
pnh_win_list &amp;lt;- ph_win_list
diff_list2 &amp;lt;- ph_win_list
for (p in seq_along(ha_list)) {
  pb &amp;lt;- 0.55
  advantage_multiplier &amp;lt;- ha_list[p]
  pbh &amp;lt;- pb * advantage_multiplier
  pba &amp;lt;- 1 - (1 - pb) * advantage_multiplier

  pnh_win_list[p] &amp;lt;- 1 - pbinom(3, 7, 0.55)

  # Calculate the probability of each possible outcome
  apo[, p := NA_real_] # Initialize new column in apo to store prob
  for(i in 1:nrow(apo)){
    prob_game &amp;lt;- rep(1, 7)
    for(j in 1:7){
      p_win &amp;lt;- ifelse(hfi[j], pbh, pba)
      prob_game[j] &amp;lt;- case_when(
          apo[i,j,with=FALSE] == &quot;W&quot; ~ p_win
        , apo[i,j,with=FALSE] == &quot;L&quot; ~ 1 - p_win
        , TRUE ~ 1
      )
    }
  apo[i, p := prod(prob_game)] # Data.table syntax
  }
  ph_win_list[p] &amp;lt;- purrr::flatten_dbl(apo[, sum(p), overall_outcome][1][,2])
  diff_list2[p] &amp;lt;- pnh_win_list[p] - ph_win_list[p]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s look at the graph. Obviously, when the home field advantage factor is increasing, the difference in probabilities between with and without home filed advantage will increase too.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;plot(x= ha_list, y=diff_list2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/rmhHPvs0/prob4-b2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Sep 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/09/12/home-advantage/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/09/12/home-advantage/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        
      </item>
    
      <item>
        <title>How often does the better team win the World Series?</title>
        <description>&lt;p&gt;Import the package at the very first.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.technadu.com/wp-content/uploads/2019/05/World-Series-Logo.png&quot; alt=&quot;WORLD SERIES&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The World Series is the annual championship series of Major League Baseball (MLB) in North America, contested since 1903 between the American League (AL) champion team and the National League (NL) champion team. The winner of the World Series championship is determined through a best-of-seven playoff, and the winning team is awarded the Commissioner’s Trophy. As the series is played during the fall season in North America, it is sometimes referred to as the Fall Classic.&lt;br /&gt;
&lt;em&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/World_Series&quot;&gt;Wikipedia - World Series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this blog, we are going to calculate the probability of several questions about the Braves and the Yankees in the World Series.&lt;/p&gt;

&lt;p&gt;First, we need to define some parameter.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameter&lt;/th&gt;
      &lt;th&gt;Explaination&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;P&lt;sub&gt;B&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;In any given game, the probability that the Braves win&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;P&lt;sub&gt;Y&lt;/sub&gt; = 1 - P&lt;sub&gt;B&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;in any given game, the probability that the Yankees win&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;questions&quot;&gt;Questions&lt;/h1&gt;
&lt;h3 id=&quot;1-what-is-the-probability-that-the-braves-win-the-world-series-given-that-pb055&quot;&gt;1. What is the probability that the Braves win the World Series given that P&lt;sub&gt;B&lt;/sub&gt;=0.55?&lt;/h3&gt;

&lt;p&gt;First,we need to set the value of P&lt;sub&gt;B&lt;/sub&gt; and P&lt;sub&gt;Y&lt;/sub&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;PB &amp;lt;- 0.55
PY &amp;lt;- 1- PB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a function to calculate the probability of win. Win is defined as win 4 times in 7 games.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;calc_prob &amp;lt;- function(p){
  pnbinom(3, 4, p)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now calculate the probability given that P&lt;sub&gt;B&lt;/sub&gt;=0.55.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;calc_prob(PB)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, when the P&lt;sub&gt;B&lt;/sub&gt; is 0.55, the probability that Braves win the World Series is 0.608.&lt;/p&gt;

&lt;h3 id=&quot;2-what-is-the-probability-that-the-braves-win-the-world-series-given-that-pbx&quot;&gt;2. What is the probability that the Braves win the World Series given that P&lt;sub&gt;B&lt;/sub&gt;=x?&lt;/h3&gt;
&lt;p&gt;Now the P&lt;sub&gt;B&lt;/sub&gt; is not defined yet, so we should assume the x could be any number between 0.5 to 1.&lt;/p&gt;

&lt;p&gt;First, we need to generate the series of P&lt;sub&gt;B&lt;/sub&gt; and the probability results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;PBseries &amp;lt;- seq(0.5, 1, 0.01)
win_prob &amp;lt;- rep(NA, length(PBseries))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now use the function we used before to calculate the probability given every P&lt;sub&gt;B&lt;/sub&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(i in 1:length(win_prob)){
  win_prob[i] &amp;lt;- calc_prob(PBseries[i])
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to intepret the relationship between P&lt;sub&gt;B&lt;/sub&gt; and the probability that the Braves win, we can draw a graph for them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;plot(x = PBseries,
     y = win_prob,
     xlim = c(0.5,1),
     ylim = 0:1,
     xlab = &quot;Probability of the Braves winning a head-to-head matchup&quot;,
     ylab = &quot;P(Braves win World Series)&quot;,
     main = &quot;Probability of winning the World Series&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/Bh7NjCIcKU2Myab.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, in this graph, when P&lt;sub&gt;B&lt;/sub&gt; is increasing, the probability that the Braves win the World Series is increasing too. In fact, when we change the x scale to 0.0-1.0, we will find the line looks like a logistic curve.&lt;/p&gt;

&lt;h3 id=&quot;3-suppose-one-could-change-the-world-series-to-be-best-of-9-or-some-other-best-of-x-series-what-is-the-shortest-series-length-so-that-pbraves-win-world-seriespb055--08&quot;&gt;3. Suppose one could change the World Series to be best-of-9 or some other best-of-X series. What is the shortest series length so that P(Braves win World Series|P&lt;sub&gt;B&lt;/sub&gt;=0.55) ≥ 0.8?&lt;/h3&gt;

&lt;p&gt;As same as the first question, the P&lt;sub&gt;B&lt;/sub&gt; needs to be 0.55. And now the game series length is not a certain. Definitely, the series length should be an odd number.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;PB &amp;lt;- 0.55
series_length &amp;lt;- seq(1, 999, 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we need to create a function to calculate the probability when the series length is parameter.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;calc_prob_sl &amp;lt;- function(sl){
  win_threshhold &amp;lt;- ceiling(sl/2)
  pnbinom(win_threshhold - 1, win_threshhold, 0.55)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end, given every series length, calculate the probability that the Braves win World Series. When the probability is equal to or more than 0.8, stop running and give the series length value and the probability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(i in 1:length(series_length)){
  pb_win &amp;lt;- calc_prob_sl(series_length[i])
  if(pb_win &amp;gt;= 0.8){
    shortest &amp;lt;- series_length[i]
    p_shortest &amp;lt;- pb_win
    break}
}
shortest
p_shortest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we get the the shortest series length. It should be 71. In that situation, the probability that the Braves win World Series is about 0.802.&lt;/p&gt;

&lt;h3 id=&quot;4-what-is-the-shortest-series-length-so-that-pbraves-win-world-seriespb-x--08-this-will-be-a-figure-see-below-with-pb-on-the-x-axis-and-series-length-is-the-y-axis&quot;&gt;4. What is the shortest series length so that P(Braves win World Series|P&lt;sub&gt;B&lt;/sub&gt;= x) ≥ 0.8? This will be a figure (see below) with P&lt;sub&gt;B&lt;/sub&gt; on the x-axis and series length is the y-axis.&lt;/h3&gt;
&lt;p&gt;Now the P&lt;sub&gt;B&lt;/sub&gt; is not defined again, so we should assume the x could be any number between 0.51 to 1.&lt;/p&gt;

&lt;p&gt;First, we need to generate the series of P&lt;sub&gt;B&lt;/sub&gt; and a series to save the length results given different P&lt;sub&gt;B&lt;/sub&gt;. But the way, we also need a series of the possible series length we will test. Now the ceiling is 9999. If it’s not enough, we can set a bigger limitation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;PBseries &amp;lt;- seq(0.51, 1, 0.01)
length_record &amp;lt;- rep(NA, length(PBseries))
series_length &amp;lt;- seq(1, 9999, 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to calculate the probabilty that the Braves win the WS, we need a new function with 2 input, because both of the series length and P&lt;sub&gt;B&lt;/sub&gt; are variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;calc_prob_sl_p &amp;lt;- function(sl,pb){
  win_threshhold &amp;lt;- ceiling(sl/2)
  pnbinom(win_threshhold - 1, win_threshhold, pb)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, calculate the shortest series length when P&lt;sub&gt;B&lt;/sub&gt; is changing. Save the values in &lt;em&gt;length_record&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(j in 1:length(PBseries)){
  for(i in 1:length(series_length)){
  pb_win &amp;lt;- calc_prob_sl_p(series_length[i],PBseries[j])
  if(pb_win &amp;gt;= 0.8){
    shortest &amp;lt;- series_length[i]
    break}
  }
  length_record[j] &amp;lt;- shortest
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have already get the shortest series length given different P&lt;sub&gt;B&lt;/sub&gt;. Let’s draw the figure to show the relationship between them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;plot(x = PBseries,
     y = length_record,
     xlim = c(0.5,1),
     xlab = &quot;Probability of the Braves winning a head-to-head matchup&quot;,
     ylab = &quot;Series length&quot;,
     main = &quot;Shortest series so that P(Win WS given p)&amp;gt;=0.8&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/jA2LkzaNK5tF6OB.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;in this graph, when P&lt;sub&gt;B&lt;/sub&gt; is increasing, the shortest series length, when the probability that the Braves win the World Series is more than 0.8, is approching 1. When the P&lt;sub&gt;B&lt;/sub&gt; is bigger than 0.8, the shortest series length is 1.&lt;/p&gt;

&lt;h3 id=&quot;5-calculate-p-pb055braves-lose-3-games-before-winning-a-4th-game-under-the-assumption-that-either--pb055-or--pb045-explain-your-solution&quot;&gt;5. Calculate P( P&lt;sub&gt;B&lt;/sub&gt;=0.55|Braves lose 3 games before winning a 4th game) under the assumption that either  P&lt;sub&gt;B&lt;/sub&gt;=0.55 or  P&lt;sub&gt;B&lt;/sub&gt;=0.45. Explain your solution.&lt;/h3&gt;

&lt;p&gt;According to Conditional probability formula，we can get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B)=\frac{P(A)P(B)}{P(B)}  \to  P(A)P(B)=P(A|B)P(B)\\
P(B|A)=\frac{P(A)P(B)}{P(A)}  \to  P(A)P(B)=P(B|A)P(A)\\
\to P(A|B)P(B)=P(A)P(B)=P(B|A)P(A)\\
\to P(A|B)=\frac{P(B|A)P(A)}{P(B)}&lt;/script&gt;

&lt;p&gt;Now the P(A) = P(P&lt;sub&gt;B&lt;/sub&gt;=0.55), P(B) = P(Braves lose 3 games before winning a 4th game).
As a result, P( P&lt;sub&gt;B&lt;/sub&gt;=0.55|Braves lose 3 games before winning a 4th game) = P(Braves lose 3 games before winning a 4th game|P&lt;sub&gt;B&lt;/sub&gt;=0.55) * P(P&lt;sub&gt;B&lt;/sub&gt;=0.55) ÷ P(Braves lose 3 games before winning a 4th game).&lt;/p&gt;

&lt;p&gt;P(P&lt;sub&gt;B&lt;/sub&gt;=0.55) = 0.5&lt;/p&gt;

&lt;p&gt;Then use &lt;em&gt;dnbinom()&lt;/em&gt; calculate P(Braves lose 3 games before winning a 4th game) and P(Braves lose 3 games before winning a 4th game|P&lt;sub&gt;B&lt;/sub&gt;=0.55):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;(dnbinom(3,4,0.45)+dnbinom(3,4,0.55))/2
dnbinom(3,4,0.55)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;P(Braves lose 3 games before winning a 4th game) = 0.1516092&lt;/p&gt;

&lt;p&gt;P(Braves lose 3 games before winning a 4th game | P&lt;sub&gt;B&lt;/sub&gt;=0.55) = 0.1667701&lt;/p&gt;

&lt;p&gt;P( P&lt;sub&gt;B&lt;/sub&gt;=0.55|Braves lose 3 games before winning a 4th game) = P(Braves lose 3 games before winning a 4th game|P&lt;sub&gt;B&lt;/sub&gt;=0.55) * P(P&lt;sub&gt;B&lt;/sub&gt;=0.55) ÷ P(Braves lose 3 games before winning a 4th game)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;0.1667701 * 0.5 / 0.1516092
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;P( P&lt;sub&gt;B&lt;/sub&gt;=0.55|Braves lose 3 games before winning a 4th game) = 0.1667701 * 0.5 ÷ 0.1516092 = 0.5499999&lt;/p&gt;

&lt;p&gt;Therefore, P( P&lt;sub&gt;B&lt;/sub&gt;=0.55|Braves lose 3 games before winning a 4th game) is 0.5499999, about 0.55.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Sep 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/09/09/world-series/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/09/09/world-series/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        
      </item>
    
      <item>
        <title>What’s the difference between Absolute Error and Relative Error?</title>
        <description>&lt;p&gt;Absolute error and relative error have a little bit difference on calculation but the difference really makes an essential change. Absolute error will be influenced by the samples size but relative error can show the real error without huge bias.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;when used as a measure of precision—is the ratio of the absolute error of a measurement to the measurement being taken. In other words, this type of error is relative to the size of the item being measured. RE is expressed as a percentage and has no units.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;From &lt;a href=&quot;https://www.statisticshowto.datasciencecentral.com/relative-error/&quot;&gt;Statistics How To&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;absolute-error&quot;&gt;Absolute Error&lt;/h2&gt;
&lt;p&gt;absolute error = |p̂−p|&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The difference between the measured or inferred value of a quantity x_0 and its actual value x.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let’s create a table to save 14*5 results of our simulation first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;n &amp;lt;- rep(NA, 14)
for(i in 1:14){n[i] &amp;lt;- 2^(i+1)}
T &amp;lt;- matrix(NA,14,5)
p &amp;lt;- c(0.01,0.05,0.10,0.25,0.5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Than generate 1 or 0 randomly for 1000 times for each situation and calculate the absolute error.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(x in 1:length(p)){
  for(y in 1:length(n)){
    TS &amp;lt;- rep(NA,10000)
    for(m in 1:10000){
      TS[m] &amp;lt;- abs(rbinom(1,n[y],p[x])/n[y]-p[x])
    }
    T[y,x] &amp;lt;- mean(TS)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change the y-scale to log_10.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;T &amp;lt;- log10(T)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end, plot the graph to show the relationship between p and absolute error.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;plot(T[,5],xlim=c(0,14),ylim=range(T),col=&quot;red&quot;,type=&quot;b&quot;,xaxt=&quot;n&quot;,xlab=&quot;N(log_2 scale)&quot;,ylab=&quot;Absolute Error&quot;,pch=16, lwd=3)
lines(T[,2],col=&quot;purple&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T[,3],col=&quot;blue&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T[,4],col=&quot;green&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T[,1],col=&quot;gray&quot;,type=&quot;b&quot;,pch=16, lwd=3)

lname &amp;lt;- c(&quot;0.01&quot;,&quot;0.05&quot;,&quot;0.10&quot;,&quot;0.25&quot;,&quot;0.50&quot;)
lname_p &amp;lt;- paste0(&quot;p = &quot;,lname)
xname &amp;lt;- c(&quot;4&quot;,&quot;8&quot;,&quot;16&quot;,&quot;32&quot;,&quot;64&quot;,&quot;128&quot;,&quot;256&quot;,&quot;512&quot;,&quot;1024&quot;,&quot;2048&quot;,&quot;4096&quot;,&quot;8192&quot;,&quot;16384&quot;,&quot;32768&quot;)

axis(1, at=1:14,las=2, lab=xname)
text(1,T[1,],lname_p,pos=2,cex=0.6)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/6yheRBQ3HIMnDcd.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Absolute error is just the absolute value of the value you have minus expected value. In this simulation, you culculated the absolute error 10000 times and get the mean value of them in every situation.
When we transfer the y scale to log_10, it is obvious that the x &amp;amp; y have linear connection.
The p is higher, the absolute error is bigger.&lt;/p&gt;

&lt;h1 id=&quot;relative-error&quot;&gt;Relative Error&lt;/h1&gt;
&lt;p&gt;relative error = |p̂−p|/p.&lt;/p&gt;

&lt;p&gt;Then do the same thing as before but when we calculate the error, use absolute error divide by p value.
Plot the graph too.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;n &amp;lt;- rep(NA, 14)
for(i in 1:14){n[i] &amp;lt;- 2^(i+1)}

T2 &amp;lt;- matrix(NA,14,5)
p &amp;lt;- c(0.01,0.05,0.10,0.25,0.5)

for(x in 1:length(p)){
  for(y in 1:length(n)){
    T2S &amp;lt;- rep(NA,10000)
    for(m in 1:10000){
      T2S[m] &amp;lt;- abs(rbinom(1,n[y],p[x])/n[y]-p[x])/p[x]
    }
    # T2[y,x] &amp;lt;- abs(rbinom(1,n[y],p[x])/n[y]-p[x])/p[x]
    T2[y,x] &amp;lt;- mean(T2S)
  }
}

T2 &amp;lt;- log10(T2)

xname &amp;lt;- c(&quot;4&quot;,&quot;8&quot;,&quot;16&quot;,&quot;32&quot;,&quot;64&quot;,&quot;128&quot;,&quot;256&quot;,&quot;512&quot;,&quot;1024&quot;,&quot;2048&quot;,&quot;4096&quot;,&quot;8192&quot;,&quot;16384&quot;,&quot;32768&quot;)
lname &amp;lt;- c(&quot;0.01&quot;,&quot;0.05&quot;,&quot;0.10&quot;,&quot;0.25&quot;,&quot;0.50&quot;)
lname_p &amp;lt;- paste0(&quot;p = &quot;,lname)

plot(T2[,1],xlim=c(0,14),ylim=range(T2),col=&quot;red&quot;,type=&quot;b&quot;,pch=16,xaxt=&quot;n&quot;,xlab=&quot;N(log_2 scale)&quot;,ylab=&quot;Relative Error&quot;, lwd=3)
lines(T2[,2],col=&quot;purple&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T2[,3],col=&quot;blue&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T2[,4],col=&quot;green&quot;,type=&quot;b&quot;,pch=16, lwd=3)
lines(T2[,5],col=&quot;gray&quot;,type=&quot;b&quot;,pch=16, lwd=3)
axis(1, at=1:14,las=2, lab=xname)
text(1,T2[1,],lname_p,pos=2,cex=0.6)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/lryCYGkREIN6dWx.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, compare with absolute error, if you want to calculate relative error, the only thing you need to do is use the absolute error divide by p, the value you have. This process keep the influence of the value size, and focus on the error itself.
When we change the y scale into log_10, the xy relationship is also linear. But the bigger p we have, the smaller relative error we get.&lt;/p&gt;
</description>
        <pubDate>Fri, 06 Sep 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/09/06/absolute&relative-error/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/09/06/absolute&relative-error/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        
      </item>
    
      <item>
        <title>Roulette Simulation</title>
        <description>&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Roulette is a casino game named after the French word meaning little wheel. In the game, players may choose to place bets on either a single number, various groupings of numbers, the colors red or black, whether the number is odd or even, or if the numbers are high (19–36) or low (1–18).
&lt;em&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Roulette&quot;&gt;Wikipedia-Roulette&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;This time we will talk about a kind of simplified Roulette, which won’t be influenced by the number on roulette but only be divided into two parts, win or lose.&lt;/p&gt;

&lt;p&gt;To play the game successfully and avoid owing unrealistic debt, we need to set some parameter at first. These parameter will be save in a state list.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameter&lt;/th&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Explaination&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the budget&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the budget threshold for successfully stoping&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the maximum number of plays&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;M&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the casino wager limit&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;plays&lt;/td&gt;
      &lt;td&gt;integer&lt;/td&gt;
      &lt;td&gt;the number of plays executed&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;previous_wager&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the wager in the previous play (0 at first play)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;previous_win&lt;/td&gt;
      &lt;td&gt;TRUE/FALSE&lt;/td&gt;
      &lt;td&gt;indicator if the previous play was a win (TRUE at first play)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;function-setup&quot;&gt;Function Setup&lt;/h1&gt;

&lt;h2 id=&quot;one-play&quot;&gt;One Play&lt;/h2&gt;

&lt;p&gt;In order to use pipes “%&amp;gt;%” in code, we need to import the package first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r,results='hide'}&quot;&gt;library(dplyr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, let’s define the process of one play.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt; one_play &amp;lt;- function(state){

    # Wager
    proposed_wager &amp;lt;- ifelse(state$previous_win, 1, 2*state$previous_wager)
    wager &amp;lt;- min(proposed_wager, state$M, state$B)

    # Spin of the wheel
    red &amp;lt;- rbinom(1,1,18/38)

    # Update state
    state$plays &amp;lt;- state$plays + 1
    state$previous_wager &amp;lt;- wager
    if(red){
      # WIN
      state$B &amp;lt;- state$B + wager
      state$previous_win &amp;lt;- TRUE
    }else{
      # LOSE
      state$B &amp;lt;- state$B - wager
      state$previous_win &amp;lt;- FALSE
    }
  state
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the player run out of the money or win enough money or play enough times, we need to stop the game by set up an stop function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;stop_play &amp;lt;- function(state){
  if(state$B &amp;lt;= 0) return(TRUE)
  if(state$plays &amp;gt;= state$L) return(TRUE)
  if(state$B &amp;gt;= state$W) return(TRUE)
  FALSE
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;multiple-plays&quot;&gt;Multiple Plays&lt;/h2&gt;

&lt;p&gt;Next, we need to play the game under our rules as a series. The function will output a budget list to record the money value after every play.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;one_series &amp;lt;- function(
    B = 200
  , W = 300
  , L = 1000
  , M = 100
){

  # initial state
  state &amp;lt;- list(
    B = B
  , W = W
  , L = L
  , M = M
  , plays = 0
  , previous_wager = 0
  , previous_win = TRUE
  )

  # vector to store budget over series of plays
  budget &amp;lt;- rep(NA, L)

  # For loop of plays
  for(i in 1:L){
    new_state &amp;lt;- state %&amp;gt;% one_play
    budget[i] &amp;lt;- new_state$B
    if(new_state %&amp;gt;% stop_play){
      return(budget[1:i])
    }
    state &amp;lt;- new_state
  }
  budget
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can get the final result of this series of play.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# helper function
get_last &amp;lt;- function(x) x[length(x)]
get_series &amp;lt;- function(x) x
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;simulation&quot;&gt;Simulation&lt;/h1&gt;

&lt;p&gt;In order to figure out the generalized result, we need to repeat the process for a huge number of times then try to find out the distribution and other characteristics of results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Simulation
walk_out_money &amp;lt;- rep(NA, 1000)
for(j in seq_along(walk_out_money)){
  walk_out_money[j] &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_last
}

# Walk out money distribution
hist(walk_out_money, breaks = 100)

# Estimated probability of walking out with extra cash
mean(walk_out_money &amp;gt; 200)

# Estimated earnings
mean(walk_out_money - 200)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;compare&quot;&gt;Compare&lt;/h1&gt;

&lt;p&gt;In this graph, we can see how the budget changes during in one series.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
plot(budget_list, type=&quot;l&quot;, xlim=c(0,500), ylim=c(0,300), xlab=&quot;play number&quot;, ylab=&quot;earning money&quot;, main=&quot;budget series&quot;,col=&quot;red&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;orange&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;yellow&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;green&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;gray&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;blue&quot;)
budget_list &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_series
lines(budget_list, col=&quot;purple&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/4kcjYL5KFvSeA9Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameter&lt;/th&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Explaination&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the budget&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the budget threshold for successfully stoping&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the maximum number of plays&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;M&lt;/td&gt;
      &lt;td&gt;number&lt;/td&gt;
      &lt;td&gt;the casino wager limit&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;change-the-budget&quot;&gt;Change the budget&lt;/h2&gt;

&lt;p&gt;When B changes, what is the mean earning.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;earning_series &amp;lt;- rep(NA,20)
for(B in seq(100,1000,by=50)){
walk_out_money &amp;lt;- rep(NA, 1000)
for(j in seq_along(walk_out_money)){
  walk_out_money[j] &amp;lt;- one_series(B, W=B+100, L = 1000, M = 100) %&amp;gt;% get_last
}
earning_series[B] &amp;lt;- mean(walk_out_money - B)/B
}
plot(earning_series,xlab=&quot;Budget&quot;,ylab=&quot;mean earning rate&quot;, main=&quot;How Budget influence earning?&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/YBcaSpgdhf7nmwi.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;change-the-budget-threshold-for-successfully-stoping&quot;&gt;Change the budget threshold for successfully stoping&lt;/h2&gt;

&lt;p&gt;When W changes, what is the mean earning.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;earning_series &amp;lt;- rep(NA,20)
for(W in seq(100,1000,by=50)){
walk_out_money &amp;lt;- rep(NA, 10000)
for(j in seq_along(walk_out_money)){
  walk_out_money[j] &amp;lt;- one_series(B=200, W, L = 1000, M = 100) %&amp;gt;% get_last
}
earning_series[W] &amp;lt;- mean(walk_out_money - 200)
}
plot(earning_series,xlab=&quot;successfully stoping number&quot;,ylab=&quot;mean earning&quot;, main=&quot;How successfully stoping influence earning?&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/wn2qDzCjXlxSGIY.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;change-the-maximum-number-of-plays&quot;&gt;Change the maximum number of plays&lt;/h2&gt;

&lt;p&gt;When L changes, what is the mean earning.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;earning_series &amp;lt;- rep(NA,100)
for(L in seq(10,1000,by=10)){
walk_out_money &amp;lt;- rep(NA, 1000)
for(j in seq_along(walk_out_money)){
  walk_out_money[j] &amp;lt;- one_series(B=200, W=300, L, M = 100) %&amp;gt;% get_last
}
earning_series[L] &amp;lt;- mean(walk_out_money - 200)
}
plot(earning_series,xlab=&quot;maximum number of plays&quot;,ylab=&quot;mean earning&quot;, main=&quot;How maximum number of plays influence earning?&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/qeWF58lmvsfiLdE.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;change-the-casino-wager-limit&quot;&gt;Change the casino wager limit&lt;/h2&gt;

&lt;p&gt;When M changes, what is the mean earning.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;earning_series &amp;lt;- rep(NA,100)
for(M in seq(10,1000,by=10)){
walk_out_money &amp;lt;- rep(NA, 1000)
for(j in seq_along(walk_out_money)){
  walk_out_money[j] &amp;lt;- one_series(B=200, W=300, L=500, M) %&amp;gt;% get_last
}
earning_series[M] &amp;lt;- mean(walk_out_money - 200)
}
plot(earning_series,xlab=&quot;casino wager limit&quot;,ylab=&quot;mean earning&quot;, main=&quot;How casino wager limit influence earning?&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/1rZdWUg4KsAntqj.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;play-times&quot;&gt;Play times&lt;/h2&gt;

&lt;p&gt;Next, we can save the times that the game played before walk out then find out the characteristics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;get_times &amp;lt;- function(x) length(x)
walk_out_times &amp;lt;- rep(NA, 10000)
for(j in seq_along(walk_out_times)){
  walk_out_times[j] &amp;lt;- one_series(B = 200, W = 300, L = 1000, M = 100) %&amp;gt;% get_times
}

hist(walk_out_times, breaks = 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/22/YqzUKQRmIahSVoE.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;mean(walk_out_times)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of walk out time is 203.0846.&lt;/p&gt;

&lt;p&gt;The limitation of simulation is obvious. It is a black box actually. We can not use it as we proof it by mathematical method. We don’t know why it happened and how it happened, so we can only change the parameter to try to understand the process. By the way, it is not a precise result. Everytime we get a answer, it will change a little bit next time.&lt;/p&gt;
</description>
        <pubDate>Mon, 26 Aug 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/08/26/roulette-simulation/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/26/roulette-simulation/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        
      </item>
    
  </channel>
</rss>
