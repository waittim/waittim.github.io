<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zekun Blog</title>
    <description>Across the Great Wall, we can reach every corner in the world</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 27 Nov 2020 16:54:46 -0500</pubDate>
    <lastBuildDate>Fri, 27 Nov 2020 16:54:46 -0500</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Tutorial for compiling NCNN library</title>
        <description>&lt;p&gt;This is a tutorial to helping compile NCNN library. The content comes from my attempts to complete the Mask-Detection project, so the operation is based on the Yolo-Fastest model. I hope it will help readers deploy the model, and also for the convenience of myself to quickly rebuild the environment in the future.&lt;/p&gt;

&lt;p&gt;According to the author nihui, the name of NCNN comes from (New/Next or Naive or Neon or Nihui) + CNN. This is the introduction on &lt;a href=&quot;https://github.com/Tencent/ncnn&quot;&gt;its Github repository&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;ncnn is a high-performance neural network inference computing framework optimized for mobile platforms. ncnn is deeply considerate about deployment and uses on mobile phones from the beginning of design. ncnn does not have third party dependencies. it is cross-platform, and runs faster than all known open source frameworks on mobile phone cpu.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In addition to the excellent optimization of the mobile devices, its no third library dependency also means that there are many fewer restrictions on the deployment of models. However, the process of compiling NCNN still requires several third-party libraries.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;g++ &amp;amp; gcc&lt;/li&gt;
  &lt;li&gt;camke&lt;/li&gt;
  &lt;li&gt;protobuf&lt;/li&gt;
  &lt;li&gt;opencv&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The following content is based on Ubuntu 18.04.5. In fact, I have tried MacOS, but after encountering various problems caused by clang instead of g++, I chose to change it. . .&lt;/p&gt;

&lt;h2 id=&quot;g--gcc&quot;&gt;g++ &amp;amp; gcc&lt;/h2&gt;

&lt;p&gt;Usually, Ubuntu has built-in this library, you can use the following code in the Terminal to check the installed version:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;g++ -v
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or you can install it via &lt;code class=&quot;highlighter-rouge&quot;&gt;apt&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt install gcc
sudo apt install g++
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cmake&quot;&gt;cmake&lt;/h2&gt;

&lt;p&gt;Similar to the previous one, we can still use &lt;code class=&quot;highlighter-rouge&quot;&gt;apt&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt install cmake
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;protobuf&quot;&gt;Protobuf&lt;/h2&gt;

&lt;p&gt;The installation of protobuf can be implemented in accordance with the instructions of its &lt;a href=&quot;&quot;&gt;github repository&lt;/a&gt;. The key is the following steps.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install autoconf automake libtool curl make g++ unzip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then download the package from its release page:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/protocolbuffers/protobuf/releases/latest&quot;&gt;https://github.com/protocolbuffers/protobuf/releases/latest&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The one I downloaded is &lt;a href=&quot;https://github.com/protocolbuffers/protobuf/releases/download/v3.14.0/protobuf-all-3.14.0.tar.gz&quot;&gt;protobuf-all-3.14.0.tar.gz&lt;/a&gt;. Then unzip the package and enter the folder in Terminal. Then run the following code.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure
 make
 make check
 sudo make install
 sudo ldconfig # refresh shared library cache.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Hint:&lt;/strong&gt; If protobuf is installed successfully, and some errors are reported when compiling NCNN, you can try to restart Ubuntu. If you still can’t compile NCNN normally, you can recompile and install protobuf before restarting.&lt;/p&gt;

&lt;h2 id=&quot;opencv&quot;&gt;OpenCV&lt;/h2&gt;

&lt;p&gt;Open &lt;a href=&quot;https://opencv.org/releases/&quot;&gt;https://opencv.org/releases/&lt;/a&gt; and select an appropriate version. What I installed is OpenCV-3.4.12. Click &lt;strong&gt;Sources&lt;/strong&gt; to download the package.&lt;/p&gt;

&lt;p&gt;Then unzip the package and enter the folder in Terminal. Then run the following code to install the dependencies.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Create a compilation folder under this folder, then cmake.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir build
cd build
cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local ..
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then start to compile it.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo make
sudo make install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add the OpenCV library to the path so that the system can find it.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo gedit /etc/ld.so.conf.d/opencv.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command opened a file. Add &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/lib&lt;/code&gt; at the end of the file and save it. Executing this command makes the configuration path effective.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ldconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following command will open a file.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo gedit /etc/bash.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add these at the end of the file and save it:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig  
export PKG_CONFIG_PATH
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Executing this command makes the configuration path effective.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source /etc/bash.bashrc
sudo updatedb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The configuration process of OpenCV has been completed, let’s test it with its own tools.&lt;/p&gt;

&lt;p&gt;Enter &lt;code class=&quot;highlighter-rouge&quot;&gt;opencv-3.4.12/samples/cpp/example_cmake&lt;/code&gt; in Terminal.
Execute the following code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cmake .
make
./opencv_example
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If the camera is automatically turned on, and a &lt;code class=&quot;highlighter-rouge&quot;&gt;hello opencv&lt;/code&gt; appears in the upper left corner of the image captured by the camera, it means that the configuration is successful.&lt;/p&gt;

&lt;h2 id=&quot;ncnn&quot;&gt;NCNN&lt;/h2&gt;

&lt;p&gt;Now that the dependent libraries have been installed, we will start to compile NCNN. You can find the official tutorial &lt;a href=&quot;https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-linux&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want to use GPU, please install Vulkan as a dependency with the following code.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://sdk.lunarg.com/sdk/download/1.2.154.0/linux/vulkansdk-linux-x86_64-1.2.154.0.tar.gz?Human=true -O vulkansdk-linux-x86_64-1.2.154.0.tar.gz
tar -xf vulkansdk-linux-x86_64-1.2.154.0.tar.gz
export VULKAN_SDK=$(pwd)/1.2.154.0/x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, cause I’m using a virtual machine running on Macbook pro, I choose to forget Vulkan. If you choose to do so, please remember to set the Vulkan option to be OFF.&lt;/p&gt;

&lt;p&gt;Then we need to clone the repository of NCNN and start to compile it.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/Tencent/ncnn.git
cd ncnn
mkdir -p build
cd build
cmake -DCMAKE_BUILD_TYPE=Release -DNCNN_VULKAN=ON -DNCNN_SYSTEM_GLSLANG=ON -DNCNN_BUILD_EXAMPLES=ON ..
make
make install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So far, the compilation process of NCNN has been completed. It is recommended to carefully read the official documents and issues in Github when compiling, you can find many solutions to errors.&lt;/p&gt;

&lt;p&gt;After this, we can start to use various tools in the &lt;code class=&quot;highlighter-rouge&quot;&gt;ncnn/build/tools&lt;/code&gt; folder to help us convert the model.&lt;/p&gt;

&lt;p&gt;For example, you can copy the &lt;strong&gt;.cfg&lt;/strong&gt; and .&lt;strong&gt;weights&lt;/strong&gt; files of your darknet model to the &lt;strong&gt;darknet&lt;/strong&gt; folder, and use the code to convert to the NCNN model. The details could be found &lt;a href=&quot;https://github.com/Tencent/ncnn/tree/master/tools/darknet&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./darknet2ncnn yolo-fastest.cfg best.weights yolo-fastest.param yolo-fastest.bin 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After getting &lt;strong&gt;.param&lt;/strong&gt; and &lt;strong&gt;.bin&lt;/strong&gt;, we can deploy the model to the mobile devices or even the browser. And the files in &lt;code class=&quot;highlighter-rouge&quot;&gt;ncnn/build/install&lt;/code&gt; would be used when you need to run your own model.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Note: Thanks to the articles “&lt;a href=&quot;https://zhuanlan.zhihu.com/p/137458205&quot;&gt;pytorch模型的部署（系列一）–ncnn的编译和使用&lt;/a&gt;” and “&lt;a href=&quot;https://blog.csdn.net/cocoaqin/article/details/78163171&quot;&gt;ubuntu16.04安装opencv3.4.1教程&lt;/a&gt;” for their help in the compilation process.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 10 Nov 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/11/10/build-ncnn/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/11/10/build-ncnn/</guid>
        
        <category>NCNN</category>
        
        <category>Deep Learning</category>
        
        <category>C++</category>
        
        
      </item>
    
      <item>
        <title>Note - How to get information from IP?</title>
        <description>&lt;p&gt;The process of getting IP information is done based on &lt;a href=&quot;https://www.maxmind.com/en/geoip2-databases&quot;&gt;GeoIP2 Databases&lt;/a&gt;. I Used the &lt;a href=&quot;https://geoip2.readthedocs.io/en/latest/&quot;&gt;MaxMind GeoIP2 Python API&lt;/a&gt; for IP information queries. The Github page for the API is &lt;a href=&quot;https://github.com/maxmind/GeoIP2-python&quot;&gt;GeoIP2-python&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You need to download &lt;a href=&quot;https://github.com/waittim/waittim.github.io/raw/master/gallery/GeoLite2-City.mmdb&quot;&gt;GeoLite2-City.mmdb&lt;/a&gt; as data source and install &lt;strong&gt;geoip2&lt;/strong&gt; module before you can use it.&lt;/p&gt;

&lt;p&gt;For complete information, see &lt;a href=&quot;https://dev.maxmind.com/geoip/geoip2/downloadable/&quot;&gt;MaxMind - GeoIP2 Downloadable Databases&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;installation&quot;&gt;Installation&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;geoip2&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For Chinese user, you can choose tsinghua mirrors
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple geoip2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;usage&quot;&gt;Usage&lt;/h3&gt;

&lt;p&gt;First, we should import the geoip2 module.
We are using the free downloaded data, therefore, please import &lt;em&gt;geoip2.database&lt;/em&gt; .&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;geoip2.database&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'129.59.93.0'&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# The IP of Vanderbilt Univeristy
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;geoip2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'GeoLite2-City.mmdb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;city&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;all of the results were reserved in the &lt;em&gt;response&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;geoip2.models.City({'city': {'geoname_id': 4644585, 'names': {'de': 'Nashville', 'en': 'Nashville', 'es': 'Nashville', 'fr': 'Nashville', 'ja': 'ナッシュビル', 'pt-BR': 'Nashville', 'ru': 'Нашвилл', 'zh-CN': '纳什维尔'}}, 'continent': {'code': 'NA', 'geoname_id': 6255149, 'names': {'de': 'Nordamerika', 'en': 'North America', 'es': 'Norteamérica', 'fr': 'Amérique du Nord', 'ja': '北アメリカ', 'pt-BR': 'América do Norte', 'ru': 'Северная Америка', 'zh-CN': '北美洲'}}, 'country': {'geoname_id': 6252001, 'iso_code': 'US', 'names': {'de': 'USA', 'en': 'United States', 'es': 'Estados Unidos', 'fr': 'États-Unis', 'ja': 'アメリカ合衆国', 'pt-BR': 'Estados Unidos', 'ru': 'США', 'zh-CN': '美国'}}, 'location': {'accuracy_radius': 20, 'latitude': 36.066, 'longitude': -86.9659, 'metro_code': 659, 'time_zone': 'America/Chicago'}, 'postal': {'code': '37221'}, 'registered_country': {'geoname_id': 6252001, 'iso_code': 'US', 'names': {'de': 'USA', 'en': 'United States', 'es': 'Estados Unidos', 'fr': 'États-Unis', 'ja': 'アメリカ合衆国', 'pt-BR': 'Estados Unidos', 'ru': 'США', 'zh-CN': '美国'}}, 'subdivisions': [{'geoname_id': 4662168, 'iso_code': 'TN', 'names': {'en': 'Tennessee', 'es': 'Tennessee', 'fr': 'Tennessee', 'ja': 'テネシー州', 'pt-BR': 'Tenessi', 'ru': 'Теннесси', 'zh-CN': '田纳西州'}}], 'traits': {'ip_address': '129.59.93.0', 'prefix_len': 20}}, ['en'])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let me re-format it to make it understandable.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'city'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'geoname_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4644585&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'names'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'de'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Nashville'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Nashville'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'es'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Nashville'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Nashville'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ja'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ナッシュビル'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pt-BR'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Nashville'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ru'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Нашвилл'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'zh-CN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'纳什维尔'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'continent'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'code'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'NA'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'geoname_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6255149&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'names'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'de'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Nordamerika'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'North America'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'es'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Norteamérica'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Amérique du Nord'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ja'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'北アメリカ'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pt-BR'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'América do Norte'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ru'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Северная Америка'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'zh-CN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'北美洲'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'country'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'geoname_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6252001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'iso_code'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'US'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'names'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'de'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'USA'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'United States'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'es'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Estados Unidos'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'États-Unis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ja'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'アメリカ合衆国'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pt-BR'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Estados Unidos'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ru'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'США'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'zh-CN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'美国'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'location'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy_radius'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'latitude'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;36.066&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'longitude'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;86.9659&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'metro_code'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;659&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'time_zone'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'America/Chicago'&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'postal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'code'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'37221'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'registered_country'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'geoname_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6252001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'iso_code'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'US'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'names'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'de'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'USA'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'United States'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'es'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Estados Unidos'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'États-Unis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ja'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'アメリカ合衆国'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pt-BR'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Estados Unidos'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ru'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'США'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'zh-CN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'美国'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'subdivisions'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'geoname_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4662168&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'iso_code'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'TN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'names'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Tennessee'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'es'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Tennessee'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Tennessee'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ja'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'テネシー州'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pt-BR'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Tenessi'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ru'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Теннесси'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'zh-CN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'田纳西州'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}],&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'traits'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ip_address'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'129.59.93.0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'prefix_len'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are some example to use the &lt;em&gt;response&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iso_code&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#'US'
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#'United States'
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'zh-CN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#'美国'
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subdivisions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_specific&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#'Tennessee'
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subdivisions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_specific&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iso_code&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#'TN'
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;city&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#'Nashville'
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;postal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#'37221'
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latitude&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#36.066
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;longitude&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#-86.9659
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;traits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#IPv4Network('129.59.80.0/20')
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Mon, 20 Jul 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/07/20/get-ip-info/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/07/20/get-ip-info/</guid>
        
        <category>Feature Engineering</category>
        
        <category>Python</category>
        
        <category>IP</category>
        
        
      </item>
    
      <item>
        <title>COVID-19 Data Exploration (Unfinished)</title>
        <description>&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The content of this article is based on the data what was collected by the project team of Professor &lt;a href=&quot;http://math.csu.edu.cn/info/1316/1554.htm&quot;&gt;Ziqi Qi&lt;/a&gt;, School of Mathematics and Statistics, Central South University (China). The data used for this analysis are not complete.&lt;/p&gt;

&lt;h1 id=&quot;10-import&quot;&gt;10-import&lt;/h1&gt;

&lt;p&gt;Load the necessary packages.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(tidyverse)
library(ggplot2)
library(gganimate)
library(plotly)
library(readxl)
library(lubridate)
library(gifski)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Read data from Excel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;abroad_raw &amp;lt;- read_excel(&quot;data/【海外】疫情数据.xlsx&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Convert wide tables to long tables and process date data into a usable form.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;data &amp;lt;- abroad_raw %&amp;gt;%
  select(-name) %&amp;gt;%
  pivot_longer(
    -country,
    names_to = &quot;date&quot;,
    values_to = &quot;number&quot;,
    values_drop_na = TRUE
  ) %&amp;gt;%
  mutate(date = as.integer(date)) %&amp;gt;%
  mutate(date = as.Date(date, origin = &quot;1899-12-30&quot;))

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;20-exploratory-data-analysis&quot;&gt;20-exploratory-data-analysis&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;line &amp;lt;- data %&amp;gt;%
  ggplot(aes(x = date, y = number, group = country)) +
  geom_line(aes(color = country), size = 2, alpha = 0.5) +
  geom_point(aes(color = country), size = 3) +
  geom_text(
    aes(label = country),
    #color = &quot;black&quot;,
    check_overlap = TRUE,
    hjust = 0,
    nudge_x = 0.3
  ) +
  ggdark::dark_theme_bw() +
  theme(plot.title = element_text(face = &quot;bold&quot;, size = 20)) +
  guides(color = FALSE) +
  labs(
    title = paste0('COVID-19 non-China cases at ', '{frame_along}'),
    y = &quot;Case number&quot;,
    x = &quot;Date&quot;
  ) +
  transition_reveal(date) +
  view_follow() +
  ease_aes(default = 'linear')

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;animate(
  line,
  width = 1000,
  height = 500,
  renderer = gifski_renderer(),
  duration = 20,
  rewind = FALSE,
  end_pause = 1
)
anim_save(&quot;output.gif&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/ZYPryMhf/output.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Project GitHub page: &lt;a href=&quot;https://github.com/waittim/COVID-19-EDA&quot;&gt;COVID-19-EDA&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 19 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/19/COVID-19-EDA/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/19/COVID-19-EDA/</guid>
        
        <category>Visualization</category>
        
        <category>R</category>
        
        <category>EDA</category>
        
        
      </item>
    
      <item>
        <title>LAPOP Data Dashboard</title>
        <description>&lt;p&gt;This project is designed a data visualization dashboard to help non-professional users for Latin American Public Opinion Project (LAPOP).&lt;/p&gt;

&lt;p&gt;Due to copyright issues and confidentiality agreements, the dashboard shown with randomly generated data based on real data formats.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to use it?&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Select the country and year you want to analyze (multiple choices) to obtain key figures and visualizations of the overall situation of the interviewee.&lt;/li&gt;
  &lt;li&gt;Select the topics and questions you want to analyze in detail, and obtain key figures and visualization results and time series visualization images for a single problem.&lt;/li&gt;
  &lt;li&gt;Select topics and questions for cross-analysis to get visual results of cross-analysis.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/waittim/waittim.github.io/raw/master/img/lapop-dashboard.png&quot; alt=&quot;demo-home.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://zekun.shinyapps.io/LAPOP-shiny-dashboard/&quot;&gt;Click here to open the dashboard&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Project GitHub page: &lt;a href=&quot;https://github.com/waittim/LAPOP-shiny-dashboard&quot;&gt;LAPOP-shiny-dashboard&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 19 Feb 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/02/19/LAPOP-dashboard/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/19/LAPOP-dashboard/</guid>
        
        <category>Dashboard</category>
        
        <category>R</category>
        
        <category>Shiny</category>
        
        
      </item>
    
      <item>
        <title>Cashbox Magazine Song Chart Analysis</title>
        <description>&lt;iframe src=&quot;https://docs.google.com/presentation/d/e/2PACX-1vT8PsgoYD97P7ZvBxpVM3aCGnxX7A5_3o_ep7CqbsbGbuq7-MWdeCIRbNUvqfyJYuRliseB9j0DuqeE/embed?start=false&amp;amp;loop=true&amp;amp;delayms=3000&quot; frameborder=&quot;0&quot; width=&quot;700&quot; height=&quot;423&quot; allowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This article mainly introduces the full picture of a data analysis pipeline by showing the simple data import, feature engineering, and exploratory data analysis processes. The data used this time is relatively clean, so basically no data cleaning has been performed except for the analysis part of this article.&lt;/p&gt;

&lt;p&gt;The entire working process is divided into three parts: 10-read-in, 20-feature-engineering, 30-exploratory-data-analysis.&lt;/p&gt;

&lt;h1 id=&quot;10-read-in&quot;&gt;10-read-in&lt;/h1&gt;

&lt;h2 id=&quot;import-libraries&quot;&gt;Import libraries.&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(readxl)
library(tidyverse)
library(janitor)
library(ggplot2)
library(tidytext)
library(wordcloud2)
library(RColorBrewer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Library introduction (The parts used in this pipeline):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;readxl&lt;/strong&gt;: read in data in Excel file.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;tidyverse&lt;/strong&gt;: help manage the data frame in tidy sentences.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;janitor&lt;/strong&gt;: clean the column names.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ggplot2&lt;/strong&gt;: create plots.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;tidytext&lt;/strong&gt;: import stop words data for text analysis and get word
sentiment dictionary.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;wordcloud2&lt;/strong&gt;: create word cloud graph.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RColorBrewer&lt;/strong&gt;: set color palette for graphes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;read-in-raw-data-and-make-the-column-names&quot;&gt;Read in raw data and make the column names.&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for(i in seq(2000,2009,1)){
  df &amp;lt;- read_excel(&quot;Analysis Exercise.xls&quot;, sheet = as.character(i))
  names(df) &amp;lt;- df[1,]
  df &amp;lt;- df[-1,] %&amp;gt;%
    clean_names() %&amp;gt;%
    mutate(year = i)
  assign(paste0(&quot;df_&quot;, i), df)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this Excel file, the data of different years was divided into sub-tables, therefore, I need to read in them one by one.&lt;/p&gt;

&lt;p&gt;And the column names in each table were not in the first row but in the second row. So I removed the first row and transferred the second row to be the column names. Then, I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;clean_names()&lt;/code&gt; function to make the names lowercase without blank space.&lt;/p&gt;

&lt;p&gt;The function &lt;code class=&quot;highlighter-rouge&quot;&gt;assign&lt;/code&gt; can help me use different names to reserve the data.&lt;/p&gt;

&lt;p&gt;After that, I merged all of the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df &amp;lt;-
  bind_rows(df_2000, df_2001, df_2002, df_2003, df_2004, df_2005, df_2006, df_2007, df_2008, df_2009)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Import the stop words for text analysis in the EDA part.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;data(&quot;stop_words&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;20-feature-engineering&quot;&gt;20-feature-engineering&lt;/h1&gt;

&lt;p&gt;This data set is clean enough and I don’t need to clean the data as normal.&lt;/p&gt;

&lt;p&gt;Basic feature engineering on raw merged data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df &amp;lt;- df %&amp;gt;%
  mutate(weeks_at_number_1 = as.integer(weeks_at_number_1),
         weeks_top_10 = as.integer(weeks_top_10),
         weeks_top_25 = as.integer(weeks_top_25),
         weeks_top_50 = as.integer(weeks_top_50),
         peak_position = as.integer(peak_position),
         bonus_weeks = as.integer(bonus_weeks),
         sub_points = as.integer(sub_points),
         peak_points = as.integer(peak_points),
         total_points = as.integer(total_points),
         peak_year = as.integer(peak_year),
         debut_date = as.integer(debut_date),
         debut_date = as.Date(debut_date, origin=&quot;1899-12-30&quot;),
         peak_date = as.integer(peak_date),
         peak_date = as.Date(peak_date, origin=&quot;1899-12-30&quot;),
         weeks_to_reach_peak = as.integer(weeks_to_reach_peak),
         year=as.factor(year)
         )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this chunk, I only need to make all of the columns have the right data
type.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hint:&lt;/strong&gt; In the date part, because Excel save the date by count the
distance between the date and 1900-01-01, the date columns in the raw
data are the days. I need to transfer them to be the real date. The
&lt;code class=&quot;highlighter-rouge&quot;&gt;origin&lt;/code&gt; attribute should be 1900-01-01, but due to 1900-leap-year-bug
of Excel, use 1899-12-30 can get the right answer.&lt;/p&gt;

&lt;h1 id=&quot;30-exploratory-data-analysis&quot;&gt;30-exploratory-data-analysis&lt;/h1&gt;

&lt;p&gt;In this part, I will answer the following questions to finish the
exploratory data analysis(EDA) part.&lt;/p&gt;

&lt;h2 id=&quot;what-percent-of-songs-that-chart-never-make-the-top-10&quot;&gt;What percent of songs that chart never make the Top 10?&lt;/h2&gt;

&lt;p&gt;For all years:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;never_top10 &amp;lt;- df %&amp;gt;%
  filter(weeks_top_10 == 0)
nrow(never_top10)/nrow(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## [1] 0.6802455
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Time series:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;never_top10_year &amp;lt;- df %&amp;gt;%
  filter(weeks_top_10 == 0) %&amp;gt;%
  group_by(year) %&amp;gt;%
  summarise(never_top10_number = n())

all_year &amp;lt;- df %&amp;gt;%
  group_by(year) %&amp;gt;%
  summarise(whole_number = n())

never_top10_year[&quot;whole_number&quot;] &amp;lt;- all_year[&quot;whole_number&quot;]
never_top10_year %&amp;gt;%
  mutate(never_top10_ratio = never_top10_number/whole_number) %&amp;gt;%
  ggplot() +
  geom_col(aes(x=year, y=never_top10_ratio),fill=&quot;#125FA0&quot;, width = 0.7)+
  theme_classic()+
  labs(x=&quot;\nYear&quot;, y=&quot;Ratio of Never-Top10\n&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/0j7gsVWT/unnamed-chunk-7-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-were-the-top-10-songs-of-the-decade-2000-2009&quot;&gt;What were the top 10 songs of the decade 2000-2009?&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  arrange(desc(total_points)) %&amp;gt;%
  select(song_title, artist, year, total_points) %&amp;gt;%
  head(10)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 10 x 4
##    song_title               artist                       year  total_points
##    &amp;lt;chr&amp;gt;                    &amp;lt;chr&amp;gt;                        &amp;lt;fct&amp;gt;        &amp;lt;int&amp;gt;
##  1 GLAMOROUS                FERGIE featuring LUDACRIS    2007          1335
##  2 BEFORE HE CHEATS         UNDERWOOD, CARRIE            2007          1192
##  3 NO SUCH THING            MAYER, JOHN                  2002          1167
##  4 CITY LOVE                MAYER, JOHN                  2003          1167
##  5 VIDEO                    INDIA.ARIE                   2002          1129
##  6 WHERE IS THE LOVE?       BLACK EYED PEAS featuring J… 2003          1111
##  7 TWO WEEKS FROM TWENTY    YELLOWCARD                   2005          1111
##  8 AMERICAN BOY             ESTELLE featuring KANYE WEST 2008          1072
##  9 TIME OF YOUR LIFE (GOOD… GREEN DAY                    2006          1040
## 10 SOMEONE TO CALL MY LOVER JACKSON, JANET               2001          1012
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;who-was-the-top-artist-of-the-decade-2000-2009&quot;&gt;Who was the top artist of the decade 2000-2009?&lt;/h2&gt;

&lt;p&gt;(Who were the top 10 artists of the decade 2000-2009?)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  group_by(artist) %&amp;gt;%
  summarise(sum_total_points = sum(total_points, na.rm = TRUE)) %&amp;gt;%
  arrange(desc(sum_total_points)) %&amp;gt;%
  head(10)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 10 x 2
##    artist             sum_total_points
##    &amp;lt;chr&amp;gt;                         &amp;lt;int&amp;gt;
##  1 MAYER, JOHN                    6861
##  2 JACKSON, JANET                 6049
##  3 PANIC AT THE DISCO             4572
##  4 BLACK EYED PEAS                4481
##  5 CAREY, MARIAH                  4470
##  6 GREEN DAY                      4378
##  7 CLARKSON, KELLY                4016
##  8 UNDERWOOD, CARRIE              3762
##  9 PINK                           3668
## 10 MAROON 5                       3604
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;based-on-the-chart-data-what-artists-would-you-call-the-beatles-of-the-decade-2000-2009&quot;&gt;Based on the chart data, what artist(s) would you call “The Beatles” of the decade 2000-2009?&lt;/h2&gt;

&lt;p&gt;Actually, this question is really difficult to answer, because “The
Beatles” has various meanings according to different interpretations.&lt;/p&gt;

&lt;p&gt;However, we can try to give some directions by analyzing the number of
songs on the chart and the mean point of the songs on the chart of each artist.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  group_by(artist) %&amp;gt;%
  summarise(song_number = n()) %&amp;gt;%
  arrange(desc(song_number)) %&amp;gt;%
  head(10)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 10 x 2
##    artist              song_number
##    &amp;lt;chr&amp;gt;                     &amp;lt;int&amp;gt;
##  1 SPEARS, BRITNEY              21
##  2 PINK                         18
##  3 CAREY, MARIAH                14
##  4 CLARKSON, KELLY              14
##  5 EMINEM                       14
##  6 BEYONCE                      13
##  7 LAVIGNE, AVRIL               13
##  8 AGUILERA, CHRISTINA          12
##  9 MAYER, JOHN                  12
## 10 NELLY                        12
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  group_by(artist) %&amp;gt;%
  summarise(mean_total_points = mean(total_points)) %&amp;gt;%
  arrange(desc(mean_total_points)) %&amp;gt;%
  head(10)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 10 x 2
##    artist                                      mean_total_points
##    &amp;lt;chr&amp;gt;                                                   &amp;lt;dbl&amp;gt;
##  1 FERGIE featuring LUDACRIS                               1335
##  2 BLACK EYED PEAS featuring JUSTIN TIMBERLAKE             1111
##  3 ESTELLE featuring KANYE WEST                            1072
##  4 LOS LONELY BOYS                                          928
##  5 LOPEZ, JENNIFER featuring JA RULE                        861
##  6 SHAGGY featuring RICARDO DUCENT                          829
##  7 FOXX, JAMIE featuring T-PAIN                             808
##  8 POWTER, DANIEL                                           779
##  9 INDIA.ARIE                                               764.
## 10 EVE featuring GWEN STEFANI                               757
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-was-the-most-commonly-used-word-in-a-song-title-for-the-decade-2000-2009&quot;&gt;What was the most commonly used word in a song title for the decade 2000-2009?&lt;/h2&gt;

&lt;p&gt;For this question, due to there are many special symbols in the song
title, I need to clean the special patterns in the title.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;title_df &amp;lt;- df[&quot;song_title&quot;]
title_df &amp;lt;- title_df %&amp;gt;% mutate(song_title = str_to_lower(song_title))
title_df$song_title &amp;lt;- gsub(pattern = &quot;\\(&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;\\)&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;,&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;n't&quot;, replacement =&quot; not &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;'s&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;'re&quot;, replacement =&quot; are &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;'m&quot;, replacement =&quot; am &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;\\\&quot;&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;\\.&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;-&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;_&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;!&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;\\?&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;…&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;[0-9]&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;/&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;&amp;amp;&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot;#&quot;, replacement =&quot; &quot;, title_df$song_title)
title_df$song_title &amp;lt;- gsub(pattern = &quot; +&quot;, replacement =&quot; &quot;, title_df$song_title)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then make the data frame become list, which can help the following
steps.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;title_list &amp;lt;- title_df[[&quot;song_title&quot;]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Split all of the title into scattered words.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;title_words &amp;lt;- data.frame()
count &amp;lt;- 1
for (i in title_list) {
  title_split &amp;lt;- str_split(string = i, pattern = &quot; &quot;)
  for(j in title_split[[1]]){
    title_words[count,1] &amp;lt;- j
    count &amp;lt;- count + 1
  }
}
names(title_words) &amp;lt;- &quot;word&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clean the different forms of the same word.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;title_words &amp;lt;- title_words %&amp;gt;%
  mutate(word = case_when(word == &quot;girls&quot; ~ &quot;girl&quot;,
                          word == &quot;angels&quot; ~ &quot;angel&quot;,
                          word == &quot;days&quot; ~ &quot;day&quot;,
                          word == &quot;lovin'&quot; ~ &quot;love&quot;,
                          word == &quot;loving&quot; ~ &quot;love&quot;,
                          word == &quot;loved'&quot; ~ &quot;love&quot;,
                          word == &quot;luv&quot; ~ &quot;love&quot;,
                          word == &quot;waiting&quot; ~ &quot;wait&quot;,
                          word == &quot;does&quot; ~ &quot;do&quot;,
                          word == &quot;lights&quot; ~ &quot;light&quot;,
                          word == &quot;things&quot; ~ &quot;thing&quot;,
                          word == &quot;comes&quot; ~ &quot;come&quot;,
                          word == &quot;dancing&quot; ~ &quot;dance&quot;,
                          word == &quot;dancin'&quot; ~ &quot;dance&quot;,
                          word == &quot;boys&quot; ~ &quot;boy&quot;,
                          word == &quot;burning&quot; ~ &quot;burn&quot;,
                          word == &quot;burnin'&quot; ~ &quot;burn&quot;,
                          word == &quot;calle&quot; ~ &quot;call&quot;,
                          word == &quot;called&quot; ~ &quot;call&quot;,
                          word == &quot;calling&quot; ~ &quot;call&quot;,
                          word == &quot;call&quot; ~ &quot;call&quot;,
                          TRUE ~ word))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Count the number of each word.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;word_count &amp;lt;- title_words %&amp;gt;%
  filter(word != &quot;&quot;) %&amp;gt;%
  anti_join(stop_words,by = &quot;word&quot;) %&amp;gt;%
  group_by(word) %&amp;gt;%
  summarise(number = n()) %&amp;gt;%
  arrange(desc(number)) %&amp;gt;%
  mutate(word = str_to_upper(word))
word_count
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 1,192 x 2
##    word  number
##    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;
##  1 LOVE      87
##  2 GIRL      43
##  3 TIME      29
##  4 LIFE      21
##  5 DAY       20
##  6 WORLD     15
##  7 BABY      14
##  8 DANCE     14
##  9 ROCK      14
## 10 SONG      14
## # … with 1,182 more rows
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Generate the word cloud by &lt;code class=&quot;highlighter-rouge&quot;&gt;wordcloud2&lt;/code&gt; function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;wordcloud2(head(word_count,100),
           size = 1,
           fontFamily = &quot;Montserrat&quot;,
           fontWeight = &quot;Bold&quot;,
           minRotation = -pi/6,
           maxRotation = -pi/6,
           rotateRatio = 1,
           color = &quot;#125FA0&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;iframe src=&quot;https://waittim.github.io/gallery/song-wordcloud.html&quot; frameborder=&quot;0&quot; width=&quot;700&quot; height=&quot;423&quot; allowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;sentiment-analysis-of-titles&quot;&gt;Sentiment analysis of titles&lt;/h2&gt;

&lt;p&gt;In order to analyze the sentiment of the words in the titles, I got the
stop words(meaningless words) dictionary and sentiment dictionary from
the &lt;code class=&quot;highlighter-rouge&quot;&gt;tidytext&lt;/code&gt; library.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;sentiments &amp;lt;- get_sentiments(&quot;nrc&quot;)

df_sentiments &amp;lt;- title_words %&amp;gt;%
  filter(word != &quot;&quot;) %&amp;gt;%
  anti_join(stop_words,by = &quot;word&quot;) %&amp;gt;%
  left_join(sentiments)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df_sentiments_filtered &amp;lt;- df_sentiments %&amp;gt;%
  filter(!is.na(sentiment)) %&amp;gt;%
  group_by(sentiment) %&amp;gt;%
  summarize(n = n()) %&amp;gt;%
  arrange(desc(n))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After removed the stop word, I counted the number of each word in the titles.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df_sentiments_filtered %&amp;gt;%
  ggplot(aes(x = reorder(sentiment, n, function(n) -n), y = n)) +
  geom_bar(stat = &quot;identity&quot;,fill=&quot;#125FA0&quot;, width = 0.7) +
  theme_classic() +
  labs(x = &quot;\nSentiments&quot;, y=&quot;number\n&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/hPqWrqhc/unnamed-chunk-19-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And we can also count what is the percentage of positive words in the
words that have sentiments.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;words_positive &amp;lt;- df_sentiments %&amp;gt;%
  filter(sentiment == &quot;positive&quot;) %&amp;gt;%
  group_by(word) %&amp;gt;%
  summarise(n=n())

words_all_sentiment &amp;lt;- df_sentiments %&amp;gt;%
  filter(!is.na(sentiment)) %&amp;gt;%
  group_by(word) %&amp;gt;%
  summarize(n = n()) %&amp;gt;%
  arrange(desc(n))

nrow(words_positive)/nrow(words_all_sentiment)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## [1] 0.4294671
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;who-spent-the-most-weeks-at-1-for-the-decade-2000-2009&quot;&gt;Who spent the most weeks at #1 for the decade 2000-2009?&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  group_by(artist) %&amp;gt;%
  summarise(top1_time_sum = sum(weeks_at_number_1)) %&amp;gt;%
  arrange(desc(top1_time_sum)) %&amp;gt;%
  head(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 1 x 2
##   artist         top1_time_sum
##   &amp;lt;chr&amp;gt;                  &amp;lt;int&amp;gt;
## 1 JACKSON, JANET            39
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And there are the songs belong to this artist.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  filter(artist == &quot;JACKSON, JANET&quot;,
         weeks_at_number_1 != 0) %&amp;gt;%
  arrange(desc(weeks_at_number_1)) %&amp;gt;%
  select(song_title, artist, year, weeks_at_number_1)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 6 x 4
##   song_title                            artist       year  weeks_at_number…
##   &amp;lt;chr&amp;gt;                                 &amp;lt;chr&amp;gt;        &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;
## 1 SOMEONE TO CALL MY LOVER              JACKSON, JA… 2001                10
## 2 ALL FOR YOU                           JACKSON, JA… 2001                10
## 3 &quot;DOESN'T REALLY MATTER (from \&quot;NUTTY… JACKSON, JA… 2000                 8
## 4 FEEDBACK                              JACKSON, JA… 2008                 6
## 5 SPECIAL                               JACKSON, JA… 2000                 3
## 6 LUV                                   JACKSON, JA… 2008                 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-song-spent-the-most-weeks-at-1-for-the-decade-2000-2009&quot;&gt;What song spent the most weeks at #1 for the decade 2000-2009?&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  arrange(desc(weeks_at_number_1)) %&amp;gt;%
  select(song_title, artist, year, weeks_at_number_1) %&amp;gt;%
  head(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 1 x 4
##   song_title    artist      year  weeks_at_number_1
##   &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;       &amp;lt;fct&amp;gt;             &amp;lt;int&amp;gt;
## 1 NO SUCH THING MAYER, JOHN 2002                 13
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-song-peaked-at-1-the-quickest-in-the-decade-2000-2009&quot;&gt;What song peaked at #1 the quickest in the decade 2000-2009?&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  filter(weeks_at_number_1 != 0) %&amp;gt;%
  arrange(weeks_to_reach_peak) %&amp;gt;%
  select(song_title, artist, year, weeks_at_number_1, weeks_to_reach_peak, debut_date, peak_date) %&amp;gt;%
  head(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 1 x 7
##   song_title artist year  weeks_at_number… weeks_to_reach_… debut_date
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;  &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;date&amp;gt;    
## 1 SOMEONE T… JACKS… 2001                10                2 2001-07-22
## # … with 1 more variable: peak_date &amp;lt;date&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-song-took-the-longest-to-reach-1-in-the-decade-2000-2009&quot;&gt;What song took the longest to reach #1 in the decade 2000-2009?&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  filter(weeks_at_number_1 != 0) %&amp;gt;%
  arrange(desc(weeks_to_reach_peak)) %&amp;gt;%
  select(song_title, artist, year, weeks_at_number_1, weeks_to_reach_peak, debut_date, peak_date) %&amp;gt;%
  head(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 1 x 7
##   song_title artist year  weeks_at_number… weeks_to_reach_… debut_date
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;  &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;date&amp;gt;    
## 1 BEFORE HE… UNDER… 2007                 3               31 2006-11-05
## # … with 1 more variable: peak_date &amp;lt;date&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;solo-men-solo-women-groups-for-the-decade-2000-2009---design-a-graphic-that-shows-the-distribution-of-songs-hitting-1&quot;&gt;Solo men, solo women, groups for the decade 2000-2009 - design a graphic that shows the distribution of songs hitting #1&lt;/h2&gt;

&lt;p&gt;The solo artists have their full name in the data by the format as
“LastName, FirstName”, as a result, we can tell whether the data is a
single person name by the presence of a comma.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  mutate(solo_group = case_when(str_detect(string = artist, pattern = &quot;,&quot;) ~ &quot;solo&quot;,
                                TRUE ~ &quot;group&quot;)) %&amp;gt;%
  mutate(solo_group = as.factor(solo_group)) %&amp;gt;%
  filter(weeks_at_number_1 != 0) %&amp;gt;%
  group_by(solo_group) %&amp;gt;%
  summarise(number = n())
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 2 x 2
##   solo_group number
##   &amp;lt;fct&amp;gt;       &amp;lt;int&amp;gt;
## 1 group          92
## 2 solo           84
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Actually, there is no built-in pie chart function in &lt;code class=&quot;highlighter-rouge&quot;&gt;ggplot2&lt;/code&gt;. But we
can use bar chart function then twisting the y-axis to the polar
coordinate system to create a pie chart.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  mutate(solo_group = case_when(str_detect(string = artist, pattern = &quot;,&quot;) ~ &quot;solo&quot;,
                                TRUE ~ &quot;group&quot;)) %&amp;gt;%
  mutate(solo_group = as.factor(solo_group)) %&amp;gt;%
  filter(weeks_at_number_1 != 0) %&amp;gt;%
  ggplot()+
  geom_bar(aes(x=1, fill = solo_group))+
  coord_polar(theta = &quot;y&quot;)+
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        legend.title = element_blank(),
        panel.background = element_blank()
        )+
  scale_fill_brewer(palette = &quot;Paired&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/fWdQmbz8/unnamed-chunk-27-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;solo-men-solo-women-groups-for-the-decade-2000-2009---design-a-graphic-that-shows-the-distribution-of-songs-hitting-the-top-10&quot;&gt;Solo men, solo women, groups for the decade 2000-2009 - design a graphic that shows the distribution of songs hitting the Top 10&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  mutate(solo_group = case_when(str_detect(string = artist, pattern = &quot;,&quot;) ~ &quot;solo&quot;,
                                TRUE ~ &quot;group&quot;)) %&amp;gt;%
  mutate(solo_group = as.factor(solo_group)) %&amp;gt;%
  filter(weeks_top_10 != 0) %&amp;gt;%
  group_by(solo_group) %&amp;gt;%
  summarise(number = n())
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 2 x 2
##   solo_group number
##   &amp;lt;fct&amp;gt;       &amp;lt;int&amp;gt;
## 1 group         318
## 2 solo          255
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  mutate(solo_group = case_when(str_detect(string = artist, pattern = &quot;,&quot;) ~ &quot;solo&quot;,
                                TRUE ~ &quot;group&quot;)) %&amp;gt;%
  mutate(solo_group = as.factor(solo_group)) %&amp;gt;%
  filter(weeks_top_10 != 0) %&amp;gt;%
  ggplot()+
  geom_bar(aes(x=1, fill = solo_group))+
  coord_polar(theta = &quot;y&quot;)+
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        legend.title = element_blank(),
        panel.background = element_blank()
        )+
  scale_fill_brewer(palette = &quot;Paired&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/ncY8SwQ7/unnamed-chunk-29-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-song-spent-the-most-time-on-the-charts-in-the-decade-2000-2009&quot;&gt;What song spent the most time on the charts in the decade 2000-2009)&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  arrange(desc(weeks_top_50)) %&amp;gt;%
  select(song_title, artist, year, weeks_top_50) %&amp;gt;%
  head(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## # A tibble: 3 x 4
##   song_title                      artist                 year  weeks_top_50
##   &amp;lt;chr&amp;gt;                           &amp;lt;chr&amp;gt;                  &amp;lt;fct&amp;gt;        &amp;lt;int&amp;gt;
## 1 TIME OF YOUR LIFE (GOOD RIDDAN… GREEN DAY              2006            65
## 2 BEFORE HE CHEATS                UNDERWOOD, CARRIE      2007            57
## 3 GLAMOROUS                       FERGIE featuring LUDA… 2007            50
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Actually, TIME OF YOUR LIFE (GOOD RIDDANCE) never be the top 1. A sad
story.&lt;/p&gt;

&lt;h2 id=&quot;is-there-a-correlation-between-weeks-spent-on-the-chart-and-weeks-at-1&quot;&gt;Is there a correlation between weeks spent on the chart and weeks at #1?&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;cor(x = df[[&quot;weeks_at_number_1&quot;]],
    y = df[[&quot;weeks_top_50&quot;]])
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## [1] 0.471703
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;df %&amp;gt;%
  ggplot(aes(x=weeks_at_number_1, y=weeks_top_50))+
  geom_jitter(alpha = 0.3, color = &quot;dodgerblue4&quot;)+
  geom_smooth(color = &quot;dodgerblue4&quot;, se = FALSE)+
  theme_classic()+
  labs(x=&quot;\nWeeks at Top 1&quot;, y=&quot;Weeks at Top 50\n&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/7ZxFFMgR/unnamed-chunk-32-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hint: Project GitHub page: &lt;a href=&quot;https://github.com/waittim/Cashbox-Magazine-song-records-Analysis&quot;&gt;Cashbox-Magazine-song-records-Analysis&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 15 Feb 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/02/15/song-analysis/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/15/song-analysis/</guid>
        
        <category>EDA</category>
        
        <category>Project</category>
        
        <category>R</category>
        
        <category>Text Analysis</category>
        
        
      </item>
    
      <item>
        <title>Think and Get Jobs - H1B Visa Analysis</title>
        <description>&lt;h1 id=&quot;slides&quot;&gt;Slides&lt;/h1&gt;
&lt;iframe src=&quot;https://docs.google.com/presentation/d/e/2PACX-1vQR_hrMRbhVNpoudmrkJRcPi3lTcSG0g1lYYnMOElPjTcrC_y1Bxk16DdwxUCLruMuoQEPX0n04Zc8k/embed?start=false&amp;amp;loop=false&amp;amp;delayms=3000&quot; frameborder=&quot;0&quot; width=&quot;700&quot; height=&quot;423&quot; allowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;h1b&quot;&gt;H1B&lt;/h1&gt;
&lt;p&gt;The H1B visa is a non-immigrant visa that allows companies in the US to hire graduate-level workers in specialty occupations that require theoretical or technical expertise in specialized fields.&lt;/p&gt;

&lt;h3 id=&quot;requirements&quot;&gt;Requirements&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;A bachelor’s degree.&lt;/li&gt;
  &lt;li&gt;Job offer from a company within the United States for a specialty position that matches your degree.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;why-is-h1b-popular&quot;&gt;Why is H1B Popular?&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;For a company in the US, applying for H1B is generally quicker than applying for a US Green Card, so it is popular when they want to bring in an employee for a longer period.&lt;/li&gt;
  &lt;li&gt;H1B is open to nationals and citizens of any country, as opposed to other visa types that are only open to people with certain countries of citizenship.&lt;/li&gt;
  &lt;li&gt;H1B allows holders to stay for three years initially and can be easily extended to three more years after the first term.&lt;/li&gt;
  &lt;li&gt;H1B allows holders to move their status from one company to another and also allows its holders to work part-time and work for multiple employers at the same time.&lt;/li&gt;
  &lt;li&gt;The main benefit of H1B that attracts a large volume of applicants is the fact that it is a dual intent visa. This means that it allows its holders to seek permanent residency while under the H1B nonimmigrant status.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;caveats&quot;&gt;Caveats&lt;/h3&gt;
&lt;p&gt;Despite having a lot of advantages, applying for the H1B comes with its own set of caveats or disadvantages.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The biggest drawback of H1B is the fact that there is a limit on the number of petitions that are approved each year. Because of the large number of petitions each year, the USCIS has chosen to have all petitions entered into a lottery. Through this an annual general cap of 65,000 in first, and then run the remaining in the master’s-and-above 20,000 categories.
This means there is a strong likelihood that for any given year, your petition will not be selected. Once rejected, you will have to wait another year to submit another petition.&lt;/li&gt;
  &lt;li&gt;Because of its lottery, the deadlines for the H1B applications are very inflexible.&lt;/li&gt;
  &lt;li&gt;It is difficult to find an employer that would be willing to sponsor an employee for H1B, as the process can get expensive and is unreliable.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;h1b-historic-timeline&quot;&gt;H1B Historic Timeline&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/23/uB49UpgOJeoR6Yh.png&quot; alt=&quot;Timeline&quot; /&gt;
The timeline of relevant events is listed above. Sources for this information are from the New York times, government official documents from USCIS (U.S. Citizenship and Immigration Services).&lt;/p&gt;

&lt;h1 id=&quot;exploration&quot;&gt;Exploration&lt;/h1&gt;

&lt;h3 id=&quot;purpose&quot;&gt;Purpose&lt;/h3&gt;
&lt;p&gt;The main purpose of our exploration was to examine and recognize the trends in H1B Visa and see how data-related jobs in the United States have changed over the past 5 years.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;To perform this analysis, we combined data sets of 5 different years from 2014 to 2018.
&lt;img src=&quot;https://i.loli.net/2019/12/23/HhcSbvMuTYt9mDf.png&quot; alt=&quot;df1&quot; /&gt;
&lt;img src=&quot;https://i.loli.net/2019/12/23/wjlLDzWR3EZJ8am.png&quot; alt=&quot;df2&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Column Name&lt;/th&gt;
      &lt;th&gt;Column Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;job_title	Job&lt;/td&gt;
      &lt;td&gt;title of the particular H1B application.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;case_status&lt;/td&gt;
      &lt;td&gt;Certified-withdrawn; Certified; Denied; Withdrawn; Rejected; Invalidated- we filter this data by certified as we only wish to keep the applications that were approved to be submitted.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;employer_name&lt;/td&gt;
      &lt;td&gt;Employer through which the H1B application was submitted.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;prevailing_wage&lt;/td&gt;
      &lt;td&gt;The annual salary of the job in a particular observation.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;year&lt;/td&gt;
      &lt;td&gt;Year the particular H1B application was submitted.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;worksite&lt;/td&gt;
      &lt;td&gt;Location of the worksite.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;lon&lt;/td&gt;
      &lt;td&gt;Longitudinal location of the worksite.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;lat&lt;/td&gt;
      &lt;td&gt;Latitudinal location of the worksite.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;data_relation&lt;/td&gt;
      &lt;td&gt;If the job title is data-related, then this will have “data-related” else “undefined.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;data_job_title&lt;/td&gt;
      &lt;td&gt;This will be used to classify the data-related job titles into 4 different categories of business analysts, data analysts, data engineers, and data scientists.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;stem&lt;/td&gt;
      &lt;td&gt;This is supposed to classify the h1b application into STEM or non-STEM based on another dataset to recognize its classification.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;soc_code&lt;/td&gt;
      &lt;td&gt;SOC is a federal statistical standard used by federal agencies to classify workers into occupational categories to collect, calculate, or disseminate data.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;number-of-applications-per-state&quot;&gt;Number of Applications Per State&lt;/h3&gt;

&lt;p&gt;We wanted to look at how the number of applications differed per state over the course of the last 5 years.
Here we can see that the states of California, Texas, New York, Michigan, Georgia, Pennsylvania, Florida, and Illinois are the states with the highest number of applications. The number of applicants is seen to increase over time (from 2015 to 2017) for Washington, Virginia and North Carolina. States that have the lowest number of applicants are Montana, Wyoming and South Dakota.&lt;/p&gt;

&lt;iframe src=&quot;https://waittim.github.io/gallery/h1b-map.html&quot; frameborder=&quot;0&quot; width=&quot;700&quot; height=&quot;435&quot; allowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;stem&quot;&gt;STEM&lt;/h1&gt;

&lt;p&gt;Instead of doing an overview of the actual data, we thought it to be better to first filter by STEM (Science, Technology, Engineering, and Management) related jobs. It was expected that the majority of the applications would be coming from STEM, but a confirmation was needed.&lt;/p&gt;

&lt;p&gt;The following analysis was done to see the proportion of applicants for H1B that were in STEM fields vs non-STEM fields. This was done using the SOC (Standard Occupational Classification)  code associated with educational background. This was then cross-referenced (left-join) with a list of SOC codes that were considered to be STEM . We combined this with our analysis of the number of applicants per year as a sum just to see if some trends had changed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/x1tpYPZV/image010.png&quot; alt=&quot;stem-nonstem&quot; title=&quot;Figure 2: Number of applications per year. Proportion of STEM and non-STEM H1B Applicants.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We observed (from Figure. 2) that the number of applications stays high as compared to the yearly cap of 85,000 applications that get approved. The number of applications also keeps increasing each year with only a slight dip in 2017. The changes in the total number of applications don’t seem to affect the proportion of STEM to non-STEM applications each year. This is also very interesting to note as the number of non-STEM jobs available in the United States is much higher than the number of STEM jobs.&lt;/p&gt;

&lt;p&gt;This disproportionately high number might be attributed to the fact that foreign students with STEM degrees are more likely to pursue STEM fields. The reason for this is that USCIS allows foreign students who pursue a STEM degree during their academic career to stay within the United States for up to three years after they have graduated as opposed to a single year for those of non-STEM backgrounds. This means that during those 3 years, you will have three chances to apply for an H1B and would potentially be eligible to apply for permanent residency.&lt;/p&gt;

&lt;p&gt;Another reason why international students might want to pursue STEM would be the prevailing wages for STEM and non-STEM jobs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/j20Zv3DP/image012.png&quot; alt=&quot;&quot; title=&quot;Figure 3: Modified box plot to show only the spread and the median of the
wage distribution between STEM and non-STEM.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The medians of the prevailing wages of the STEM and non-STEM jobs have gotten wider apart but the median for the STEM is always higher than the non-STEM (Figure. 3). In 2018, we see the gap between the two to appear to be the widest.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/VN2RLZBp/image014.png&quot; alt=&quot;&quot; title=&quot;Figure 4: Density plot for the prevailing wages for STEM vs non-STEM jobs.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is visible from the density plot of the wage distribution of the year 2018 that the wages of the STEM jobs in 2018 are not only higher but the distribution is also tighter as compared to the distribution for the non-STEM jobs.&lt;/p&gt;

&lt;h3 id=&quot;top-jobs-in-stem-in-2018&quot;&gt;Top Jobs In STEM in 2018&lt;/h3&gt;

&lt;p&gt;What naturally followed after this was an analysis of what these STEM jobs were, to gain an understanding of why so many foreign students were attracted to them (Figure 5). Software developer and software engineer positions seem to be the top two jobs with the highest number of applications. This might be attributed to the tech boom of the last 5 years, which has greatly increased the demand for software developers.&lt;/p&gt;

&lt;p&gt;Following right after are business analyst and programmer analyst positions. It is important to note that approximately 10 out of these 20 jobs could be associated with data-related jobs. Even if they were not explicitly mentioned as a data-related job, they have, in their job description, data-related roles.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/76nN1xRL/WX20200319-015802-2x.png&quot; alt=&quot;&quot; title=&quot;Figure 5: Top 20 jobs in STEM with the highest number of applications in 2018. All jobs are colored blue, data-related jobs are colored red.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After recognizing the number of data-related jobs within the top 20 job titles in the last year, the next step was to break down what these data-related roles were and if we could combine them and/or break them down based on how complex we wanted our analysis to be.&lt;/p&gt;

&lt;h1 id=&quot;data-related-jobs-trends&quot;&gt;Data Related Jobs Trends&lt;/h1&gt;

&lt;p&gt;We created four categories of data-related jobs for this analysis. This categorization was based on the keywords in the job titles.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Data Job Title&lt;/th&gt;
      &lt;th&gt;Associated Keywords&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Business Analyst&lt;/td&gt;
      &lt;td&gt;“Business Intelligence”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Data Analyst&lt;/td&gt;
      &lt;td&gt;“EDA”, “Visualization”,  “Data aggregation”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Data Scientist&lt;/td&gt;
      &lt;td&gt;“Machine learning”, “Model”, “Algorithm”, “A/B testing”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Data Engineer&lt;/td&gt;
      &lt;td&gt;“Pipeline”, “Data lake”, “ETL”, “Database”, “Warehouse”&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We combined machine learning and deep learning jobs into data science jobs, as they are highly correlated and it also made the following visualizations easier to look at. Since these data points were essentially just combined with another category, we need to note that the number of machine learning/deep learning jobs (with those explicit keywords as titles) have remained very few.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/N026V7q4/image020.png&quot; alt=&quot;&quot; title=&quot;Figure 6: Data related jobs and their four categories. Business Analysts having the highest number of applications and only going down after 2016.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Two important things to notice here (Figure 6). Firstly, the total number of jobs for business analysts is much higher than the total number of jobs for other roles. There also seems to be a downward trend in the number of jobs for business analysts after 2016; however, the number of jobs within business analytics remains much higher than the number of jobs in any of the other categories in any of the years. Secondly, the number of jobs for data scientists, analysts and engineers are all showing an upward trend. There appears to be a higher number of jobs for data analysts as compared to data scientists and data engineers. A reason for this is that the role of data scientists only emerged recently and the number of jobs available in the industry has just recently, in the past couple of years, started growing.&lt;/p&gt;

&lt;h3 id=&quot;prevailing-wages-per-data-related-job-category&quot;&gt;Prevailing Wages Per Data Related Job Category&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/520q5WsD/image022.png&quot; alt=&quot;&quot; title=&quot;Figure 7: Prevailing wages for data-related jobs in 2018&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the prevailing wages for the different categories that we have established, we see that the median salary (from Figure 7) for a data scientist is a lot higher for the median salary for the business analyst. This coincides with our earlier analysis when we saw that the number of applications for H1B is a lot higher than the number of job applications from a data scientist or analyst role. The higher number of jobs and lower salaries make it a good option for someone within data science to use that as an opportunity to get into the industry.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/qvzyhXRT/image024.png&quot; alt=&quot;&quot; title=&quot;Figure 8: Top 5 companies with the highest number of applications for data-related roles&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To get a general trend of what was going on in the industry for data-related roles, we look at five companies with the highest number of applications in the years 2014 to 2017 (Fig. 8). We notice here that despite expecting the highest number of applications to come from tech companies, it comes from 3 consulting firms and two tech companies.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/Y0MQh54n/image026.png&quot; alt=&quot;&quot; title=&quot;Figure 9: Infosys and the downward trend in job applications&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The company Infosys showed a very steep downward trend and also had the highest number of H1B applications in the year 2014 which was on a completely different scale as compared to other similar companies (Fig. 9). This drew the attention of our analysis because it was a highly abnormal trend, but upon deeper examination, it revealed a precautionary tale for foreign students trying to get jobs in the United States.&lt;/p&gt;

&lt;h3 id=&quot;story-behind-infosys&quot;&gt;Story Behind Infosys&lt;/h3&gt;

&lt;p&gt;In 2013, Infosys agreed to pay $34 million to settle a lawsuit against them which claimed they were involved in fraud and abuse of the immigration process in the United States . Infosys brought foreign nationals into the country on visa types that are not authorized for employment in the United States. This means that the company was involved in fraudulent activity. The USCIS has been stricter on companies like Infosys (Wipro, Tata) because they have a history of trying to go around the system and abusing it to employ cheap labor. The denial rate  for these companies has been extremely high and can be seen to effectively reduce the total number of applications submitted through them over the course of the last 5 years.&lt;/p&gt;

&lt;h3 id=&quot;top-tech-companies&quot;&gt;Top Tech Companies&lt;/h3&gt;

&lt;p&gt;A natural assumption for someone entering the market as a data scientist would be that the top tech companies have the highest number of jobs for data-related roles. We examine the top tech giants that are
&lt;img src=&quot;https://i.postimg.cc/HsL7R4by/image028.png&quot; alt=&quot;&quot; title=&quot;Figure 10: Data related job trends in the top tech companies in the past 5 years&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is a general upward trend to the number of data-related jobs for all tech companies, with Google hiring the least number of people within data-related capacities and Amazon hiring the most. While most companies are showing an upward trend, IBM shows a decreasing trend from 2015 to 2017.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/JhXXKrZY/image030.png&quot; alt=&quot;&quot; title=&quot;Microsoft&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/kXhtm6yh/image032.png&quot; alt=&quot;&quot; title=&quot;Google&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;a) Microsoft&lt;/td&gt;
      &lt;td&gt;b) Google&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/cJfKwDGV/image034.png&quot; alt=&quot;&quot; title=&quot;Facebook&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/Zn5Ck4vS/image036.png&quot; alt=&quot;&quot; title=&quot;Amazon &quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;c) Facebook&lt;/td&gt;
      &lt;td&gt;d) Amazon&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Figure 11: Trends in data some of the top tech companies&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All of the tech companies show an increasing trend in the data-related. However, there are two companies that draw our attention. Google (Fig 11 (b)) has a much lower number of data-related jobs as compared to the other companies. This might be attributed to the fact that some jobs might have data-related roles even if they don’t have data-related titles. But overall, Google has very few explicit data roles and thus fewer opportunities for international students.&lt;/p&gt;

&lt;p&gt;Another trend of interest is that of Amazon (Fig 11 (d)) which sees a decrease in the number of applications for data-related roles after the year 2017. This might be attributed to stricter H1B regulations implemented under Trump’s administration and also could be attributed to data scientist jobs doing jobs of data scientists.&lt;/p&gt;

&lt;h3 id=&quot;ibm&quot;&gt;IBM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/wv7jFFCq/image039.png&quot; alt=&quot;&quot; title=&quot;Figure 12: IBM trends for data-related roles - (a)IBM Spaghetti plot for all jobs&quot; /&gt;
&lt;img src=&quot;https://i.postimg.cc/g04WCRVx/image042.png&quot; alt=&quot;&quot; title=&quot;Figure 12: IBM trends for data-related roles - (b)Data-related roles for IBM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;IBM H1B petitions see an overall decrease after the year 2017 (Fig 12 (a)). This is because of how Trump’s administration has affected the H1B denial rates. We can see that  IBM has an increasing denial rate that is not like those of tech firms. This can be attributed to the fact that IBM is not a traditional tech firm; it provides a lot of consulting services. Due to a stricter review, the H1B application process involves providing more information about the type of exact work a company is involved in, the projects and the subcontractors. This hinders non-tech companies from hiring more international students as it will be both expensive and cumbersome. These companies would rather hire US citizens with the same skills if they can avoid it and most of the time, a foreign employee can be replaced with a US citizen.&lt;/p&gt;

&lt;p&gt;This downward trend is probably linked to IBM’s consulting sector combined with stricter government regulations . Despite this trend, the number of applications for specific data-related roles in IBM (Fig 12 (b)) seems to be unaffected by this downward trend.&lt;/p&gt;

&lt;h3 id=&quot;other-consulting-companies&quot;&gt;Other Consulting Companies&lt;/h3&gt;

&lt;p&gt;It would make sense that other consulting or non-tech companies should show trends similar to that of IBM with a decreasing number of applications. And this can be seen in both Deloitte and Accenture (Fig. 13 (a) and (b). Even though Deloitte has been a major employer for data-related roles in the past 5 years, it has shown a sharp decrease in these roles after 2016 (Fig 13 (a)). This decline started before President Trump’s election around 2015, when stricter regulations were implemented on working offsite on H1B.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/FFw7C5F2/image044.png&quot; alt=&quot;&quot; title=&quot;Deloitte&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/9FTzv8rz/image046.png&quot; alt=&quot;&quot; title=&quot;Accenture&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;a) Deloitte Consulting&lt;/td&gt;
      &lt;td&gt;b) Accenture Consulting&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Figure 13: Consulting companies and the trends in data-related roles&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The worksite regulations implemented in 2015 can cause problems for consulting companies as a lot of consulting work may be off-site in another city and not in the official workplace. Also, consulting companies that deal with U.S. Federal Government might not hire international students as the work might require the workers to be U.S. citizens.&lt;/p&gt;

&lt;h1 id=&quot;locations-for-data-related-jobs&quot;&gt;Locations for Data-Related Jobs&lt;/h1&gt;

&lt;h3 id=&quot;nationwide-overview&quot;&gt;Nationwide Overview&lt;/h3&gt;

&lt;iframe src=&quot;https://waittim.github.io/gallery/h1b-map-data.html&quot; frameborder=&quot;0&quot; width=&quot;700&quot; height=&quot;435&quot; allowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This is an incomplete but interesting cartographic analysis (Fig. 14). The first thing we can see from here, however, is that the most number of applications in data-related roles are coming from (starting from the west coast to east) Washington, California, Texas, Illinois, Atlanta, Florida, New York, New Jersey, and Massachusetts.&lt;/p&gt;

&lt;p&gt;Just because the colors seem similar for certain states in this diagram, does not mean that the numbers of data-related H1B applications are the same. They belong to the same interval or above a certain threshold. The numbers for the major states are as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;California - 7,417 applications&lt;/li&gt;
  &lt;li&gt;New York - 2516 applications&lt;/li&gt;
  &lt;li&gt;Washington - 1428 applications&lt;/li&gt;
  &lt;li&gt;Texas - 2499 applications&lt;/li&gt;
  &lt;li&gt;New Jersey - 2007 applications&lt;/li&gt;
  &lt;li&gt;Massachusetts - 1570 applications&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Wyoming and Montana have 2 and 5 H1B applications, respectively,  in 2018. This indicates that these would be the worst states to apply for data-related jobs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/rFqvkzpW/image051.png&quot; alt=&quot;&quot; title=&quot;Figure 15: Alluvial diagram showing the distribution of the number of applications from each role &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Upon taking a deeper look at the different jobs and the top states for those jobs, we can see how the number of applications for the data-related roles is being distributed among the top states. What is interesting to note here is that the Business Analyst role is higher for almost all states except for the state of Michigan where the Data Analyst applications are higher than the Business Analyst applications. We can see the majority of Data Scientist applications are coming from California while Washington and New York follow right after.&lt;/p&gt;

&lt;h3 id=&quot;regions-of-the-united-states&quot;&gt;Regions of the United States&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/vTtknw0c/image053.png&quot; alt=&quot;&quot; title=&quot;Figure 16: 1 Point = 1 City. Color = Data position with the highest number of jobs in that city&quot; /&gt;
&lt;em&gt;Figure 16: 1 Point = 1 City. Color = Data position with the highest number of jobs in that city&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To find where would be best to apply based on the type of data role, a deeper analysis is needed into what cities would be best to apply to, based on the number of proportions of jobs available for each of the data roles. In Figure 16, each point is a city and the color of the city is given by what data-position provided the majority of the application in 2017 and 2018. Using two years for the analysis instead of one is better as it provides a better picture of the data-related jobs.  We can see lots of blue clusters forming on the east coast, while some black clusters forming in Michigan and more of a variety of colors in California in the Bay Area and Washington.&lt;/p&gt;

&lt;h3 id=&quot;the-western-united-states&quot;&gt;The Western United States&lt;/h3&gt;

&lt;p&gt;The Western United States has two main states of interest for data-related roles, Washington and California.
&lt;img src=&quot;https://i.postimg.cc/zG8D4qsP/image055.png&quot; alt=&quot;&quot; title=&quot;Figure 17: State of Washington and data-related roles in that state &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Washington has a significant amount of data-related roles with Seattle having a majority of Business Analyst applications and Redmond (red circle) has a higher number of data scientists. This may be because of the Amazon and Microsoft headquarters being located in Seattle and Redmond.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/85JTLcvs/image057.png&quot; alt=&quot;&quot; title=&quot;Figure 18: The state of California and data-related roles in that state&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If pursuing a data scientist role or job title, state of California seems to have the most variety and also the highest number of applications for H1B applications for that role (668 H1B applications). The big red circles show that a large number of data scientists applied for H1B from the bay area (San Jose, San Francisco, and Oakland). This makes complete sense because of the presence of Silicon Valley, where most of the headquarters for tech companies are located, and the recent boom in data science jobs in the tech industry (cite).&lt;/p&gt;

&lt;h3 id=&quot;the-northeastern-united-states&quot;&gt;The Northeastern United States&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/cJPpymK4/image059.png&quot; alt=&quot;&quot; title=&quot;Figure 19: The Northeastern States and data-related roles in those states&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see four main clusters on this map. Boston, New York, Philadelphia, and Washington DC. The majority of these clusters are blue as northeast has a relatively large number of finance and insurance companies (+ 22.7% relative as compared to other places in the United States) , and these companies have a large number of business analyst or data analytics positions as compared to other roles. The existence of some black clusters is an indication of this industry requiring data analysts.&lt;/p&gt;

&lt;h3 id=&quot;the-midwestern-united-states&quot;&gt;The Midwestern United States&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/wvJS6mJ5/image061.png&quot; alt=&quot;&quot; title=&quot;Figure 20: Midwestern states and data-related roles in those states&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The midwest shows relatively more black clusters as compared to any other region. There is a large black cluster in the state of Michigan indicating a significant number of data analyst positions, in Detroit. And a large blue cluster in Chicago indicating a large number of business analyst positions.&lt;/p&gt;

&lt;h3 id=&quot;state-of-tennessee&quot;&gt;State of Tennessee&lt;/h3&gt;

&lt;p&gt;The final and most important analysis, relevant to the authors of this paper and the potential individuals reading it, is an analysis of the data-related job market for international students, using H1B, in the state of Tennessee.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/zXjYbKQ0/image063.png&quot; alt=&quot;&quot; title=&quot;Figure 21: Data related jobs in the state of Tennessee &quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are two major cities where you find the biggest clusters for data-related roles: Nashville and Memphis. Both these cities have applications mainly for business analytics. We can see some sprinkles of data analyst roles in other parts of the state, with one small dot representing data science applications around the Oak Ridge area. This is from an employer named Oak Ridge National Laboratory and they hire data scientists for the work and research that they perform.&lt;/p&gt;

&lt;p&gt;This does not make Tennessee the best state for foreign students who are pursuing data science; however, growth in data science jobs is projected over the next few years, due to tech companies moving to Tennessee.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/wMYYJVHb/image065.png&quot; alt=&quot;&quot; title=&quot;Figure 13: Proportion of data-related jobs as compared to other jobs&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 13 shows that there has been a growth in data-related roles in the past 5 years, with 2018 showing the highest number of applications for data-related roles in the year 2018. This is promising, as it will mean a higher number of data-related roles that will hire and sponsor international students in the years to come.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;To reiterate, the purpose of this data exploration was to examine and recognize the trends in H1B visa applications and see how data-related jobs in the United States have changed over the past 5 years. This analysis can be used to assist international students in pursuing a data-related degree in determining what type of jobs they should apply for and where they should be considering moving to in the future if they wanted to stay within the United States.&lt;/p&gt;

&lt;p&gt;We can gather from our analysis that among the four data-related job titles that we divided our dataset into (business analyst, data analyst, data engineers and data scientists), the highest number of jobs are available for the business analyst job title or job classifications. We also note that the salary for this job role is much lower as compared to the other data-related jobs. The barrier to entry for data scientists, for this specific role, might be a lot lower as compared to other roles. This is because there are a lot of jobs for data scientists, and potential employers would be more open to hiring data scientists from business backgrounds, as they would be able to perform well in roles associated with business analytics.&lt;/p&gt;

&lt;p&gt;In terms of what type of company data scientists should apply for, we find that some big companies such as Google and Apple (shown in the final-report.Rmd), have fewer data-related jobs for international students despite being major tech companies. However, there does seem to be an upward trend in data-related jobs in the tech industry.&lt;/p&gt;

&lt;p&gt;We can also see those consulting companies are harder to get jobs in for data-related roles, as they are more affected by governmental regulations and policies. These positions may be limited to U.S. citizens or permanent residents. It is also cheaper for these companies to hire non-foreign workers, as consulting roles are not as profitable as purely technical roles and therefore may not need extremely specialized foreign talent (for those companies).&lt;/p&gt;

&lt;p&gt;The location analysis will help international students pursuing a data-related degree, to find the most probable location they should move to based on the type of data-related role they want to pursue. The best location to find a job in business analytics is in the northeastern states as they have a large number of finance and insurance companies. However, there is no shortage of business analytics roles, since these are very commonly found in almost every company. So almost every state has a large number of these roles. Nevertheless, it is still important to note that the total number of applications for H1B in business analytics is showing a generally downward trend.&lt;/p&gt;

&lt;p&gt;The best location to find a job as a data analyst or in data analytics in the Northeast or the Midwest, as seen from the clusters in the maps that we explored. However, since a large number of job applications come from California, almost all of these roles are plentiful in the state of California. California is also where a large number of specific data scientists and data engineering roles are available, specifically in the bay area where a large number of tech companies are located. The state of Washington (Seattle and Redmond area) is good to find data engineering and data science jobs as well because of the existence of Microsoft and Amazon.&lt;/p&gt;

&lt;p&gt;Finally, although Tennessee is not the best for international students pursuing data-related roles, there does seem to be a potential for growth since tech giants such as Amazon and Microsoft are creating more opportunities for jobs in the Nashville area in the future.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hint: Project GitHub page: &lt;a href=&quot;https://github.com/waittim/H1B-visas-analysis&quot;&gt;H1B-visas-analysis&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Dec 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/12/12/h1b-analysis/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/12/12/h1b-analysis/</guid>
        
        <category>H1B</category>
        
        <category>EDA</category>
        
        <category>Project</category>
        
        <category>R</category>
        
        
      </item>
    
      <item>
        <title>As data scientists, what should we do?</title>
        <description>&lt;h1 id=&quot;slides&quot;&gt;Slides&lt;/h1&gt;
&lt;iframe src=&quot;https://docs.google.com/presentation/d/e/2PACX-1vTr02Z5Zn7lGTvEo2Jz5P0ADkz76YFfvNOnQUXeQiHwZ5CrpOJhR2fL8avHvWIIcQtWGTb6wOTmLNRS/embed?start=false&amp;amp;loop=true&amp;amp;delayms=15000&quot; frameborder=&quot;0&quot; width=&quot;700&quot; height=&quot;423&quot; allowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;3 big parts in Data Science jobs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Use &lt;strong&gt;EXPLORATION&lt;/strong&gt; to turn the unknown into the known.&lt;/li&gt;
  &lt;li&gt;Use &lt;strong&gt;INFERENCE&lt;/strong&gt; to help find something new from the old.&lt;/li&gt;
  &lt;li&gt;Use &lt;strong&gt;PREDICTION&lt;/strong&gt; to make better decisions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;scene-setting&quot;&gt;Scene Setting&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Imagine a scenario in which our superior asked us to make suggestions for the launch of a new product, and all we have is a data set that contains the past relevant situation of this type of product.&lt;/p&gt;

  &lt;p&gt;What can we do now? What should we do?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is a common scenario for the industry. Maybe different backgrounds, different fields, but the same purpose: we need to start with pure data and go on to obtain a product that can be implemented.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So what are the key steps in this process?&lt;/strong&gt;
Overall, we can divide it into three parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;EXPLORATION&lt;/li&gt;
  &lt;li&gt;INFERENCE&lt;/li&gt;
  &lt;li&gt;PREDICTION&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;concepts--connections&quot;&gt;Concepts &amp;amp; Connections&lt;/h1&gt;

&lt;h4 id=&quot;01-exploration&quot;&gt;01 Exploration&lt;/h4&gt;

&lt;p&gt;It is also called &lt;strong&gt;visualization&lt;/strong&gt;. Use visual exploration to understand what is in a dataset and the characteristics of the data.&lt;/p&gt;

&lt;p&gt;Identify potential relationships or insights that may be hidden in the data which will help &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;prediction&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;02-inference&quot;&gt;02 Inference&lt;/h4&gt;

&lt;p&gt;Use data analysis to deduce the properties of a dataset. The properties will give the analyst a more accurate perception of the data and provide a theoretical basis for &lt;strong&gt;prediction&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;03-prediction&quot;&gt;03 Prediction&lt;/h4&gt;

&lt;p&gt;Reference &lt;strong&gt;exploration&lt;/strong&gt; and &lt;strong&gt;inference&lt;/strong&gt; results encompass a variety of techniques from data mining, predictive modeling, and machine learning, that analyze current and historical facts to make predictions about unknown events.&lt;/p&gt;

&lt;h1 id=&quot;01-exploration-visualization&quot;&gt;01 Exploration (Visualization)&lt;/h1&gt;

&lt;p&gt;It is difficult for humans to process large numbers quickly, especially if the numbers are not specifically ordered. Not to mention the quick calculation of the proportional relationship between the numbers, the information within the group under certain conditions.&lt;/p&gt;

&lt;p&gt;But the &lt;strong&gt;human eye can quickly process image information&lt;/strong&gt; and extract key content from it. Therefore, facing a new data set, visualizing the data is one of the best ways to understand it.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For product data, we can explore the sales of this type of product under different channels, delivery costs, and changes over time. We can also compare the differences between different sub-categories under this large category of products to help differentiate the positioning of products.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The process of exploration and visualization can help us &lt;strong&gt;turn the unknown into the known&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Based on fMRI images of the brain of insomnia patients, explore whether there is any difference between them and the images of ordinary people.&lt;/p&gt;

  &lt;p&gt;Explore whether physical pain and social pain behave differently in the brain.&lt;/p&gt;

  &lt;p&gt;– Catie Chang (Assistant Professor of Computer Science, Vanderbilt University)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Visualizing personal stories that are closer to real life enables viewers to have a more immersive experience.&lt;/p&gt;

  &lt;p&gt;– Yorgos Askilidis (Senior Data Scientist, Instagram)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;02-inference-1&quot;&gt;02 Inference&lt;/h1&gt;
&lt;p&gt;Due to collection costs, storage costs, or historical reasons, we cannot have a data set that contains everything that may be needed. In actual data analysis, we may need to display information that is not included in the data set directly. &lt;strong&gt;Visualization can only show information directly represented&lt;/strong&gt; in the data, but for the data that is not directly displayed, we need to use inference related theory to make inferences based on the existing information.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For product data example, we can only see the values of variables of each product in the past, but the values do not directly tell us what products are successful. We need to infer and judge the success of a product based on sales volume, cost, profit margin, capital turnover cycle and the stability of the production process of the product itself.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The significance of inference is to get new information from what is already known. In other words, the inference can help &lt;strong&gt;find something new from the old&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Use billing codes, drug names, and more to mark Parkinson’s patients.&lt;/p&gt;

  &lt;p&gt;Use pathological data to distinguish normal samples from abnormal samples in order to extract control groups for analysis.&lt;/p&gt;

  &lt;p&gt;– Paul Harris (Professor of Biomedical Informatics, Vanderbilt University REDCap)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Find out the most accurate and efficient indicators that can determine whether a patient has hypertension among a large number of indicators such as genetic data.&lt;/p&gt;

  &lt;p&gt;– Josh Denny (Professor of Biomedical Informatics and Medicine, Vanderbilt University)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;03-prediction-1&quot;&gt;03 Prediction&lt;/h1&gt;

&lt;p&gt;The information obtained by the visualization and inference process is based on the existing data, and the existing data must be a description of what happened before.&lt;/p&gt;

&lt;p&gt;In actual work, it is not enough to focus on the past. We often need to &lt;strong&gt;make decisions about the future&lt;/strong&gt;. At this point, data-based forecasting will become a vital part.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In the example of the product launch, we may need to predict the possible sales and profitability of the planned product before the actual product launch. According to the forecast results, we can change the resource allocation methods such as product line and pre-production volume to reduce risks and obtain higher returns.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The value of prediction lies in reducing the cost of trial and error and &lt;strong&gt;making better decisions&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Prediction finds target markets that are “likely to buy” and “definitely buy”, and launches products when the predicted value exceeds a certain threshold.&lt;/p&gt;

  &lt;p&gt;– Kelly Goldsmith (Associate Professor of Marketing, Vanderbilt University)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Predict the number of viewers to help determine the number of ad views that can be sold, and adjust the number of ads sale based on the forecast to complete the expected playback with minimal cost.&lt;/p&gt;

  &lt;p&gt;– Minchun Zhou (Senior Data Scientist, Comcast)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;For Wal-Mart in-home delivery, predict what and how often specific consumers might buy, and related products that they might want to try.&lt;/p&gt;

  &lt;p&gt;– Betsy Barton (Director of Data Science, Walmart In-Home Delivery)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;epicycle&quot;&gt;Epicycle&lt;/h1&gt;

&lt;p&gt;Of course, &lt;strong&gt;this process may be performed more than once&lt;/strong&gt;. We may need to perform multiple processing on the same data set, and re-reflect and summarize after each processing, considering what can be implemented next.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For example, after visualization, think about whether there are still questions that not clear enough and need to be explored. After using inference to investigate, consider again how to visualize the results. And based on the results obtained in previous steps, we can also think about whether we can use them to make predictions and what are the suitable prediction methods.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In a broad sense, this process also belongs to some kind of epicycle. These parts do not appear in a purely linear sequence, but exist in a &lt;strong&gt;repeatable and mutually affecting relationship&lt;/strong&gt;. They constantly promote each other and ultimately form a successful project.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;In order to achieve the goal more efficiently and successfully, in the actual operation process, we should first &lt;strong&gt;determine what type of things we need to do&lt;/strong&gt;. Do we want to understand the data, or dig deeper based on the existing information, or predict what has not happened based on the existing data.&lt;/p&gt;

&lt;p&gt;After confirming the purpose, it is clear from the definition whether what we need to do is exploration, inference or prediction.&lt;/p&gt;

&lt;p&gt;Finally, the best solution will be found according to the category.&lt;/p&gt;

&lt;p&gt;At the same time, we need to carry out this process &lt;strong&gt;multiple times&lt;/strong&gt; in the same project, and constantly improve and expand until a satisfactory result is achieved.&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Dec 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/12/11/ds-applications/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/12/11/ds-applications/</guid>
        
        <category>Career</category>
        
        <category>Talk</category>
        
        
      </item>
    
      <item>
        <title>Power and sample size calculations correlational studies</title>
        <description>&lt;p&gt;A common research objective is to demonstrate that the two measurements are highly correlated. One measurement, call it A, may reflect the severity of disease but is difficult or costly to collect. Another measurement, call it B, may be easier to collect and potentially related to measurement A. If there is a strong association between A and B, a cost-effective strategy for diagnosis may be to collect measurement B instead of A.&lt;/p&gt;

&lt;p&gt;The researcher will collect both measurements on N individuals. The analysis will proceed by calculating a one-sided confidence interval. If the confidence interval is completely within the range from 0.8 to 1, then the researcher will consider the study to be a success: A conclusive demonstration that the correlation between A and B is greater than 0.8.&lt;/p&gt;

&lt;p&gt;Power is the probability that the study will end in success when the true underlying correlation is. The code below provides the power calculation for a single combination of N and population correlation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;set.seed(1122)
suppressPackageStartupMessages(require(mvtnorm))
N &amp;lt;- 50
rho &amp;lt;- .95
null_correlation &amp;lt;- 0.8
R &amp;lt;- 5000

sigma &amp;lt;- array(c(1,rho,rho,1), c(2,2))
mu &amp;lt;- c(0,0)

detect &amp;lt;- rep(NA, R)
for(i in 1:R){
  data &amp;lt;- rmvnorm(N, mean = mu, sigma = sigma)
  results &amp;lt;- cor.test(x = data[,1], y = data[,2], alternative = &quot;greater&quot;)
  detect[i] &amp;lt;- results$conf.int[1] &amp;gt; null_correlation
}
power &amp;lt;- mean(detect)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the simulation part, we need to use correlations from 0.8 to 0.95 and the sample size from 25 to 100. Now, let’s create a table to save the generated powers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;corr_list &amp;lt;-  seq(0.8,0.95,0.01)
N_list &amp;lt;- seq(25,100,25)
result &amp;lt;- expand.grid(corr=corr_list, N = N_list, power=NA)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And use a for loop to apply all of the correlations and sample size and calculate the powers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;for (j in 1:nrow(result)){
  N &amp;lt;- result[j,2]    #50
  rho &amp;lt;- result[j,1]  #.8
  null_correlation &amp;lt;- 0.8
  R &amp;lt;- 5000

  sigma &amp;lt;- array(c(1,rho,rho,1), c(2,2))
  mu &amp;lt;- c(0,0)

  detect &amp;lt;- rep(NA, R)
  for(i in 1:R){
    data &amp;lt;- rmvnorm(N, mean = mu, sigma = sigma)
    results &amp;lt;- cor.test(x = data[,1], y = data[,2], alternative = &quot;greater&quot;)
    detect[i] &amp;lt;- results$conf.int[1] &amp;gt; null_correlation
  }
  power &amp;lt;- mean(detect)
  result[j,3] &amp;lt;- power
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Transform the table into the data frame type and plot the graph.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;result_df &amp;lt;- as.data.frame(result) %&amp;gt;%
  mutate(N=factor(N))

result_df %&amp;gt;%
  ggplot()+
  geom_line(aes(x=corr,y=power,color=N),size=1)+
  theme_bw()+
  labs(x=&quot;Correlation&quot;,y=&quot;Power&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/02c64Qkt/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, with the N increasing, the power under a certain correlation is growing up. In other words, when we can collect more samples, the probability that the study will end in success will be higher. And definitely, when the real correlation is higher, the probability will be higher.&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Nov 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/11/22/power-correlation/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/22/power-correlation/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        <category>Correlation</category>
        
        
      </item>
    
      <item>
        <title>Central limite theorem - Approximation</title>
        <description>&lt;p&gt;Import the packages first.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(magrittr)
library(sn)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;situation-description&quot;&gt;Situation Description&lt;/h1&gt;

&lt;p&gt;The central limit theorem is an important computational short-cut for generating and making inference from the sampling distribution of the mean. I will recall that the central limit theorem short-cut relies on a
number of conditions, specifically:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Independent observations&lt;/li&gt;
  &lt;li&gt;Identically distributed observations&lt;/li&gt;
  &lt;li&gt;Mean and variance exist&lt;/li&gt;
  &lt;li&gt;Sample size large enough for convergence&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this simulation study, I.m going to compare the sampling distribution of the mean generated by simulation to the sampling distribution implied by the central limit theorem. I will compare the distributions graphically in QQ-plots.&lt;/p&gt;

&lt;p&gt;This will be a 4 × 4 factorial experiment. The first factor will be the sample size, with N = 5, 10, 20, and 40. The second factor will be the degree of skewness in the underlying distribution. The underlying distribution will be the Skew-Normal distribution. The Skew-Normal distribution has three parameters: location
&lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt;, scale
&lt;script type=&quot;math/tex&quot;&gt;\omega&lt;/script&gt;, and slant
&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;. When the slant parameter is 0, the distribution reverts to the normal distribution. As the slant parameter increases, the distribution becomes increasingly skewed. In this simulation, the slant will be set to 0, 2, 10, 100. Set location and scale to 0 and 1, respectively, for all simulation settings.&lt;/p&gt;

&lt;h1 id=&quot;plot-preparation&quot;&gt;Plot preparation&lt;/h1&gt;

&lt;p&gt;In the very beginning, we need to set up the parameters that do not change in the following steps. The slant of Skew-Normal distribution will change later, therefore, only the location
&lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt; and scale
&lt;script type=&quot;math/tex&quot;&gt;\omega&lt;/script&gt; will be set in this part.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;R &amp;lt;- 5000
location &amp;lt;- 0
scale &amp;lt;- 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;location:
&lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt;, scale:
&lt;script type=&quot;math/tex&quot;&gt;\omega&lt;/script&gt;, slant:
&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Before creating the function, let’s clarify the functions for calculating the delta, mean and standard deviation for the central limit theorem (CLT).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Delta:
&lt;script type=&quot;math/tex&quot;&gt;\delta =\frac{\alpha}{\sqrt{1+\alpha^2}}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mean:
&lt;script type=&quot;math/tex&quot;&gt;\xi+\omega \delta \sqrt{\frac{2}{\pi}}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Standard deviation:
&lt;script type=&quot;math/tex&quot;&gt;\sqrt{\omega^2(1-\frac{2\delta^2}{\pi})  }&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then define the function for the CLT process and generating the QQplots by using the functions before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;qqplot_creator &amp;lt;- function(slant, N) {
  delta &amp;lt;- slant / (sqrt(1 + slant ^ 2))

  # Quantites to calculate/generate
  pop_mean &amp;lt;- location + scale * delta * (sqrt(2 / pi))
  pop_sd &amp;lt;- sqrt(scale ^ 2 * (1 - ((2 * delta ^ 2) / pi)))

  Z &amp;lt;- rnorm(R) # generate the normal distribution as the basement

  #CLT approximation
  sample_dist_clt &amp;lt;- Z * (pop_sd / sqrt(N)) + pop_mean

  #Simulation approximation
  random.skew &amp;lt;- array(rsn(R * N, xi = location, omega = scale, alpha = slant),
                      dim = c(R, N))

  sample_dist_sim &amp;lt;- apply(random.skew, 1, mean)

  qqplot(sample_dist_clt, sample_dist_sim, axes = FALSE, frame.plot=TRUE, ann = FALSE)
  abline(0,1)

  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;qqplot-generation&quot;&gt;QQplot generation&lt;/h1&gt;

&lt;p&gt;Now we can set the slants and Ns we want to test in the following steps. As the requirement, the N = 5, 10, 20, and 40 and slant will be set to 0, 2, 10, 100. Then create a sequence to define the points where we want to test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;slant &amp;lt;- c(0,2,10,100)
N &amp;lt;- c(5,10,20,40)
x &amp;lt;- seq(-2,2,0.01)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set a graph for put all of the QQplots together and use the qqplot_creator function to fill the QQplots inside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;par(mfrow=c(4,5),mai=c(0.1,0.1,0.1,0.1), oma = c(0, 4, 4, 0))

for(i in slant){
  plot(dsn(x,
           xi = location,
           omega = scale,
           alpha = i),
       axes = FALSE,
       frame.plot=TRUE,
       type = &quot;l&quot;,
       xlab = NA, ylab = NA)
  for(j in N){
    qqplot_creator(i, j)
  }
}
mtext(text=&quot;Distribution              N=5                N=10                   N=20                    N=40&quot;,
      side = 3,
      outer = TRUE)
mtext(text=&quot;slant = 100   slant = 10       slant = 2      slant = 0&quot;,
      side = 2,
      outer = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/rwCbSBq2/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Definitely, when the N is bigger, the QQplot will fit the y=x line better, which means the CLT works better when it wants to simulate the distribution. And when the slant is bigger, in other words, the Skew-Normal distribution has higher skewness, it will be more difficult to simulate the distribution.&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Nov 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/11/12/clt/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/12/clt/</guid>
        
        <category>Probability</category>
        
        <category>Simulation</category>
        
        <category>R</category>
        
        <category>CLT</category>
        
        
      </item>
    
      <item>
        <title>What makes a trending video?</title>
        <description>&lt;h1 id=&quot;slides&quot;&gt;Slides&lt;/h1&gt;
&lt;iframe src=&quot;https://docs.google.com/presentation/d/e/2PACX-1vTN6OHLfZ70A3x6X97oe68aBpaGtQUUmvcnJ-n9QSCdz99m_S4hYXZYY__sMhHhwB0hapDY1y2HuHuo/embed?start=false&amp;amp;loop=true&amp;amp;delayms=30000&quot; frameborder=&quot;0&quot; width=&quot;700&quot; height=&quot;423&quot; allowfullscreen=&quot;true&quot; mozallowfullscreen=&quot;true&quot; webkitallowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;YouTube is one of the biggest media platforms of the 21st century.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5 Billion&lt;/strong&gt; videos are watched daily.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;500 Hours&lt;/strong&gt; of videos are uploaded every minute.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;51%&lt;/strong&gt; of users say that they visit the website daily.&lt;/p&gt;

&lt;p&gt;This makes YouTube a powerful tool for advertising, business, research and content creators.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Being able to predict if a YouTube video is likely to be trending or not is highly useful if you are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;A content creator&lt;/strong&gt; who wants to make trending videos.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;An advertiser&lt;/strong&gt; who wants to know the best videos to put advertisements on before they become trending videos.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;A researcher&lt;/strong&gt; trying to derive insight about behavior from YouTube trends.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To do this, you will need to know :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;When&lt;/strong&gt; to upload/look for, a potentially trending YouTube video&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How&lt;/strong&gt; to upload/look for, a potentially trending YouTube video&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What&lt;/strong&gt; to upload/look for, in a potentially trending YouTube video&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;a-look-at-the-trending-videos&quot;&gt;A look at the trending videos&lt;/h2&gt;

&lt;p&gt;A trending video can be identified by three major measures:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Number of &lt;strong&gt;Likes&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Number of &lt;strong&gt;Views&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Number of &lt;strong&gt;Comments&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Likes&lt;/th&gt;
      &lt;th&gt;Views&lt;/th&gt;
      &lt;th&gt;Comments&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/MZQhgDcr/image.png&quot; alt=&quot;&quot; title=&quot;dislikes &amp;amp; likes&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/2ytyqxDt/image.png&quot; alt=&quot;&quot; title=&quot;views&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/XqtNN7KS/image.png&quot; alt=&quot;&quot; title=&quot;comments&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Upon looking at the distribution of likes, dislikes , views and comments of the trending YouTube videos, we can see that, “most of” the trending YouTube videos have a total of:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;1,000,000&lt;/strong&gt; views&lt;/li&gt;
  &lt;li&gt;About &lt;strong&gt;1,000&lt;/strong&gt; dislikes or &lt;strong&gt;10,000&lt;/strong&gt; likes&lt;/li&gt;
  &lt;li&gt;About &lt;strong&gt;2,000&lt;/strong&gt; comments&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will be looking at the number of videos being posted and the number of views being received.&lt;/p&gt;

&lt;h1 id=&quot;when-to-post-a-trending-video&quot;&gt;When to post a trending video&lt;/h1&gt;

&lt;h2 id=&quot;best-month&quot;&gt;Best month&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/Dy2mZ8KF/image.png&quot; alt=&quot;&quot; title=&quot;best month&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lowest&lt;/strong&gt; number of trending videos are being posted in the months of &lt;strong&gt;June&lt;/strong&gt; to &lt;strong&gt;October&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;It also shows that the &lt;strong&gt;highest&lt;/strong&gt; number of trending videos are posted in the month of &lt;strong&gt;May&lt;/strong&gt; with highest number of views.&lt;/p&gt;

&lt;p&gt;The best time to post a video would be &lt;strong&gt;April&lt;/strong&gt; and &lt;strong&gt;May&lt;/strong&gt; as they show the &lt;strong&gt;highest&lt;/strong&gt; number of views in comparison to the number of videos posted.  Months December, March and February also see a high number of trending videos being posted but not as many views.&lt;/p&gt;

&lt;h2 id=&quot;best-day-of-the-week&quot;&gt;Best day of the week&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/nL7nTJt0/image.png&quot; alt=&quot;&quot; title=&quot;Best day of the week&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most trending videos are being posted on &lt;strong&gt;Tuesdays&lt;/strong&gt; through &lt;strong&gt;Friday&lt;/strong&gt;. Thursday and Friday see the highest number of trending videos with Friday showing the highest number of views.&lt;/p&gt;

&lt;p&gt;Interestingly,  &lt;strong&gt;Saturday&lt;/strong&gt; and &lt;strong&gt;Sunday&lt;/strong&gt; are the worst to post videos as there are fewer videos being posted on these and these days see &lt;strong&gt;lowest number of views&lt;/strong&gt; as compared to other days.&lt;/p&gt;

&lt;p&gt;However, there might be an extremely small chance that a video that you post on Sunday reaches &lt;strong&gt;200 million views&lt;/strong&gt;!&lt;/p&gt;

&lt;h2 id=&quot;best-time-of-day&quot;&gt;Best time of day&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/tCYQ2j3W/image.png “Best time of day 1”&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://i.postimg.cc/YCKnYKbp/image.png&quot; alt=&quot;&quot; title=&quot;Best time of day 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see the highest number of trending videos being posted around &lt;strong&gt;8AM to 5PM&lt;/strong&gt; with the peak time from &lt;strong&gt;9AM to 1PM&lt;/strong&gt;. (CST)&lt;/p&gt;

&lt;p&gt;Surprisingly, around &lt;strong&gt;11PM&lt;/strong&gt;, where few of the the lowest amount of videos are being posted, we see the &lt;strong&gt;highest variation&lt;/strong&gt; in the number of views!&lt;/p&gt;

&lt;p&gt;The best time to post a video would be around &lt;strong&gt;8AM to 11AM&lt;/strong&gt;. However, there is small chance if you post at &lt;strong&gt;11PM&lt;/strong&gt;, that your video will get the highest number of views, though it might be risky as this is also.&lt;/p&gt;

&lt;h1 id=&quot;how-to-post-a-trending-video&quot;&gt;How to post a trending video&lt;/h1&gt;

&lt;h2 id=&quot;rows-of-description&quot;&gt;Rows of description&lt;/h2&gt;

&lt;p&gt;“Rows of description” are defined by the number of rows it takes to give the description of the video.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/SRcZtgK0/image.png&quot; alt=&quot;&quot; title=&quot;Rows of description&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most trending YouTube videos have &lt;strong&gt;1 - 3 rows&lt;/strong&gt; of description only and videos with &lt;strong&gt;1 row of description&lt;/strong&gt; see the highest number of views. Keeping a simple description of the video may be what attracts or loses a viewer.&lt;/p&gt;

&lt;p&gt;However, we see a spike in number of views at around 20 rows of description. These might be videos which required further explanation.&lt;/p&gt;

&lt;h2 id=&quot;number-of-tags&quot;&gt;Number of tags&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/dVgyLwBB/image.png&quot; alt=&quot;&quot; title=&quot;Number of tagshttps://i.postimg.cc/dVgyLwBB/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are a large number of trending videos with no tags but they see a lower number of views. Anywhere between &lt;strong&gt;3 to 20&lt;/strong&gt; tags see a higher number of views. Most people uploading videos chose to add tags within this range.&lt;/p&gt;

&lt;p&gt;We can also see that adding more tags outside of this range resulted in a decline in the number of views. Furthermore, the number of people choosing to add tags greater than this range also declined.&lt;/p&gt;

&lt;h2 id=&quot;title-of-a-trending-video&quot;&gt;Title of a trending video&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Number of Uppercase Letters&lt;/th&gt;
      &lt;th&gt;Length of Title&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/cHL0W8n5/10.png&quot; alt=&quot;&quot; title=&quot;Number of Uppercase Letters&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://i.postimg.cc/0QXxy2G5/11.png&quot; alt=&quot;&quot; title=&quot;Length of Title&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;the greatest number of views appeared on videos where the title had between &lt;strong&gt;1 to 15&lt;/strong&gt; uppercase letters. Most uploaders chose to stay within this range.&lt;/p&gt;

&lt;p&gt;the highest number of views appeared on videos where the title was less than &lt;strong&gt;50&lt;/strong&gt; characters long. Uploaders mostly chose titles between &lt;strong&gt;25-50&lt;/strong&gt; characters long.&lt;/p&gt;

&lt;h1 id=&quot;what-is-in-a-trending-video&quot;&gt;What is in a trending video&lt;/h1&gt;

&lt;h2 id=&quot;categorical-analysis&quot;&gt;Categorical analysis&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/TPFMzHTh/image.png&quot; alt=&quot;&quot; title=&quot;Categorical analysis&quot; /&gt;
Categories of &lt;strong&gt;Entertainment&lt;/strong&gt; and &lt;strong&gt;Music&lt;/strong&gt; have the highest number of trending videos. However, Music has a higher number of views as compared to Entertainment.&lt;/p&gt;

&lt;p&gt;Film and animation has a lower number of trending videos as compared to Entertainment but has comparable views.&lt;/p&gt;

&lt;p&gt;If you are a &lt;strong&gt;musician&lt;/strong&gt; or an &lt;strong&gt;entertainer&lt;/strong&gt;, your videos are more likely to be trending as compared to any other type of video.&lt;/p&gt;

&lt;h2 id=&quot;likesdislikes-ratio&quot;&gt;Likes/Dislikes ratio&lt;/h2&gt;

&lt;p&gt;If the L/D ratio is less than 1, more people disliked the video. If greater than 1, more people liked the video. If equal to 1, an equal number of people liked and disliked the video. A video is &lt;strong&gt;controversial&lt;/strong&gt; when the L/D ratio is less than 1 or equal to 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/6qzHXyCz/image.png&quot; alt=&quot;&quot; title=&quot;Likes/Dislikes ratio&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If L/D = &lt;strong&gt;10-100&lt;/strong&gt; we see &lt;strong&gt;the highest views&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If L/D ratio is &lt;strong&gt;less than 10&lt;/strong&gt;, the number of views &lt;strong&gt;decline&lt;/strong&gt;. &lt;strong&gt;The more controversial&lt;/strong&gt; a video is, &lt;strong&gt;the less number&lt;/strong&gt; of views it has.&lt;/p&gt;

&lt;p&gt;However, videos with higher &lt;strong&gt;L/D ratios (&amp;gt;100)&lt;/strong&gt; coincided with &lt;strong&gt;fewer views&lt;/strong&gt;. It seems that videos which are slightly controversial may be viewed more than videos which are not controversial at all.&lt;/p&gt;

&lt;h2 id=&quot;sentiment-analysis&quot;&gt;Sentiment Analysis&lt;/h2&gt;

&lt;p&gt;Understanding how sentiment plays a role in the making of a trending of YouTube video is an interesting area to look at.&lt;/p&gt;

&lt;p&gt;To do this sort of analysis we look at the channel title, title tags, and description to see how positive or negative a typical trending video is.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;
For this problem we can classify our videos into four distinct categories:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Popular :&lt;/strong&gt; views &amp;lt; 10,000&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Very Popular :&lt;/strong&gt; 10,000 &amp;lt; views &amp;lt; 100,000&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Highly Popular :&lt;/strong&gt; 100,000 &amp;lt; views &amp;lt; 1 Million&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hyper Popular :&lt;/strong&gt; views &amp;gt; 1 Million&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/ryb0h86L/image.png&quot; alt=&quot;&quot; title=&quot;Sentiment Analysis 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From this we can see that the very popular to hyper popular trending videos have positive sentiments in their descriptions, tags and channel title of around &lt;strong&gt;70%&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;least&lt;/strong&gt; popular videos of these four categories had a positive sentiment of about &lt;strong&gt;57%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is consistent with our analysis of Likes/Dislikes ratio. It shows that a video needs to be &lt;strong&gt;controversial&lt;/strong&gt;; however &lt;strong&gt;the more negative&lt;/strong&gt; it tends to be, &lt;strong&gt;the less likely&lt;/strong&gt; it is to be the most trending video.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://i.postimg.cc/HsdxvjSR/image.png&quot; alt=&quot;&quot; title=&quot;breakdown&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Knowing the components of a Trending YouTube video might be able to provide some insight about what a trending video looks like and how to recognize it but &lt;strong&gt;human behavior is unpredictable&lt;/strong&gt; and there will always be outliers.&lt;/p&gt;

&lt;p&gt;In general, your video is much more likely to be trending if you are a &lt;strong&gt;musician&lt;/strong&gt; or an &lt;strong&gt;entertainer&lt;/strong&gt;. This might have to do with the demographic and the reasons why people visit YouTube.&lt;/p&gt;

&lt;p&gt;While creating video content, it’s important to keep the idea of &lt;strong&gt;controversy&lt;/strong&gt; in mind. This is based on human psyche: people are attracted to anything that grabs attention, and controversy does just that.&lt;/p&gt;

&lt;p&gt;However, it’s important to &lt;strong&gt;strike a balance&lt;/strong&gt;. You will see the most amount of views and have the highest likelihood of a trending video when you hit the sweet spot between challenging a thought and inciting emotion.
As a content creator, you will have to make sure to keep that in mind while trying to enter an online media market that is highly saturated.&lt;/p&gt;

&lt;h1 id=&quot;appendix&quot;&gt;Appendix&lt;/h1&gt;

&lt;p&gt;Motivated by the results from this research, we build a &lt;strong&gt;classification model&lt;/strong&gt; using &lt;strong&gt;naive bayes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The inputs features are title, channel_title, tags, description.&lt;/p&gt;

&lt;p&gt;We labeled our dataset into four categories as before:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Popular : 10,000 &amp;gt; views&lt;/li&gt;
  &lt;li&gt;Very Popular : 10,000 &amp;lt; views &amp;lt; 100,000&lt;/li&gt;
  &lt;li&gt;Highly Popular : 100,000 &amp;lt; views &amp;lt; 1 Million&lt;/li&gt;
  &lt;li&gt;Hyper Popular : views &amp;gt; 1 Million&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;.&lt;/th&gt;
      &lt;th&gt;precision&lt;/th&gt;
      &lt;th&gt;recall&lt;/th&gt;
      &lt;th&gt;fl-score&lt;/th&gt;
      &lt;th&gt;support&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.14&lt;/td&gt;
      &lt;td&gt;0.89&lt;/td&gt;
      &lt;td&gt;0.25&lt;/td&gt;
      &lt;td&gt;162&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.20&lt;/td&gt;
      &lt;td&gt;0.68&lt;/td&gt;
      &lt;td&gt;0.31&lt;/td&gt;
      &lt;td&gt;878&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.80&lt;/td&gt;
      &lt;td&gt;0.39&lt;/td&gt;
      &lt;td&gt;0.53&lt;/td&gt;
      &lt;td&gt;3873&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.81&lt;/td&gt;
      &lt;td&gt;0.58&lt;/td&gt;
      &lt;td&gt;0.68&lt;/td&gt;
      &lt;td&gt;3277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;accuracy&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;0.51&lt;/td&gt;
      &lt;td&gt;8190&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;macro avg&lt;/td&gt;
      &lt;td&gt;0.49&lt;/td&gt;
      &lt;td&gt;0.64&lt;/td&gt;
      &lt;td&gt;0.44&lt;/td&gt;
      &lt;td&gt;8190&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;weighted avg&lt;/td&gt;
      &lt;td&gt;0.73&lt;/td&gt;
      &lt;td&gt;0.51&lt;/td&gt;
      &lt;td&gt;0.56&lt;/td&gt;
      &lt;td&gt;8190&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The model has 0.73 weighted average precision, which is a solid result. It further proves the importance of  the four features: &lt;strong&gt;title, channel_title, tags, description&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hint 1: Project GitHub page: &lt;a href=&quot;https://github.com/waittim/Youtube-trending-videos-analysis&quot;&gt;Youtube-trending-videos-analysis&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hint 2: Data source: &lt;a href=&quot;https://www.kaggle.com/datasnaek/youtube-new&quot;&gt;Trending YouTube Video Statistics&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Oct 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2019/10/29/trending-video/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/10/29/trending-video/</guid>
        
        <category>YouTube</category>
        
        <category>EDA</category>
        
        <category>Project</category>
        
        <category>R</category>
        
        <category>Python</category>
        
        
      </item>
    
  </channel>
</rss>
